
@article{nyul_new_2000,
	title = {New variants of a method of {MRI} scale standardization},
	volume = {19},
	issn = {0278-0062},
	doi = {10.1109/42.836373},
	abstract = {One of the major drawbacks of magnetic resonance imaging (MRI) has been the lack of a standard and quantifiable interpretation of image intensities. Unlike in other modalities, such as X-ray computerized tomography, MR images taken for the same patient on the same scanner at different times may appear different from each other due to a variety of scanner-dependent variations and, therefore, the absolute intensity values do not have a fixed meaning. We have devised a two-step method wherein all images (independent of patients and the specific brand of the MR scanner used) can be transformed in such a way that for the same protocol and body region, in the transformed images similar intensities will have similar tissue meaning. Standardized images can be displayed with fixed windows without the need of per-case adjustment. More importantly, extraction of quantitative information about healthy organs or about abnormalities can be considerably simplified. This paper introduces and compares new variants of this standardizing method that can help to overcome some of the problems with the original method.},
	language = {eng},
	number = {2},
	journal = {IEEE transactions on medical imaging},
	author = {Ny{\'u}l, L. G. and Udupa, J. K. and Zhang, X.},
	month = feb,
	year = {2000},
	pmid = {10784285},
	keywords = {Algorithms, Brain, Humans, Image Processing, Computer-Assisted, Magnetic Resonance Imaging, Multiple Sclerosis},
	pages = {143--150},
}

@article{zhuo_mr_2006,
	title = {{MR} {Artifacts}, {Safety}, and {Quality} {Control}},
	volume = {26},
	issn = {0271-5333},
	url = {https://pubs.rsna.org/doi/full/10.1148/rg.261055134},
	doi = {10.1148/rg.261055134},
	abstract = {Artifacts in magnetic resonance (MR) imaging result from the complex interaction of contemporary imager subsystems, including the main magnet, gradient coils, radiofrequency (RF) transmitter and receiver, and reconstruction algorithm used. An understanding of the sources of artifacts enables optimization of the MR imaging system performance. The increasing clinical use of very high magnetic field strengths, high-performance gradients, and multiple RF channels also mandates renewed attention to the biologic effects and physical safety of MR imaging. Radiologists should be aware of the potential physiologic effects of prolonged exposure to magnetic fields, acoustic noise, and RF energy during MR imaging and should use all the available methods for avoiding accidents and adverse effects. Imaging equipment should be regularly tested and monitored to ensure its stability and the uniformity of its functioning. Newly installed or upgraded MR systems should be tested by a physicist or qualified engineer before use. In addition, the authors recommend participation in the MR imaging accreditation program of the American College of Radiology to establish the initial framework for an adequate quality assurance program, which then can be further developed to fulfill local institutional needs.{\textcopyright} RSNA, 2006},
	number = {1},
	urldate = {2020-03-03},
	journal = {RadioGraphics},
	author = {Zhuo, Jiachen and Gullapalli, Rao P.},
	month = jan,
	year = {2006},
	pages = {275--297},
	file = {Snapshot:/home/fernando/Zotero/storage/RK7W6RGE/rg.html:text/html},
}

@misc{isensee_batchgenerators_2020,
	title = {batchgenerators - a python framework for data augmentation},
	url = {https://zenodo.org/record/3632567#.Xlqnb5P7S8o},
	abstract = {A framework for on the fly data augmentation for 2D and 3D images. Works with all commonly used deep learning frameworks. If you use it, please cite it. The most recent version of batchgenerators can be found at:~https://github.com/MIC-DKFZ/batchgenerators},
	urldate = {2020-02-29},
	publisher = {Zenodo},
	author = {Isensee, Fabian and J{\"a}ger, Paul and Wasserthal, Jakob and Zimmerer, David and Petersen, Jens and Kohl, Simon and Schock, Justus and Klein, Andre and Ro{\ss}, Tobias and Wirkert, Sebastian and Neher, Peter and Dinkelacker, Stefan and K{\"o}hler, Gregor and Maier-Hein, Klaus},
	month = jan,
	year = {2020},
	doi = {10.5281/zenodo.3632567},
	keywords = {data augmentation, image analysis, deep learning, machine learning, image segmentation},
	file = {Zenodo Snapshot:/home/fernando/Zotero/storage/ZSSJNW4V/3632567.html:text/html},
}

@article{omigbodun_effects_2019,
	title = {The effects of physics-based data augmentation on the generalizability of deep neural networks: {Demonstration} on nodule false-positive reduction},
	volume = {46},
	copyright = {{\textcopyright} 2019 American Association of Physicists in Medicine},
	issn = {2473-4209},
	shorttitle = {The effects of physics-based data augmentation on the generalizability of deep neural networks},
	url = {https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.13755},
	doi = {10.1002/mp.13755},
	abstract = {Purpose An important challenge for deep learning models is generalizing to new datasets that may be acquired with acquisition protocols different from the training set. It is not always feasible to expand training data to the range encountered in clinical practice. We introduce a new technique, physics-based data augmentation (PBDA), that can emulate new computed tomography (CT) data acquisition protocols. We demonstrate two forms of PBDA, emulating increases in slice thickness and reductions of dose, on the specific problem of false-positive reduction in the automatic detection of lung nodules. Methods We worked with CT images from the lung image database consortium (LIDC) collection. We employed a hybrid ensemble convolutional neural network (CNN), which consists of multiple CNN modules (VGG, DenseNet, ResNet), for a classification task of determining whether an image patch was a suspicious nodule or a false positive. To emulate a reduction in tube current, we injected noise by simulating forward projection, noise addition, and backprojection corresponding to 1.5 mAs (a {\textquotedblleft}chest x-ray{\textquotedblright} dose). To simulate thick slice CT scans from thin slice CT scans, we grouped and averaged spatially contiguous CT within thin slice data. The neural network was trained with 10\% of the LIDC dataset that was selected to have either the highest tube current or the thinnest slices. The network was tested on the remaining data. We compared PBDA to a baseline with standard geometric augmentations (such as shifts and rotations) and Gaussian noise addition. Results PBDA improved the performance of the networks when generalizing to the test dataset in a limited number of cases. We found that the best performance was obtained by applying augmentation at very low doses (1.5 mAs), about an order of magnitude less than most screening protocols. In the baseline augmentation, a comparable level of Gaussian noise was injected. For dose reduction PBDA, the average sensitivity of 0.931 for the hybrid ensemble network was not statistically different from the average sensitivity of 0.935 without PBDA. Similarly for slice thickness PBDA, the average sensitivity of 0.900 when augmenting with doubled simulated slice thicknesses was not statistically different from the average sensitivity of 0.895 without PBDA. While there were cases detailed in this paper in which we observed improvements, the overall picture was one that suggests PBDA may not be an effective data enrichment tool. Conclusions PBDA is a newly proposed strategy for mitigating the performance loss of neural networks related to the variation of acquisition protocol between the training dataset and the data that is encountered in deployment or testing. We found that PBDA does not provide robust improvements with the four neural networks (three modules and the ensemble) tested and for the specific task of false-positive reduction in nodule detection.},
	language = {en},
	number = {10},
	urldate = {2020-03-02},
	journal = {Medical Physics},
	author = {Omigbodun, Akinyinka O. and Noo, Frederic and McNitt-Gray, Michael and Hsu, William and Hsieh, Scott S.},
	year = {2019},
	keywords = {data augmentation, ensemble CNN, false-positive reduction, lung CT, lung nodule detection},
	pages = {4563--4574},
	file = {Snapshot:/home/fernando/Zotero/storage/QMDSSRST/mp.html:text/html},
}

@article{lowekamp_design_2013,
	title = {The {Design} of {SimpleITK}},
	volume = {7},
	issn = {1662-5196},
	doi = {10.3389/fninf.2013.00045},
	abstract = {SimpleITK is a new interface to the Insight Segmentation and Registration Toolkit (ITK) designed to facilitate rapid prototyping, education and scientific activities via high level programming languages. ITK is a templated C++ library of image processing algorithms and frameworks for biomedical and other applications, and it was designed to be generic, flexible and extensible. Initially, ITK provided a direct wrapping interface to languages such as Python and Tcl through the WrapITK system. Unlike WrapITK, which exposed ITK's complex templated interface, SimpleITK was designed to provide an easy to use and simplified interface to ITK's algorithms. It includes procedural methods, hides ITK's demand driven pipeline, and provides a template-less layer. Also SimpleITK provides practical conveniences such as binary distribution packages and overloaded operators. Our user-friendly design goals dictated a departure from the direct interface wrapping approach of WrapITK, toward a new facade class structure that only exposes the required functionality, hiding ITK's extensive template use. Internally SimpleITK utilizes a manual description of each filter with code-generation and advanced C++ meta-programming to provide the higher-level interface, bringing the capabilities of ITK to a wider audience. SimpleITK is licensed as open source software library under the Apache License Version 2.0 and more information about downloading it can be found at http://www.simpleitk.org.},
	language = {eng},
	journal = {Frontiers in Neuroinformatics},
	author = {Lowekamp, Bradley C. and Chen, David T. and Ib{\'a}{\~n}ez, Luis and Blezek, Daniel},
	year = {2013},
	pmid = {24416015},
	pmcid = {PMC3874546},
	keywords = {image processing and analysis, image processing software, Insight Toolkit, segmentation, software design, software development},
	pages = {45},
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0: fundamental algorithms for scientific computing in {Python}},
	copyright = {2020 The Author(s)},
	issn = {1548-7105},
	shorttitle = {{SciPy} 1.0},
	url = {https://www.nature.com/articles/s41592-019-0686-2},
	doi = {10.1038/s41592-019-0686-2},
	abstract = {This Perspective describes the development and capabilities of SciPy 1.0, an open source scientific computing library for the Python programming language.},
	language = {en},
	urldate = {2020-03-02},
	journal = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, Ilhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and van Mulbregt, Paul},
	month = feb,
	year = {2020},
	pages = {1--12},
	file = {Snapshot:/home/fernando/Zotero/storage/ZSALICRL/s41592-019-0686-2.html:text/html},
}

@article{van_leemput_automated_1999,
	title = {Automated model-based tissue classification of {MR} images of the brain},
	volume = {18},
	issn = {0278-0062},
	doi = {10.1109/42.811270},
	abstract = {We describe a fully automated method for model-based tissue classification of magnetic resonance (MR) images of the brain. The method interleaves classification with estimation of the model parameters, improving the classification at each iteration. The algorithm is able to segment single- and multispectral MR images, corrects for MR signal inhomogeneities, and incorporates contextual information by means of Markov random Fields (MRF's). A digital brain atlas containing prior expectations about the spatial location of tissue classes is used to initialize the algorithm. This makes the method fully automated and therefore it provides objective and reproducible segmentations. We have validated the technique on simulated as well as on real MR images of the brain.},
	language = {eng},
	number = {10},
	journal = {IEEE transactions on medical imaging},
	author = {Van Leemput, K. and Maes, F. and Vandermeulen, D. and Suetens, P.},
	month = oct,
	year = {1999},
	pmid = {10628949},
	keywords = {Algorithms, Brain, Humans, Magnetic Resonance Imaging, Bias, Computer Simulation, Likelihood Functions, Markov Chains, Models, Neurological, Reproducibility of Results},
	pages = {897--908},
}

@article{nyul_standardizing_1999,
	title = {On standardizing the {MR} image intensity scale},
	volume = {42},
	issn = {0740-3194},
	doi = {10.1002/(sici)1522-2594(199912)42:6<1072::aid-mrm11>3.0.co;2-m},
	abstract = {The lack of a standard image intensity scale in MRI causes many difficulties in image display and analysis. A two-step postprocessing method is proposed for standardizing the intensity scale in such a way that for the same MR protocol and body region, similar intensities will have similar tissue meaning. In the first step, the parameters of the standardizing transformation are "learned" from a set of images. In the second step, for each MR study these parameters are used to map their histogram into the standardized histogram. The method was tested quantitatively on 90 whole-brain studies of multiple sclerosis patients for several protocols and qualitatively for several other protocols and body regions. Measurements using mean squared difference showed that the standardized image intensities have statistically significantly (P {\textless} 0.01) more consistent range and meaning than the originals. Fixed gray level windows can be established for the standardized images and used for display without the need of per case adjustment. Preliminary results also indicate that the method facilitates improving the degree of automation of image segmentation. Magn Reson Med 42:1072-1081, 1999.},
	language = {eng},
	number = {6},
	journal = {Magnetic Resonance in Medicine},
	author = {Ny{\'u}l, L. G. and Udupa, J. K.},
	month = dec,
	year = {1999},
	pmid = {10571928},
	keywords = {Algorithms, Brain, Humans, Image Processing, Computer-Assisted, Magnetic Resonance Imaging, Multiple Sclerosis, Data Display, Foot, Image Enhancement},
	pages = {1072--1081},
}

@misc{brett_nipynibabel_2020,
	title = {nipy/nibabel: 3.0.1},
	shorttitle = {nipy/nibabel},
	url = {https://zenodo.org/record/3628482#.XlyGkJP7S8o},
	abstract = {Most work on NiBabel so far has been by Matthew Brett (MB), Chris Markiewicz (CM), Michael Hanke (MH), Marc-Alexandre C{\^o}t{\'e} (MC), Ben Cipollini (BC), Paul McCarthy (PM), Chris Cheng (CC), Yaroslav Halchenko (YOH), Satra Ghosh (SG), Eric Larson (EL), Demian Wassermann, and Stephan Gerhard. References like "pr/298" refer to github pull request numbers. 3.0.1 (Monday 27 January 2020) Bug fixes Test failed by using array method on tuple. (pr/860) (Ben Darwin, reviewed by CM) Validate ExpiredDeprecationError{\textbackslash}s, promoted by 3.0 release from DeprecationWarning{\textbackslash}s. (pr/857) (CM) Maintenance Remove logic accommodating numpy without float16 types. (pr/866) (CM) Accommodate new numpy dtype strings. (pr/858) (CM)},
	urldate = {2020-03-02},
	publisher = {Zenodo},
	author = {Brett, Matthew and Markiewicz, Christopher J. and Hanke, Michael and C{\^o}t{\'e}, Marc-Alexandre and Cipollini, Ben and McCarthy, Paul and Cheng, Christopher P. and Halchenko, Yaroslav O. and Cottaar, Michiel and Ghosh, Satrajit and Larson, Eric and Wassermann, Demian and Gerhard, Stephan and Lee, Gregory R. and Wang, Hao-Ting and Kastman, Erik and Rokem, Ariel and Madison, Cindee and Morency, F{\'e}lix C. and Moloney, Brendan and Goncalves, Mathias and Riddell, Cameron and Burns, Christopher and Millman, Jarrod and Gramfort, Alexandre and Lepp{\"a}kangas, Jaakko and Markello, Ross and van den Bosch, Jasper J.F. and Vincent, Robert D. and Braun, Henry and Subramaniam, Krish and Jarecka, Dorota and Gorgolewski, Krzysztof J. and Raamana, Pradeep Reddy and Nichols, B. Nolan and Baker, Eric M. and Hayashi, Soichi and Pinsard, Basile and Haselgrove, Christian and Hymers, Mark and Esteban, Oscar and Koudoro, Serge and Oosterhof, Nikolaas N. and Amirbekian, Bago and Nimmo-Smith, Ian and Nguyen, Ly and Reddigari, Samir and St-Jean, Samuel and Panfilov, Egor and Garyfallidis, Eleftherios and Varoquaux, Gael and Kaczmarzyk, Jakub and Legarreta, Jon Haitz and Hahn, Kevin S. and Hinds, Oliver P. and Fauber, Bennet and Poline, Jean-Baptiste and Stutters, Jon and Jordan, Kesshi and Cieslak, Matthew and Moreno, Miguel Estevan and Haenel, Valentin and Schwartz, Yannick and Darwin, Benjamin C and Thirion, Bertrand and Papadopoulos Orfanos, Dimitri and P{\'e}rez-Garc{\'i}a, Fernando and Solovey, Igor and Gonzalez, Ivan and Palasubramaniam, Jath and Lecher, Justin and Leinweber, Katrin and Raktivan, Konstantinos and Fischer, Peter and Gervais, Philippe and Gadde, Syam and Ballinger, Thomas and Roos, Thomas and Reddam, Venkateswara Reddy and freec84},
	month = jan,
	year = {2020},
	doi = {10.5281/zenodo.3628482},
	keywords = {neuroimaging},
	file = {Zenodo Snapshot:/home/fernando/Zotero/storage/FXYPB7DB/3628482.html:text/html},
}

@article{mccormick_itk_2014,
	title = {{ITK}: enabling reproducible research and open science},
	volume = {8},
	issn = {1662-5196},
	shorttitle = {{ITK}},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2014.00013/full},
	doi = {10.3389/fninf.2014.00013},
	abstract = {Reproducibility verification is essential to the practice of the scientific method. Researchers report their findings, which are strengthened as other independent groups in the scientific community share similar outcomes. In the many scientific fields where software has become a fundamental tool for capturing and analyzing data, this requirement of reproducibility implies that reliable and comprehensive software platforms and tools should be made available to the scientific community. The tools will empower them and the public to verify, through practice, the reproducibility of observations that are reported in the scientific literature.Medical image analysis is one of the fields in which the use of computational resources, both software and hardware, are an essential platform for performing experimental work. In this arena, the introduction of the Insight Toolkit (ITK) in 1999 has transformed the field and facilitates its progress by accelerating the rate at which algorithmic implementations are developed, tested, disseminated and improved. By building on the efficiency and quality of open source methodologies, ITK has provided the medical image community with an effective platform on which to build a daily workflow that incorporates the true scientific practices of reproducibility verification.This article describes the multiple tools, methodologies, and practices that the ITK community has adopted, refined, and followed during the past decade, in order to become one of the research communities with the most modern reproducibility verification infrastructure. For example, 207 contributors have created over 2400 unit tests that provide over 84\% code line test coverage. The Insight Journal, an open publication journal associated with the toolkit, has seen over 360,000 publication downloads. The median normalized closeness centrality, a measure of knowledge flow, resulting from the distributed peer code review system was high, 0.46.},
	language = {English},
	urldate = {2020-03-02},
	journal = {Frontiers in Neuroinformatics},
	author = {McCormick, Matthew Michael and Liu, Xiaoxiao and Ibanez, Luis and Jomier, Julien and Marion, Charles},
	year = {2014},
	keywords = {Insight Toolkit, Code Review, Insight Journal, ITK, Open Science, reproducibility},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called {\textquotedblleft}ImageNet{\textquotedblright}, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {computer vision, Explosions, Image databases, image resolution, image retrieval, Image retrieval, ImageNet database, Information retrieval, Internet, large-scale hierarchical image database, large-scale ontology, Large-scale systems, multimedia computing, multimedia data, Multimedia databases, Ontologies, ontologies (artificial intelligence), Robustness, Spine, subtree, trees (mathematics), very large databases, visual databases, wordNet structure},
	pages = {248--255},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/B26YTJK4/5206848.html:text/html},
}

@article{lucena_convolutional_2019,
	title = {Convolutional neural networks for skull-stripping in brain {MR} imaging using silver standard masks},
	volume = {98},
	issn = {1873-2860},
	doi = {10.1016/j.artmed.2019.06.008},
	abstract = {Manual annotation is considered to be the "gold standard" in medical imaging analysis. However, medical imaging datasets that include expert manual segmentation are scarce as this step is time-consuming, and therefore expensive. Moreover, single-rater manual annotation is most often used in data-driven approaches making the network biased to only that single expert. In this work, we propose a CNN for brain extraction in magnetic resonance (MR) imaging, that is fully trained with what we refer to as "silver standard" masks. Therefore, eliminating the cost associated with manual annotation. Silver standard masks are generated by forming the consensus from a set of eight, public, non-deep-learning-based brain extraction methods using the Simultaneous Truth and Performance Level Estimation (STAPLE) algorithm. Our method consists of (1) developing a dataset with "silver standard" masks as input, and implementing (2) a tri-planar method using parallel 2D U-Net-based convolutional neural networks (CNNs) (referred to as CONSNet). This term refers to our integrated approach, i.e., training with silver standard masks and using a 2D U-Net-based architecture. We conducted our analysis using three public datasets: the Calgary-Campinas-359 (CC-359), the LONI Probabilistic Brain Atlas (LPBA40), and the Open Access Series of Imaging Studies (OASIS). Five performance metrics were used in our experiments: Dice coefficient, sensitivity, specificity, Hausdorff distance, and symmetric surface-to-surface mean distance. Our results showed that we outperformed (i.e., larger Dice coefficients) the current state-of-the-art skull-stripping methods without using gold standard annotation for the CNNs training stage. CONSNet is the first deep learning approach that is fully trained using silver standard data and is, thus, more generalizable. Using these masks, we eliminate the cost of manual annotation, decreased inter-/intra-rater variability, and avoided CNN segmentation overfitting towards one specific manual annotation guideline that can occur when gold standard masks are used. Moreover, once trained, our method takes few seconds to process a typical brain image volume using modern a high-end GPU. In contrast, many of the other competitive methods have processing times in the order of minutes.},
	language = {eng},
	journal = {Artificial Intelligence in Medicine},
	author = {Lucena, Oeslle and Souza, Roberto and Rittner, Let{\'i}cia and Frayne, Richard and Lotufo, Roberto},
	year = {2019},
	pmid = {31521252},
	keywords = {Convolutional neural network (CNN), Data augmentation, Silver standard masks, Skull-stripping},
	pages = {48--58},
}

@misc{gabriel_rise_1991,
	title = {Rise of {Worse} {Is} {Better}},
	url = {http://dreamsongs.com/RiseOfWorseIsBetter.html},
	urldate = {2020-03-02},
	author = {Gabriel, Richard P.},
	year = {1991},
	file = {Rise of Worse Is Better:/home/fernando/Zotero/storage/KBUFZHKG/RiseOfWorseIsBetter.html:text/html},
}

@misc{wiredfool_pillow_2016,
	title = {Pillow: 3.1.0},
	shorttitle = {Pillow},
	url = {https://zenodo.org/record/44297#.Xlx04pP7S8o},
	abstract = {The friendly PIL fork},
	urldate = {2020-03-02},
	publisher = {Zenodo},
	author = {wiredfool and Alex Clark and Hugo and Andrew Murray and Alexander Karpinsky and Christoph Gohlke and Brian Crowell and David Schmidt and Alastair Houghton and Steve Johnson and Sandro Mani and Josh Ware and David Caro and Steve Kossouho and Eric W. Brown and Antony Lee and Mikhail Korobov and Micha{\l } G{\'o}rny and Esteban Santana Santana and Nicolas Pieuchot and Oliver Tonnhofer and Michael Brown and Benoit Pierre and Joaqu{\'i}n Cuenca Abela and Lars J{\o}rgen Solberg and Felipe Reyes and Alexey Buzanov and Yifu Yu and eliempje and Fredrik Tolf},
	month = jan,
	year = {2016},
	doi = {10.5281/zenodo.44297},
	file = {Zenodo Snapshot:/home/fernando/Zotero/storage/V7TW3V5G/44297.html:text/html},
}

@article{nikolov_deep_2018,
	title = {Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy},
	url = {http://arxiv.org/abs/1809.04430},
	abstract = {Over half a million individuals are diagnosed with head and neck cancer each year worldwide. Radiotherapy is an important curative treatment for this disease, but it requires manually intensive delineation of radiosensitive organs at risk (OARs). This planning process can delay treatment commencement. While auto-segmentation algorithms offer a potentially time-saving solution, the challenges in defining, quantifying and achieving expert performance remain. Adopting a deep learning approach, we demonstrate a 3D U-Net architecture that achieves performance similar to experts in delineating a wide range of head and neck OARs. The model was trained on a dataset of 663 deidentified computed tomography (CT) scans acquired in routine clinical practice and segmented according to consensus OAR definitions. We demonstrate its generalisability through application to an independent test set of 24 CT scans available from The Cancer Imaging Archive collected at multiple international sites previously unseen to the model, each segmented by two independent experts and consisting of 21 OARs commonly segmented in clinical practice. With appropriate validation studies and regulatory approvals, this system could improve the effectiveness of radiotherapy pathways.},
	urldate = {2020-03-02},
	journal = {arXiv:1809.04430 [physics, stat]},
	author = {Nikolov, Stanislav and Blackwell, Sam and Mendes, Ruheena and De Fauw, Jeffrey and Meyer, Clemens and Hughes, C{\'i}an and Askham, Harry and Romera-Paredes, Bernardino and Karthikesalingam, Alan and Chu, Carlton and Carnell, Dawn and Boon, Cheng and D'Souza, Derek and Moinuddin, Syed Ali and Sullivan, Kevin and Consortium, DeepMind Radiographer and Montgomery, Hugh and Rees, Geraint and Sharma, Ricky and Suleyman, Mustafa and Back, Trevor and Ledsam, Joseph R. and Ronneberger, Olaf},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.04430},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Physics - Medical Physics},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/SFDQTRIL/1809.html:text/html},
}

@misc{nvidia_nvidiadali_2020,
	title = {{NVIDIA}/{DALI}},
	url = {https://github.com/NVIDIA/DALI},
	abstract = {A library containing both highly optimized building blocks and an execution engine for data pre-processing in deep learning applications},
	urldate = {2020-02-29},
	publisher = {NVIDIA Corporation},
	author = {NVIDIA},
	month = feb,
	year = {2020},
	note = {original-date: 2018-06-01T22:18:01Z},
	keywords = {data-augmentation, data-processing, deep-learning, fast-data-pipeline, gpu, gpu-tensorflow, image-augmentation, image-processing, machine-learning, neural-network, python},
}

@article{haarburger_delira_2019,
	title = {Delira: {A} {High}-{Level} {Framework} for {Deep} {Learning} in {Medical} {Image} {Analysis}},
	volume = {4},
	issn = {2475-9066},
	shorttitle = {Delira},
	url = {https://joss.theoj.org/papers/10.21105/joss.01488},
	doi = {10.21105/joss.01488},
	abstract = {Haarburger et al., (2019). Delira: A High-Level Framework for Deep Learning in Medical Image Analysis. Journal of Open Source Software, 4(38), 1488, https://doi.org/10.21105/joss.01488},
	language = {en},
	number = {38},
	urldate = {2020-03-02},
	journal = {Journal of Open Source Software},
	author = {Haarburger, Christoph and Schock, Justus and Baumgartner, Michael and Rippel, Oliver and Merhof, Dorit},
	month = jun,
	year = {2019},
	pages = {1488},
	file = {Snapshot:/home/fernando/Zotero/storage/Z9X6Y4BH/joss.html:text/html},
}

@misc{he_state_2019,
	title = {The {State} of {Machine} {Learning} {Frameworks} in 2019},
	url = {http://bit.ly/3cjpliJ},
	abstract = {Since deep learning regained prominence in 2012, many machine learning frameworks have clamored to become the new favorite among researchers and industry practitioners. From the early academic outputs Caffe and Theano to the massive industry-backed PyTorch and TensorFlow, this deluge of options makes it difficult to keep track of what},
	language = {en},
	urldate = {2020-02-29},
	journal = {The Gradient},
	author = {He, Horace},
	month = oct,
	year = {2019},
	file = {Snapshot:/home/fernando/Zotero/storage/UBNXHDKQ/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry.html:text/html},
}

@article{shan_unsupervised_2018,
	title = {Unsupervised {End}-to-end {Learning} for {Deformable} {Medical} {Image} {Registration}},
	url = {http://arxiv.org/abs/1711.08608},
	abstract = {We propose a registration algorithm for 2D CT/MRI medical images with a new unsupervised end-to-end strategy using convolutional neural networks. The contributions of our algorithm are threefold: (1) We transplant traditional image registration algorithms to an end-to-end convolutional neural network framework, while maintaining the unsupervised nature of image registration problems. The image-to-image integrated framework can simultaneously learn both image features and transformation matrix for registration. (2) Training with additional data without any label can further improve the registration performance by approximately 10 \%. (3) The registration speed is 100x faster than traditional methods. The proposed network is easy to implement and can be trained efficiently. Experiments demonstrate that our system achieves state-of-the-art results on 2D brain registration and achieves comparable results on 2D liver registration. It can be extended to register other organs beyond liver and brain such as kidney, lung, and heart.},
	urldate = {2020-03-01},
	journal = {arXiv:1711.08608 [cs]},
	author = {Shan, Siyuan and Yan, Wen and Guo, Xiaoqing and Chang, Eric I.-Chao and Fan, Yubo and Xu, Yan},
	month = jan,
	year = {2018},
	note = {arXiv: 1711.08608},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/STWEAKBZ/1711.html:text/html},
}

@article{lu_multimodal_2018,
	title = {Multimodal and {Multiscale} {Deep} {Neural} {Networks} for the {Early} {Diagnosis} of {Alzheimer}'s {Disease} using structural {MR} and {FDG}-{PET} images},
	volume = {8},
	issn = {2045-2322},
	doi = {10.1038/s41598-018-22871-z},
	abstract = {Alzheimer's Disease (AD) is a progressive neurodegenerative disease where biomarkers for disease based on pathophysiology may be able to provide objective measures for disease diagnosis and staging. Neuroimaging scans acquired from MRI and metabolism images obtained by FDG-PET provide in-vivo measurements of structure and function (glucose metabolism) in a living brain. It is hypothesized that combining multiple different image modalities providing complementary information could help improve early diagnosis of AD. In this paper, we propose a novel deep-learning-based framework to discriminate individuals with AD utilizing a multimodal and multiscale deep neural network. Our method delivers 82.4\% accuracy in identifying the individuals with mild cognitive impairment (MCI) who will convert to AD at 3 years prior to conversion (86.4\% combined accuracy for conversion within 1-3 years), a 94.23\% sensitivity in classifying individuals with clinical diagnosis of probable AD, and a 86.3\% specificity in classifying non-demented controls improving upon results in published literature.},
	language = {eng},
	number = {1},
	journal = {Scientific Reports},
	author = {Lu, Donghuan and Popuri, Karteek and Ding, Gavin Weiguang and Balachandar, Rakesh and Beg, Mirza Faisal and {Alzheimer{\textquoteright}s Disease Neuroimaging Initiative}},
	year = {2018},
	pmid = {29632364},
	pmcid = {PMC5890270},
	keywords = {Brain, Humans, Magnetic Resonance Imaging, Aged, Aged, 80 and over, Middle Aged, Neural Networks, Computer, Alzheimer Disease, Case-Control Studies, Cognitive Dysfunction, Deep Learning, Early Diagnosis, Fluorodeoxyglucose F18, Multimodal Imaging, Positron-Emission Tomography, Radiopharmaceuticals, Sensitivity and Specificity},
	pages = {5697},
}

@article{poplin_prediction_2018,
	title = {Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning},
	volume = {2},
	copyright = {2018 The Author(s)},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-018-0195-0},
	doi = {10.1038/s41551-018-0195-0},
	abstract = {Deep learning predicts, from retinal images, cardiovascular risk factors{\textemdash}such as smoking status, blood pressure and age{\textemdash}not previously thought to be present or quantifiable in these images.},
	language = {en},
	number = {3},
	urldate = {2020-03-01},
	journal = {Nature Biomedical Engineering},
	author = {Poplin, Ryan and Varadarajan, Avinash V. and Blumer, Katy and Liu, Yun and McConnell, Michael V. and Corrado, Greg S. and Peng, Lily and Webster, Dale R.},
	month = mar,
	year = {2018},
	pages = {158--164},
	file = {Snapshot:/home/fernando/Zotero/storage/JKCV2R5K/s41551-018-0195-0.html:text/html},
}

@article{chen_variable-density_2018,
	title = {Variable-{Density} {Single}-{Shot} {Fast} {Spin}-{Echo} {MRI} with {Deep} {Learning} {Reconstruction} by {Using} {Variational} {Networks}},
	volume = {289},
	issn = {0033-8419},
	url = {https://pubs.rsna.org/doi/10.1148/radiol.2018180445},
	doi = {10.1148/radiol.2018180445},
	abstract = {PurposeTo develop a deep learning reconstruction approach to improve the                            reconstruction speed and quality of highly undersampled variable-density                            single-shot fast spin-echo imaging by using a variational network (VN),                            and to clinically evaluate the feasibility of this approach.Materials and MethodsImaging was performed with a 3.0-T imager with a coronal variable-density                            single-shot fast spin-echo sequence at 3.25 times acceleration in 157                            patients referred for abdominal imaging (mean age, 11 years; range,                            1{\textendash}34 years; 72 males [mean age, 10 years; range, 1{\textendash}26                            years] and 85 females [mean age, 12 years; range, 1{\textendash}34 years])                            between March 2016 and April 2017. A VN was trained based on the                            parallel imaging and compressed sensing (PICS) reconstruction of 130                            patients. The remaining 27 patients were used for evaluation. Image                            quality was evaluated in an independent blinded fashion by three                            radiologists in terms of overall image quality, perceived                            signal-to-noise ratio, image contrast, sharpness, and residual artifacts                            with scores ranging from 1 (nondiagnostic) to 5 (excellent). Wilcoxon                            tests were performed to test the hypothesis that there was no                            significant difference between VN and PICS.ResultsVN achieved improved perceived signal-to-noise ratio (P                            = .01) and improved sharpness (P {\textless} .001),                            with no difference in image contrast (P = .24) and                            residual artifacts (P = .07). In terms of overall                            image quality, VN performed better than did PICS (P                            = .02). Average reconstruction time {\textpm} standard deviation was                            5.60 seconds {\textpm} 1.30 per section for PICS and 0.19 second {\textpm}                            0.04 per section for VN.ConclusionCompared with the conventional parallel imaging and compressed sensing                            reconstruction (PICS), the variational network (VN) approach accelerates                            the reconstruction of variable-density single-shot fast spin-echo                            sequences and achieves improved overall image quality with higher                            perceived signal-to-noise ratio and sharpness.{\textcopyright} RSNA, 2018Online supplemental material is available for this                                    article.},
	number = {2},
	urldate = {2020-03-01},
	journal = {Radiology},
	author = {Chen, Feiyu and Taviani, Valentina and Malkiel, Itzik and Cheng, Joseph                            Y. and Tamir, Jonathan                            I. and Shaikh, Jamil and Chang, Stephanie                            T. and Hardy, Christopher                            J. and Pauly, John                        M. and Vasanawala, Shreyas                            S.},
	month = jul,
	year = {2018},
	pages = {366--373},
	file = {Snapshot:/home/fernando/Zotero/storage/6H7NNR35/radiol.html:text/html},
}

@article{holmes_enhancement_1998,
	title = {Enhancement of {MR} images using registration for signal averaging},
	volume = {22},
	issn = {0363-8715},
	doi = {10.1097/00004728-199803000-00032},
	abstract = {PURPOSE: With the advent of noninvasive neuroimaging, a plethora of digital human neuroanatomical atlases has been developed. The accuracy of these atlases is constrained by the resolution and signal-gathering powers of available imaging equipment. In an attempt to circumvent these limitations and to produce a high resolution in vivo human neuroanatomy, we investigated the usefulness of intrasubject registration for post hoc MR signal averaging.
METHOD: Twenty-seven high resolution (7 x 0.78 and 20 x 1.0 mm3) T1-weighted volumes were acquired from a single subject, along with 12 double echo T2/proton density-weighted volumes. These volumes were automatically registered to a common stereotaxic space in which they were subsampled and intensity averaged. The resulting images were examined for anatomical quality and usefulness for other analytical techniques.
RESULTS: The quality of the resulting image from the combination of as few as five T1 volumes was visibly enhanced. The signal-to-noise ratio was expected to increase as the root of the number of contributing scans to 5.2, n = 27. The improvement in the n = 27 average was great enough that fine anatomical details, such as thalamic subnuclei and the gray bridges between the caudate and putamen, became crisply defined. The gray/white matter boundaries were also enhanced, as was the visibility of any finer structure that was surrounded by tissue of varying T1 intensity. The T2 and proton density average images were also of higher quality than single scans, but the improvement was not as dramatic as that of the T1 volumes.
CONCLUSION: Overall, the enhanced signal in the averaged images resulted in higher quality anatomical images, and the data lent themselves to several postprocessing techniques. The high quality of the enhanced images permits novel uses of the data and extends the possibilities for in vivo human neuroanatomy.},
	language = {eng},
	number = {2},
	journal = {Journal of Computer Assisted Tomography},
	author = {Holmes, C. J. and Hoge, R. and Collins, L. and Woods, R. and Toga, A. W. and Evans, A. C.},
	month = apr,
	year = {1998},
	pmid = {9530404},
	keywords = {Brain, Humans, Magnetic Resonance Imaging, Image Enhancement, Artifacts, Brain Mapping, Color},
	pages = {324--333},
}

@misc{noauthor_phoenixdlrising_2020,
	title = {{PhoenixDL}/rising},
	copyright = {MIT},
	url = {https://github.com/PhoenixDL/rising},
	abstract = {Provides everything needed for high performance data loading and augmentation in pytorch.},
	urldate = {2020-02-29},
	publisher = {PhoenixDL},
	month = feb,
	year = {2020},
	note = {original-date: 2019-11-17T17:46:31Z},
}

@article{riba_kornia_2019,
	title = {Kornia: an {Open} {Source} {Differentiable} {Computer} {Vision} {Library} for {PyTorch}},
	shorttitle = {Kornia},
	url = {http://arxiv.org/abs/1910.02190},
	abstract = {This work presents Kornia -- an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. The package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries.},
	urldate = {2020-02-29},
	journal = {arXiv:1910.02190 [cs]},
	author = {Riba, Edgar and Mishkin, Dmytro and Ponsa, Daniel and Rublee, Ethan and Bradski, Gary},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.02190},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/6XVALA4T/1910.html:text/html},
}

@inproceedings{shaw_mri_2019,
	title = {{MRI} k-{Space} {Motion} {Artefact} {Augmentation}: {Model} {Robustness} and {Task}-{Specific} {Uncertainty}},
	shorttitle = {{MRI} k-{Space} {Motion} {Artefact} {Augmentation}},
	url = {http://proceedings.mlr.press/v102/shaw19a.html},
	abstract = {Patient movement during the acquisition of magnetic resonance images (MRI) can cause unwanted image artefacts. These artefacts may affect the quality of diagnosis by clinicians and cause errors in ...},
	language = {en},
	urldate = {2020-02-29},
	booktitle = {International {Conference} on {Medical} {Imaging} with {Deep} {Learning}},
	author = {Shaw, Richard and Sudre, Carole and Ourselin, Sebastien and Cardoso, M. Jorge},
	month = may,
	year = {2019},
	pages = {427--436},
	file = {Snapshot:/home/fernando/Zotero/storage/4V8N7HI8/shaw19a.html:text/html},
}

@misc{christian_s_perone_peronemedicaltorch_2018,
	title = {perone/medicaltorch: {Release} v0.2},
	shorttitle = {perone/medicaltorch},
	url = {https://zenodo.org/record/1495335#.XlqwUZP7S8o},
	abstract = {This is the release v0.2 of medicaltorch. Please see the official documentation for more information and the changelog.},
	urldate = {2020-02-29},
	publisher = {Zenodo},
	author = {Christian S. Perone and cclauss and Elvis Saravia and Pedro Lemos Ballester and MohitTare},
	month = nov,
	year = {2018},
	doi = {10.5281/zenodo.1495335},
	file = {Zenodo Snapshot:/home/fernando/Zotero/storage/PV6KTMCH/1495335.html:text/html},
}

@article{pawlowski_dltk_2017,
	title = {{DLTK}: {State} of the {Art} {Reference} {Implementations} for {Deep} {Learning} on {Medical} {Images}},
	shorttitle = {{DLTK}},
	url = {http://arxiv.org/abs/1711.06853},
	abstract = {We present DLTK, a toolkit providing baseline implementations for efficient experimentation with deep learning methods on biomedical images. It builds on top of TensorFlow and its high modularity and easy-to-use examples allow for a low-threshold access to state-of-the-art implementations for typical medical imaging problems. A comparison of DLTK's reference implementations of popular network architectures for image segmentation demonstrates new top performance on the publicly available challenge data "Multi-Atlas Labeling Beyond the Cranial Vault". The average test Dice similarity coefficient of \$81.5\$ exceeds the previously best performing CNN (\$75.7\$) and the accuracy of the challenge winning method (\$79.0\$).},
	urldate = {2020-02-29},
	journal = {arXiv:1711.06853 [cs]},
	author = {Pawlowski, Nick and Ktena, Sofia Ira and Lee, Matthew C. H. and Kainz, Bernhard and Rueckert, Daniel and Glocker, Ben and Rajchl, Martin},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.06853},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/7YQBNUST/1711.html:text/html},
}

@article{gibson_niftynet_2018,
	title = {{NiftyNet}: a deep-learning platform for medical imaging},
	volume = {158},
	issn = {0169-2607},
	shorttitle = {{NiftyNet}},
	url = {http://www.sciencedirect.com/science/article/pii/S0169260717311823},
	doi = {10.1016/j.cmpb.2018.01.025},
	abstract = {Background and objectives
Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. Established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this domain of application requires substantial implementation effort. Consequently, there has been substantial duplication of effort and incompatible infrastructure developed across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon.
Methods
The NiftyNet infrastructure provides a modular deep-learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications. Components of the NiftyNet pipeline including data loading, data augmentation, network architectures, loss functions and evaluation metrics are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted intervention. NiftyNet is built on the TensorFlow framework and supports features such as TensorBoard visualization of 2D and 3D images and computational graphs by default.
Results
We present three illustrative medical image analysis applications built using NiftyNet infrastructure: (1) segmentation of multiple abdominal organs from computed tomography; (2) image regression to predict computed tomography attenuation maps from brain magnetic resonance images; and (3) generation of simulated ultrasound images for specified anatomical poses.
Conclusions
The NiftyNet infrastructure enables researchers to rapidly develop and distribute deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications.},
	language = {en},
	urldate = {2020-02-29},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Gibson, Eli and Li, Wenqi and Sudre, Carole and Fidon, Lucas and Shakir, Dzhoshkun I. and Wang, Guotai and Eaton-Rosen, Zach and Gray, Robert and Doel, Tom and Hu, Yipeng and Whyntie, Tom and Nachev, Parashkev and Modat, Marc and Barratt, Dean C. and Ourselin, S{\'e}bastien and Cardoso, M. Jorge and Vercauteren, Tom},
	month = may,
	year = {2018},
	keywords = {Deep learning, Convolutional neural network, Generative adversarial network, Image regression, Medical image analysis, Segmentation},
	pages = {113--122},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/84GGUFUM/S0169260717311823.html:text/html},
}

@article{bloice_biomedical_2019,
	title = {Biomedical image augmentation using {Augmentor}},
	volume = {35},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/35/21/4522/5466454},
	doi = {10.1093/bioinformatics/btz259},
	abstract = {AbstractMotivation.  Image augmentation is a frequently used technique in computer vision and has been seeing increased interest since the popularity of deep le},
	language = {en},
	number = {21},
	urldate = {2020-02-29},
	journal = {Bioinformatics},
	author = {Bloice, Marcus D. and Roth, Peter M. and Holzinger, Andreas},
	month = nov,
	year = {2019},
	pages = {4522--4524},
	file = {Snapshot:/home/fernando/Zotero/storage/4LE4BDPG/5466454.html:text/html},
}

@article{kamnitsas_efficient_2017,
	title = {Efficient {Multi}-{Scale} {3D} {CNN} with {Fully} {Connected} {CRF} for {Accurate} {Brain} {Lesion} {Segmentation}},
	volume = {36},
	issn = {13618415},
	doi = {10.1016/j.media.2016.10.004},
	abstract = {We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumors, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.},
	urldate = {2020-02-26},
	journal = {Medical Image Analysis},
	author = {Kamnitsas, Konstantinos and Ledig, Christian and Newcombe, Virginia F. J. and Simpson, Joanna P. and Kane, Andrew D. and Menon, David K. and Rueckert, Daniel and Glocker, Ben},
	month = feb,
	year = {2017},
	note = {arXiv: 1603.05959},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	pages = {61--78},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/5VSGBL5L/1603.html:text/html},
}

@article{fiest_prevalence_2017,
	title = {Prevalence and incidence of epilepsy: {A} systematic review and meta-analysis of international studies},
	volume = {88},
	issn = {1526-632X},
	shorttitle = {Prevalence and incidence of epilepsy},
	doi = {10.1212/WNL.0000000000003509},
	abstract = {OBJECTIVE: To review population-based studies of the prevalence and incidence of epilepsy worldwide and use meta-analytic techniques to explore factors that may explain heterogeneity between estimates.
METHODS: The Preferred Reporting Items for Systematic Reviews and Meta-Analyses standards were followed. We searched MEDLINE and EMBASE for articles published on the prevalence or incidence of epilepsy since 1985. Abstract, full-text review, and data abstraction were conducted in duplicate. Meta-analyses and meta-regressions were used to explore the association between prevalence or incidence, age group, sex, country level income, and study quality.
RESULTS: A total of 222 studies were included (197 on prevalence, 48 on incidence). The point prevalence of active epilepsy was 6.38 per 1,000 persons (95\% confidence interval [95\% CI] 5.57-7.30), while the lifetime prevalence was 7.60 per 1,000 persons (95\% CI 6.17-9.38). The annual cumulative incidence of epilepsy was 67.77 per 100,000 persons (95\% CI 56.69-81.03) while the incidence rate was 61.44 per 100,000 person-years (95\% CI 50.75-74.38). The prevalence of epilepsy did not differ by age group, sex, or study quality. The active annual period prevalence, lifetime prevalence, and incidence rate of epilepsy were higher in low to middle income countries. Epilepsies of unknown etiology and those with generalized seizures had the highest prevalence.
CONCLUSIONS: This study provides a comprehensive synthesis of the prevalence and incidence of epilepsy from published international studies and offers insight into factors that contribute to heterogeneity between estimates. Significant gaps (e.g., lack of incidence studies, stratification by age groups) were identified. Standardized reporting of future epidemiologic studies of epilepsy is needed.},
	language = {eng},
	number = {3},
	journal = {Neurology},
	author = {Fiest, Kirsten M. and Sauro, Khara M. and Wiebe, Samuel and Patten, Scott B. and Kwon, Churl-Su and Dykeman, Jonathan and Pringsheim, Tamara and Lorenzetti, Diane L. and Jett{\'e}, Nathalie},
	month = jan,
	year = {2017},
	pmid = {27986877},
	pmcid = {PMC5272794},
	keywords = {Humans, Female, Male, Databases, Bibliographic, Epilepsy, Incidence, International Cooperation, Prevalence},
	pages = {296--303},
}

@article{gudbjartsson_rician_1995,
	title = {The {Rician} {Distribution} of {Noisy} {MRI} {Data}},
	volume = {34},
	issn = {0740-3194},
	abstract = {The image intensity in magnetic resonance magnitude images in the presence of noise is shown to be governed by a Rician distribution. Low signal intensities (SNR {\textless} 2) are therefore biased due to the noise. It is shown how the underlying noise can be estimated from the images and a simple correction scheme is provided to reduce the bias. The noise characteristics in phase images are also studied and shown to be very different from those of the magnitude images. Common to both, however, is that the noise distributions are nearly Gaussian for SNR larger than two.},
	number = {6},
	urldate = {2020-02-22},
	journal = {Magnetic resonance in medicine},
	author = {Gudbjartsson, H{\'a}kon and Patz, Samuel},
	month = dec,
	year = {1995},
	pmid = {8598820},
	pmcid = {PMC2254141},
	pages = {910--914},
}

@article{perlin_improving_2002,
	title = {Improving noise},
	volume = {21},
	issn = {0730-0301},
	doi = {10.1145/566654.566636},
	abstract = {Two deficiencies in the original Noise algorithm are corrected: second order interpolation discontinuity and unoptimal gradient computation. With these defects corrected, Noise both looks better and runs faster. The latter change also makes it easier to define a uniform mathematical reference standard.},
	number = {3},
	urldate = {2020-01-15},
	journal = {ACM Transactions on Graphics (TOG)},
	author = {Perlin, Ken},
	month = jul,
	year = {2002},
	keywords = {procedural texture},
	pages = {681--682},
}

@article{meng_where_2018,
	title = {Where and when to look? {Spatial}-temporal attention for action recognition in videos},
	shorttitle = {Where and when to look?},
	url = {https://openreview.net/forum?id=BkesJ3R9YX},
	abstract = {Inspired by the observation that humans are able to process videos efficiently by only paying attention when and where it is needed, we propose a novel spatial-temporal attention mechanism for...},
	urldate = {2020-02-06},
	author = {Meng, Lili and Zhao, Bo and Chang, Bo and Huang, Gao and Tung, Frederick and Sigal, Leonid},
	month = sep,
	year = {2018},
	file = {Snapshot:/home/fernando/Zotero/storage/RIWKN49K/forum.html:text/html},
}

@article{meng_interpretable_2019,
	title = {Interpretable {Spatio}-temporal {Attention} for {Video} {Action} {Recognition}},
	url = {http://arxiv.org/abs/1810.04511},
	abstract = {Inspired by the observation that humans are able to process videos efficiently by only paying attention where and when it is needed, we propose an interpretable and easy plug-in spatial-temporal attention mechanism for video action recognition. For spatial attention, we learn a saliency mask to allow the model to focus on the most salient parts of the feature maps. For temporal attention, we employ a convolutional LSTM based attention mechanism to identify the most relevant frames from an input video. Further, we propose a set of regularizers to ensure that our attention mechanism attends to coherent regions in space and time. Our model not only improves video action recognition accuracy, but also localizes discriminative regions both spatially and temporally, despite being trained in a weakly-supervised manner with only classification labels (no bounding box labels or time frame temporal labels). We evaluate our approach on several public video action recognition datasets with ablation studies. Furthermore, we quantitatively and qualitatively evaluate our model's ability to localize discriminative regions spatially and critical frames temporally. Experimental results demonstrate the efficacy of our approach, showing superior or comparable accuracy with the state-of-the-art methods while increasing model interpretability.},
	urldate = {2020-02-06},
	journal = {arXiv:1810.04511 [cs, stat]},
	author = {Meng, Lili and Zhao, Bo and Chang, Bo and Huang, Gao and Sun, Wei and Tung, Frederich and Sigal, Leonid},
	month = jun,
	year = {2019},
	note = {arXiv: 1810.04511},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/5XEWA2N3/1810.html:text/html},
}

@article{ripolles_analysis_2012,
	title = {Analysis of automated methods for spatial normalization of lesioned brains},
	volume = {60},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811912001115},
	doi = {10.1016/j.neuroimage.2012.01.094},
	abstract = {Normalization of brain images is a crucial step in MRI data analysis, especially when dealing with abnormal brains. Although cost function masking (CFM) appears to successfully solve this problem and seems to be necessary for patients with chronic stroke lesions, this procedure is very time consuming. The present study sought to find viable, fully automated alternatives to cost function masking, such as Automatic Lesion Identification (ALI) and Diffeomorphic Anatomical Registration using Exponentiated Lie algebra (DARTEL). It also sought to quantitatively assess, for the first time, Symmetrical Normalization (SyN) with constrained cost function masking. The second aim of this study was to investigate the normalization process in a group of drug-resistant epileptic patients with large resected regions (temporal lobe and amygdala) and in a group of stroke patients. A dataset of 500 artificially generated lesions was created using ten patients with brain-resected regions (temporal lobectomy), ten stroke patients and twenty five-healthy subjects. The results indicated that although a fully automated method such as DARTEL using New Segment with an extra prior (the mean of the white matter and cerebro-spinal fluid) obtained the most accurate normalization in both patient groups, it produced a shrinkage in lesion volume when compared to Unified Segmentation with CFM. Taken together, these findings suggest that further research is needed in order to improve automatic normalization processes in brains with large lesions and to completely abandon manual, time consuming normalization methods.},
	language = {en},
	number = {2},
	urldate = {2020-02-06},
	journal = {NeuroImage},
	author = {Ripoll{\'e}s, P. and Marco-Pallar{\'e}s, J. and de Diego-Balaguer, R. and Mir{\'o}, J. and Falip, M. and Juncadella, M. and Rubio, F. and Rodriguez-Fornells, A.},
	month = apr,
	year = {2012},
	keywords = {Epilepsy, Cost function masking, Diffeomorphic, Normalization, Stroke, Unified Segmentation},
	pages = {1296--1306},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/FHCJNEJW/S1053811912001115.html:text/html},
}

@article{crinion_spatial_2007,
	title = {Spatial normalization of lesioned brains: {Performance} evaluation and impact on {fMRI} analyses},
	volume = {37},
	issn = {1053-8119},
	shorttitle = {Spatial normalization of lesioned brains},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3223520/},
	doi = {10.1016/j.neuroimage.2007.04.065},
	abstract = {A key component of group analyses of neuroimaging data is precise and valid spatial normalization (i.e., inter-subject image registration). When patients have structural brain lesions, such as a stroke, this process can be confounded by the lack of correspondence between the subject and standardized template images. Current procedures for dealing with this problem include regularizing the estimate of warping parameters used to match lesioned brains to the template, or {\textquotedblleft}cost function masking{\textquotedblright}; both these solutions have significant drawbacks. We report three experiments that identify the best spatial normalization for structurally damaged brains and establish whether differences among normalizations have a significant effect on inferences about functional activations. Our novel protocols evaluate the effects of different normalization solutions and can be applied easily to any neuroimaging study. This has important implications for users of both structural and functional imaging techniques in the study of patients with structural brain damage.},
	number = {3},
	urldate = {2020-02-06},
	journal = {Neuroimage},
	author = {Crinion, Jenny and Ashburner, John and Leff, Alex and Brett, Matthew and Price, Cathy and Friston, Karl},
	month = sep,
	year = {2007},
	pmid = {17616402},
	pmcid = {PMC3223520},
	pages = {866--875},
}

@article{alexandre_risk_2015,
	title = {Risk factors of postictal generalized {EEG} suppression in generalized convulsive seizures},
	volume = {85},
	issn = {1526-632X},
	doi = {10.1212/WNL.0000000000001949},
	abstract = {OBJECTIVE: To identify the clinical determinants of occurrence of postictal generalized EEG suppression (PGES) after generalized convulsive seizures (GCS).
METHODS: We reviewed the video-EEG recordings of 417 patients included in the REPO2MSE study, a multicenter prospective cohort study of patients with drug-resistant focal epilepsy. According to ictal semiology, we classified GCS into 3 types: tonic-clonic GCS with bilateral and symmetric tonic arm extension (type 1), clonic GCS without tonic arm extension or flexion (type 2), and GCS with unilateral or asymmetric tonic arm extension or flexion (type 3). Association between PGES and person-specific or seizure-specific variables was analyzed after correction for individual effects and the varying number of seizures.
RESULTS: A total of 99 GCS in 69 patients were included. Occurrence of PGES was independently associated with GCS type (p {\textless} 0.001) and lack of early administration of oxygen (p {\textless} 0.001). Odds ratio (OR) for GCS type 1 in comparison with GCS type 2 was 66.0 (95\% confidence interval [CI 5.4-801.6]). In GCS type 1, risk of PGES was significantly increased when the seizure occurred during sleep (OR 5.0, 95\% CI 1.2-20.9) and when oxygen was not administered early (OR 13.4, 95\% CI 3.2-55.9).
CONCLUSION: The risk of PGES dramatically varied as a function of GCS semiologic characteristics. Whatever the type of GCS, occurrence of PGES was prevented by early administration of oxygen.},
	language = {eng},
	number = {18},
	journal = {Neurology},
	author = {Alexandre, Veriano and Mercedes, Blanca and Valton, Luc and Maillard, Louis and Bartolomei, Fabrice and Szurhaj, William and Hirsch, Edouard and Marchal, C{\'e}cile and Chassoux, Francine and Petit, J{\'e}r{\^o}me and Crespel, Arielle and Nica, Anca and Navarro, Vincent and Kahane, Philippe and De Toffol, Bertrand and Thomas, Pierre and Rosenberg, Sarah and Denuelle, Marie and Jonas, Jacques and Ryvlin, Philippe and Rheims, Sylvain and {REPO2MSE study group}},
	month = nov,
	year = {2015},
	pmid = {26333799},
	keywords = {Humans, Adult, Female, Male, Brain Waves, Cohort Studies, Early Medical Intervention, Electroencephalography, Epilepsy, Generalized, Odds Ratio, Oxygen Inhalation Therapy, Prospective Studies, Risk Factors, Seizures, Video Recording},
	pages = {1598--1603},
}

@article{fisher_new_2017,
	title = {The {New} {Classification} of {Seizures} by the {International} {League} {Against} {Epilepsy} 2017},
	volume = {17},
	issn = {1534-6293},
	url = {https://doi.org/10.1007/s11910-017-0758-6},
	doi = {10.1007/s11910-017-0758-6},
	abstract = {Purpose of ReviewThis review presents the newly developed International League Against Epilepsy (ILAE) 2017 classification of seizure types.Recent FindingsThe fundamental distinction is between seizures that begin focally in one hemisphere of the brain, generalized onset seizures that apparently originate in both hemispheres, and seizures of unknown onset. Focal seizures optionally can be subclassified according to whether awareness (a surrogate marker for consciousness) is intact or impaired. The next level of classification for focal seizures is motor (with subgroups automatisms, atonic, clonic, epileptic spasms, hyperkinetic, myoclonic, tonic), non-motor (with subgroups autonomic, behavior arrest, cognitive, emotional, sensory), and focal to bilateral tonic-clonic. Generalized seizures are categorized as motor (tonic-clonic, clonic, tonic, myoclonic, myoclonic-tonic-clonic, myoclonic-atonic, atonic, epileptic spasms) and non-motor/absence (typical, atypical, myoclonic, eyelid myoclonia).SummaryThe classification allows new types of focal seizures and a few new generalized seizures, and clarifies terms used to name seizures.},
	language = {en},
	number = {6},
	urldate = {2020-01-31},
	journal = {Current Neurology and Neuroscience Reports},
	author = {Fisher, Robert S.},
	month = apr,
	year = {2017},
	keywords = {Epilepsy, Classification, Focal seizure, Generalized seizure, Seizure, Taxonomy},
	pages = {48},
}

@inproceedings{kuehne_hmdb_2011,
	title = {{HMDB}: {A} large video database for human motion recognition},
	shorttitle = {{HMDB}},
	doi = {10.1109/ICCV.2011.6126543},
	abstract = {With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	author = {Kuehne, H. and Jhuang, H. and Garrote, E. and Poggio, T. and Serre, T.},
	month = nov,
	year = {2011},
	note = {ISSN: 1550-5499},
	keywords = {Humans, action recognition databases, camera motion, Cameras, computer vision research, Databases, digitized movies, HMDB, human action datasets, human motion recognition, image categories, image motion analysis, large video database, Motion pictures, object recognition, occlusion, social networking (online), static image datasets, Training, video databases, video quality, viewpoint, Visualization, YouTube},
	pages = {2556--2563},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/AJ3SDESZ/6126543.html:text/html},
}

@article{feichtenhofer_convolutional_2016,
	title = {Convolutional {Two}-{Stream} {Network} {Fusion} for {Video} {Action} {Recognition}},
	url = {http://arxiv.org/abs/1604.06573},
	abstract = {Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters; (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy; finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.},
	urldate = {2020-01-31},
	journal = {arXiv:1604.06573 [cs]},
	author = {Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
	month = sep,
	year = {2016},
	note = {arXiv: 1604.06573},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/J5CJWR36/1604.html:text/html},
}

@article{donahue_long-term_2016,
	title = {Long-term {Recurrent} {Convolutional} {Networks} for {Visual} {Recognition} and {Description}},
	url = {http://arxiv.org/abs/1411.4389},
	abstract = {Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or "temporally deep", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are "doubly deep"' in that they can be compositional in spatial and temporal "layers". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.},
	urldate = {2020-01-31},
	journal = {arXiv:1411.4389 [cs]},
	author = {Donahue, Jeff and Hendricks, Lisa Anne and Rohrbach, Marcus and Venugopalan, Subhashini and Guadarrama, Sergio and Saenko, Kate and Darrell, Trevor},
	month = may,
	year = {2016},
	note = {arXiv: 1411.4389},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/XHPR8XKB/1411.html:text/html},
}

@inproceedings{karpathy_large-scale_2014,
	title = {Large-{Scale} {Video} {Classification} with {Convolutional} {Neural} {Networks}},
	doi = {10.1109/CVPR.2014.223},
	abstract = {Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3\% to 63.9\%), but only a surprisingly modest improvement compared to single-frame models (59.3\% to 60.9\%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3\% up from 43.9\%).},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
	month = jun,
	year = {2014},
	note = {ISSN: 1063-6919},
	keywords = {multimedia computing, image motion analysis, social networking (online), Training, action, classification, CNN, Computational modeling, Computer architecture, convolutional, convolutional neural networks, dataset, Feature extraction, feature-based baselines, image classification, image recognition problems, large-scale, local spatiotemporal information, network, neural, neural nets, recognition, Spatial resolution, spatiotemporal networks, spatiotemporal phenomena, sports, Streaming media, UCF-101 action recognition dataset, UCF-101 baseline model, video, video classification, video signal processing, YouTube videos},
	pages = {1725--1732},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/4GD7AQXH/6909619.html:text/html},
}

@article{poppe_survey_2010,
	title = {A survey on vision-based human action recognition},
	volume = {28},
	issn = {0262-8856},
	url = {http://www.sciencedirect.com/science/article/pii/S0262885609002704},
	doi = {10.1016/j.imavis.2009.11.014},
	abstract = {Vision-based human action recognition is the process of labeling image sequences with action labels. Robust solutions to this problem have applications in domains such as visual surveillance, video retrieval and human{\textendash}computer interaction. The task is challenging due to variations in motion performance, recording settings and inter-personal differences. In this survey, we explicitly address these challenges. We provide a detailed overview of current advances in the field. Image representations and the subsequent classification process are discussed separately to focus on the novelties of recent research. Moreover, we discuss limitations of the state of the art and outline promising directions of research.},
	language = {en},
	number = {6},
	urldate = {2020-01-31},
	journal = {Image and Vision Computing},
	author = {Poppe, Ronald},
	month = jun,
	year = {2010},
	keywords = {Action detection, Human action recognition, Motion analysis},
	pages = {976--990},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/ZHPML3W8/S0262885609002704.html:text/html},
}

@article{kong_human_2018,
	title = {Human {Action} {Recognition} and {Prediction}: {A} {Survey}},
	shorttitle = {Human {Action} {Recognition} and {Prediction}},
	url = {http://arxiv.org/abs/1806.11230},
	abstract = {Derived from rapid advances in computer vision and machine learning, video analysis tasks have been moving from inferring the present state to predicting the future state. Vision-based action recognition and prediction from videos are such tasks, where action recognition is to infer human actions (present state) based upon complete action executions, and action prediction to predict human actions (future state) based upon incomplete action executions. These two tasks have become particularly prevalent topics recently because of their explosively emerging real-world applications, such as visual surveillance, autonomous driving vehicle, entertainment, and video retrieval, etc. Many attempts have been devoted in the last a few decades in order to build a robust and effective framework for action recognition and prediction. In this paper, we survey the complete state-of-the-art techniques in the action recognition and prediction. Existing models, popular algorithms, technical difficulties, popular action databases, evaluation protocols, and promising future directions are also provided with systematic discussions.},
	urldate = {2020-01-31},
	journal = {arXiv:1806.11230 [cs]},
	author = {Kong, Yu and Fu, Yun},
	month = jul,
	year = {2018},
	note = {arXiv: 1806.11230},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/G8Q25XVL/1806.html:text/html},
}

@article{sharma_action_2016,
	title = {Action {Recognition} using {Visual} {Attention}},
	url = {http://arxiv.org/abs/1511.04119},
	abstract = {We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.},
	urldate = {2020-01-31},
	journal = {arXiv:1511.04119 [cs]},
	author = {Sharma, Shikhar and Kiros, Ryan and Salakhutdinov, Ruslan},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.04119},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/T8ZUXNMP/1511.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2020-01-30},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2020-01-30},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/T5H8E4XC/1810.html:text/html},
}

@inproceedings{ballard_modular_1987,
	address = {Seattle, Washington},
	series = {{AAAI}'87},
	title = {Modular learning in neural networks},
	isbn = {978-0-934613-42-2},
	abstract = {In the development of large-scale knowledge networks much recent progress has been inspired by connections to neurobiology. An important component of any "neural" network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must work for very large numbers of units. Studies of large-scale systems have so far been restricted to systems Without internal units (units With no direct connections to the input or output). Internal units are crucial to such systems as they are the means by which a system can encode high-order regularities (or invariants) that are Implicit in its inputs and outputs. Computer simulations of learning using internal units have been restricted to small-scale systems. This paper describes away of coupling autoassociative learning modules Into hierarchies that should greatly improve the performance of learning algorithms in large-scale systems. The Idea has been tested experimentally with positive results.},
	urldate = {2020-01-30},
	booktitle = {Proceedings of the sixth {National} conference on {Artificial} intelligence - {Volume} 1},
	publisher = {AAAI Press},
	author = {Ballard, Dana H.},
	month = jul,
	year = {1987},
	pages = {279--284},
}

@article{mann_test_1947,
	title = {On a {Test} of {Whether} one of {Two} {Random} {Variables} is {Stochastically} {Larger} than the {Other}},
	volume = {18},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177730491},
	doi = {10.1214/aoms/1177730491},
	abstract = {Let xxx and yyy be two random variables with continuous cumulative distribution functions fff and ggg. A statistic UUU depending on the relative ranks of the xxx's and yyy's is proposed for testing the hypothesis f=gf=gf = g. Wilcoxon proposed an equivalent test in the Biometrics Bulletin, December, 1945, but gave only a few points of the distribution of his statistic. Under the hypothesis f=gf=gf = g the probability of obtaining a given UUU in a sample of nx'snx'sn x's and my'smy'sm y's is the solution of a certain recurrence relation involving nnn and mmm. Using this recurrence relation tables have been computed giving the probability of UUU for samples up to n=m=8n=m=8n = m = 8. At this point the distribution is almost normal. From the recurrence relation explicit expressions for the mean, variance, and fourth moment are obtained. The 2rth moment is shown to have a certain form which enabled us to prove that the limit distribution is normal if m,nm,nm, n go to infinity in any arbitrary manner. The test is shown to be consistent with respect to the class of alternatives f(x){\textgreater}g(x)f(x){\textgreater}g(x)f(x) {\textgreater} g(x) for every xxx.},
	language = {EN},
	number = {1},
	urldate = {2020-01-29},
	journal = {The Annals of Mathematical Statistics},
	author = {Mann, H. B. and Whitney, D. R.},
	month = mar,
	year = {1947},
	mrnumber = {MR22058},
	zmnumber = {0041.26103},
	pages = {50--60},
	file = {Snapshot:/home/fernando/Zotero/storage/8BQW6LNP/1177730491.html:text/html},
}

@article{weiss_survey_2016,
	title = {A survey of transfer learning},
	volume = {3},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-016-0043-6},
	doi = {10.1186/s40537-016-0043-6},
	abstract = {Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.},
	number = {1},
	urldate = {2020-01-28},
	journal = {Journal of Big Data},
	author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
	month = may,
	year = {2016},
	pages = {9},
	file = {Snapshot:/home/fernando/Zotero/storage/548ABQ2P/s40537-016-0043-6.html:text/html},
}

@article{kruskal_use_1952,
	title = {Use of {Ranks} in {One}-{Criterion} {Variance} {Analysis}},
	volume = {47},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2280779},
	doi = {10.2307/2280779},
	abstract = {Given C samples, with n$_{\textrm{i}}$ observations in the ith sample, a test of the hypothesis that the samples are from the same population may be made by ranking the observations from from 1 to {\textless}tex-math{\textgreater}\${\textbackslash}Sum n\_i\${\textless}/tex-math{\textgreater} (giving each observation in a group of ties the mean of the ranks tied for), finding the C sums of ranks, and computing a statistic H. Under the stated hypothesis, H is distributed approximately as $\chi$$^{\textrm{2}}$(C - 1), unless the samples are too small, in which case special approximations or exact tables are provided. One of the most important applications of the test is in detecting differences among the population means.},
	number = {260},
	urldate = {2020-01-27},
	journal = {Journal of the American Statistical Association},
	author = {Kruskal, William H. and Wallis, W. Allen},
	year = {1952},
	pages = {583--621},
}

@article{wilcoxon_individual_1945,
	title = {Individual {Comparisons} by {Ranking} {Methods}},
	volume = {1},
	issn = {0099-4987},
	url = {https://www.jstor.org/stable/3001968},
	doi = {10.2307/3001968},
	number = {6},
	urldate = {2020-01-27},
	journal = {Biometrics Bulletin},
	author = {Wilcoxon, Frank},
	year = {1945},
	pages = {80--83},
}

@article{zou_statistical_2004,
	title = {Statistical {Validation} of {Image} {Segmentation} {Quality} {Based} on a {Spatial} {Overlap} {Index}},
	volume = {11},
	issn = {1076-6332},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1415224/},
	doi = {10.1016/S1076-6332(03)00671-8},
	abstract = {Rationale and Objectives
To examine a statistical validation method based on the spatial overlap between two sets of segmentations of the same anatomy.

Materials and Methods
The Dice similarity coefficient (DSC) was used as a statistical validation metric to evaluate the performance of both the reproducibility of manual segmentations and the spatial overlap accuracy of automated probabilistic fractional segmentation of MR images, illustrated on two clinical examples. Example 1: 10 consecutive cases of prostate brachytherapy patients underwent both preoperative 1.5T and intraoperative 0.5T MR imaging. For each case, 5 repeated manual segmentations of the prostate peripheral zone were performed separately on preoperative and on intraoperative images. Example 2: A semi-automated probabilistic fractional segmentation algorithm was applied to MR imaging of 9 cases with 3 types of brain tumors. DSC values were computed and logit-transformed values were compared in the mean with the analysis of variance (ANOVA).

Results
Example 1: The mean DSCs of 0.883 (range, 0.876{\textendash}0.893) with 1.5T preoperative MRI and 0.838 (range, 0.819{\textendash}0.852) with 0.5T intraoperative MRI (P {\textless} .001) were within and at the margin of the range of good reproducibility, respectively. Example 2: Wide ranges of DSC were observed in brain tumor segmentations: Meningiomas (0.519{\textendash}0.893), astrocytomas (0.487{\textendash}0.972), and other mixed gliomas (0.490{\textendash}0.899).

Conclusion
The DSC value is a simple and useful summary measure of spatial overlap, which can be applied to studies of reproducibility and accuracy in image segmentation. We observed generally satisfactory but variable validation results in two clinical applications. This metric may be adapted for similar validation tasks.},
	number = {2},
	urldate = {2020-01-26},
	journal = {Academic radiology},
	author = {Zou, Kelly H. and Warfield, Simon K. and Bharatha, Aditya and Tempany, Clare M.C. and Kaus, Michael R. and Haker, Steven J. and Wells, William M. and Jolesz, Ferenc A. and Kikinis, Ron},
	month = feb,
	year = {2004},
	pmid = {14974593},
	pmcid = {PMC1415224},
	pages = {178--189},
}

@article{zijdenbos_morphometric_1994,
	title = {Morphometric analysis of white matter lesions in {MR} images: method and validation},
	volume = {13},
	issn = {0278-0062},
	shorttitle = {Morphometric analysis of white matter lesions in {MR} images},
	doi = {10.1109/42.363096},
	abstract = {The analysis of MR images is evolving from qualitative to quantitative. More and more, the question asked by clinicians is how much and where, rather than a simple statement on the presence or absence of abnormalities. The authors present a study in which the results obtained with a semiautomatic, multispectral segmentation technique are quantitatively compared to manually delineated regions. The core of the semiautomatic image analysis system is a supervised artificial neural network classifier augmented with dedicated preand postprocessing algorithms, including anisotropic noise filtering and a surface-fitting method for the correction of spatial intensity variations. The study was focused on the quantitation of white matter lesions in the human brain. A total of 36 images from six brain volumes was analyzed twice by each of two operators, under supervision of a neuroradiologist. Both the intra- and interrater variability of the methods were studied in terms of the average tissue area detected per slice, the correlation coefficients between area measurements, and a measure of similarity derived from the kappa statistic. The results indicate that, compared to a manual method, the use of the semiautomatic technique not only facilitates the analysis of the images, but also has similar or lower intra- and interrater variabilities.},
	language = {eng},
	number = {4},
	journal = {IEEE transactions on medical imaging},
	author = {Zijdenbos, A. P. and Dawant, B. M. and Margolin, R. A. and Palmer, A. C.},
	year = {1994},
	pmid = {18218550},
	pages = {716--724},
}

@article{vilas-boas_movement_2016,
	title = {Movement {Quantification} in {Neurological} {Diseases}: {Methods} and {Applications}},
	volume = {9},
	issn = {1941-1189},
	shorttitle = {Movement {Quantification} in {Neurological} {Diseases}},
	doi = {10.1109/RBME.2016.2543683},
	abstract = {The movement of the human body offers neurologists important clues for the diagnosis and follow-up of many neurological diseases. The typical diagnosis approach is accomplished through simple observation of movements of interest (MOI) associated with a specific neurological disease. This approach is highly subjective because it is mainly based on qualitative evaluation of MOIs. Quantitative movement techniques are then obvious diagnosis-aid systems to approach these cases. Nevertheless, the use of motion quantification techniques in these pathologies is still relatively rare. In this paper, we intend to review this area and provide a clear picture of the current state of the art, both in the methods used and their applications to the main movement-related neurological diseases. We approach some historic aspects and the current state of the motion capture techniques and present the results of a survey to the literature that includes 82 papers, since 2006, covering the usage of these techniques in neurological diseases. Furthermore, we discuss the pros and cons of using quantitative approaches in these clinical scenarios. Finally, we present some conclusions and discuss the trends we foresee for the future.},
	language = {eng},
	journal = {IEEE reviews in biomedical engineering},
	author = {Vilas-Boas, Maria do Carmo and Cunha, Joao Paulo Silva},
	year = {2016},
	pmid = {27008673},
	keywords = {Humans, Movement, Nervous System Diseases},
	pages = {15--31},
}

@article{pediaditis_vision-based_2012,
	title = {Vision-based motion detection, analysis and recognition of epileptic seizures--a systematic review},
	volume = {108},
	issn = {1872-7565},
	doi = {10.1016/j.cmpb.2012.08.005},
	abstract = {The analysis of human motion from video has been the object of interest for many application areas, these including surveillance, control, biomedical analysis, video annotation etc. This paper addresses the advances within this topic in relation to epilepsy, a domain where human motion is with no doubt one of the most important elements of a patient's clinical image. It describes recent achievements in vision-based detection, analysis and recognition of human motion in epilepsy for marker-based and marker-free systems. An overview of motion-characterizing features extracted so far is presented separately. The objective is to gain existing knowledge in this field and set the route marks for the future development of an integrated decision support system for epilepsy diagnosis and disease management based on automated video analysis. This review revealed that the quantification of motion patterns of selected epileptic seizures has been studied thoroughly while the recognition of seizures is currently in its beginnings, but however feasible. Moreover, only a limited set of seizure types have been analyzed so far, indicating that a holistic approach addressing all epileptic syndromes is still missing.},
	language = {eng},
	number = {3},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Pediaditis, Matthew and Tsiknakis, Manolis and Leitgeb, Norbert},
	month = dec,
	year = {2012},
	pmid = {22954620},
	keywords = {Humans, Epilepsy, Motion, Vision, Ocular},
	pages = {1133--1148},
}

@article{dagostino_tests_1973,
	title = {Tests for {Departure} from {Normality}. {Empirical} {Results} for the {Distributions} of b2 and ? b1},
	volume = {60},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2335012},
	doi = {10.2307/2335012},
	abstract = {This paper is a preliminary to a detailed survey of the relative powers of a number of omnibus and directional tests of nonnormality. The probability integrals of ? b1 and b2, the standardized third and fourth moment statistics, are found for random samples from a normal distribution. Main attention is given to b2. Extensive computer simulation and curve fitting have been used to provide charts of probability levels out to the 0.1\% point, for 20 ? n ? 200. For ? b1, the parameters of Johnson's symmetrical SU approximation are tabled for values of n between 8 and 1000. An illustration is given of two `omnibus' tests applying the charts and table, involving the joint use of ? b1 and b2.},
	number = {3},
	urldate = {2020-01-26},
	journal = {Biometrika},
	author = {D'Agostino, Ralph and Pearson, E. S.},
	year = {1973},
	pages = {613--622},
}

@article{dagostino_omnibus_1971,
	title = {An {Omnibus} {Test} of {Normality} for {Moderate} and {Large} {Size} {Samples}},
	volume = {58},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2334522},
	doi = {10.2307/2334522},
	abstract = {We present a test of normality based on a statistic D which is up to a constant the ratio of Downton's linear unbiased estimator of the population standard deviation to the sample standard deviation. For the usual levels of significance Monte Carlo simulations indicate that Cornish-Fisher expansions adequately approximate the null distribution of D if the sample size is 50 or more. The test is an omnibus test, being appropriate to detect deviations from normality due either to skewness or kurtosis. Simulation results of powers for various alternatives when the sample size is 50 indicate that the test compares favourably with the Shapiro-Wilk W test, ? b1, b2 and the ratio of range to standard deviation.},
	number = {2},
	urldate = {2020-01-26},
	journal = {Biometrika},
	author = {D'Agostino, Ralph B.},
	year = {1971},
	pages = {341--348},
}

@article{memarian_multimodal_2015,
	title = {Multimodal data and machine learning for surgery outcome prediction in complicated cases of mesial temporal lobe epilepsy},
	volume = {64},
	issn = {1879-0534},
	doi = {10.1016/j.compbiomed.2015.06.008},
	abstract = {BACKGROUND: This study sought to predict postsurgical seizure freedom from pre-operative diagnostic test results and clinical information using a rapid automated approach, based on supervised learning methods in patients with drug-resistant focal seizures suspected to begin in temporal lobe.
METHOD: We applied machine learning, specifically a combination of mutual information-based feature selection and supervised learning classifiers on multimodal data, to predict surgery outcome retrospectively in 20 presurgical patients (13 female; mean age{\textpm}SD, in years 33{\textpm}9.7 for females, and 35.3{\textpm}9.4 for males) who were diagnosed with mesial temporal lobe epilepsy (MTLE) and subsequently underwent standard anteromesial temporal lobectomy. The main advantage of the present work over previous studies is the inclusion of the extent of ipsilateral neocortical gray matter atrophy and spatiotemporal properties of depth electrode-recorded seizures as training features for individual patient surgery planning.
RESULTS: A maximum relevance minimum redundancy (mRMR) feature selector identified the following features as the most informative predictors of postsurgical seizure freedom in this study's sample of patients: family history of epilepsy, ictal EEG onset pattern (positive correlation with seizure freedom), MRI-based gray matter thickness reduction in the hemisphere ipsilateral to seizure onset, proportion of seizures that first appeared in ipsilateral amygdala to total seizures, age, epilepsy duration, delay in the spread of ipsilateral ictal discharges from site of onset, gender, and number of electrode contacts at seizure onset (negative correlation with seizure freedom). Using these features in combination with a least square support vector machine (LS-SVM) classifier compared to other commonly used classifiers resulted in very high surgical outcome prediction accuracy (95\%).
CONCLUSIONS: Supervised machine learning using multimodal compared to unimodal data accurately predicted postsurgical outcome in patients with atypical MTLE.},
	language = {eng},
	journal = {Computers in Biology and Medicine},
	author = {Memarian, Negar and Kim, Sally and Dewar, Sandra and Engel, Jerome and Staba, Richard J.},
	month = sep,
	year = {2015},
	pmid = {26149291},
	pmcid = {PMC4554822},
	keywords = {Humans, Magnetic Resonance Imaging, Adult, Female, Machine Learning, Male, Young Adult, Electrocorticography, Epilepsy, Temporal Lobe, Mesial temporal epilepsy, Mutual information, Predictive Value of Tests, Prognosis, Retrospective Studies, Signal Processing, Computer-Assisted, Supervised learning, Surgical outcome prediction, Treatment Outcome},
	pages = {67--78},
}

@article{bernhardt_magnetic_2015,
	title = {Magnetic resonance imaging pattern learning in temporal lobe epilepsy: classification and prognostics},
	volume = {77},
	issn = {1531-8249},
	shorttitle = {Magnetic resonance imaging pattern learning in temporal lobe epilepsy},
	doi = {10.1002/ana.24341},
	abstract = {OBJECTIVE: In temporal lobe epilepsy (TLE), although hippocampal atrophy lateralizes the focus, the value of magnetic resonance imaging (MRI) to predict postsurgical outcome is rather modest. Prediction solely based on the hippocampus may be hampered by widespread mesiotemporal structural damage shown by advanced imaging. Increasingly complex and high-dimensional representation of MRI metrics motivates a shift to machine learning to establish objective, data-driven criteria for pathogenic processes and prognosis.
METHODS: We applied clustering to 114 consecutive unilateral TLE patients using 1.5T MRI profiles derived from surface morphology of hippocampus, amygdala, and entorhinal cortex. To evaluate the diagnostic validity of the classification, we assessed its yield to predict outcome in 79 surgically treated patients. Reproducibility of outcome prediction was assessed in an independent cohort of 27 patients evaluated on 3.0T MRI.
RESULTS: Four similarly sized classes partitioned our cohort; in all, alterations spanned over the 3 mesiotemporal structures. Compared to 46 controls, TLE-I showed marked bilateral atrophy; in TLE-II atrophy was ipsilateral; TLE-III showed mild bilateral atrophy; whereas TLE-IV showed hypertrophy. Classes differed with regard to histopathology and freedom from seizures. Classwise surface-based classifiers accurately predicted outcome in 92 {\textpm} 1\% of patients, outperforming conventional volumetry. Predictors of relapse were distributed bilaterally across structures. Prediction accuracy was similarly high in the independent cohort (96\%), supporting generalizability.
INTERPRETATION: We provide a novel description of individual variability across the TLE spectrum. Class membership was associated with distinct patterns of damage and outcome predictors that did not spatially overlap, emphasizing the ability of machine learning to disentangle the differential contribution of morphology to patient phenotypes, ultimately refining the prognosis of epilepsy surgery.},
	language = {eng},
	number = {3},
	journal = {Annals of Neurology},
	author = {Bernhardt, Boris C. and Hong, Seok-Jun and Bernasconi, Andrea and Bernasconi, Neda},
	month = mar,
	year = {2015},
	pmid = {25546153},
	keywords = {Humans, Image Processing, Computer-Assisted, Magnetic Resonance Imaging, Adolescent, Adult, Female, Male, Middle Aged, Young Adult, Epilepsy, Temporal Lobe, Prognosis, Amygdala, Artificial Intelligence, Atrophy, Entorhinal Cortex, Hippocampus, Recurrence},
	pages = {436--446},
}

@article{kassahun_automatic_2014,
	title = {Automatic classification of epilepsy types using ontology-based and genetics-based machine learning},
	volume = {61},
	issn = {1873-2860},
	doi = {10.1016/j.artmed.2014.03.001},
	abstract = {OBJECTIVES: In the presurgical analysis for drug-resistant focal epilepsies, the definition of the epileptogenic zone, which is the cortical area where ictal discharges originate, is usually carried out by using clinical, electrophysiological and neuroimaging data analysis. Clinical evaluation is based on the visual detection of symptoms during epileptic seizures. This work aims at developing a fully automatic classifier of epileptic types and their localization using ictal symptoms and machine learning methods.
METHODS: We present the results achieved by using two machine learning methods. The first is an ontology-based classification that can directly incorporate human knowledge, while the second is a genetics-based data mining algorithm that learns or extracts the domain knowledge from medical data in implicit form.
RESULTS: The developed methods are tested on a clinical dataset of 129 patients. The performance of the methods is measured against the performance of seven clinicians, whose level of expertise is high/very high, in classifying two epilepsy types: temporal lobe epilepsy and extra-temporal lobe epilepsy. When comparing the performance of the algorithms with that of a single clinician, who is one of the seven clinicians, the algorithms show a slightly better performance than the clinician on three test sets generated randomly from 99 patients out of the 129 patients. The accuracy obtained for the two methods and the clinician is as follows: first test set 65.6\% and 75\% for the methods and 56.3\% for the clinician, second test set 66.7\% and 76.2\% for the methods and 61.9\% for the clinician, and third test set 77.8\% for the methods and the clinician. When compared with the performance of the whole population of clinicians on the rest 30 patients out of the 129 patients, where the patients were selected by the clinicians themselves, the mean accuracy of the methods (60\%) is slightly worse than the mean accuracy of the clinicians (61.6\%). Results show that the methods perform at the level of experienced clinicians, when both the methods and the clinicians use the same information.
CONCLUSION: Our results demonstrate that the developed methods form important ingredients for realizing a fully automatic classification of epilepsy types and can contribute to the definition of signs that are most important for the classification.},
	language = {eng},
	number = {2},
	journal = {Artificial Intelligence in Medicine},
	author = {Kassahun, Yohannes and Perrone, Roberta and De Momi, Elena and Bergh{\"o}fer, Elmar and Tassi, Laura and Canevini, Maria Paola and Spreafico, Roberto and Ferrigno, Giancarlo and Kirchner, Frank},
	month = jun,
	year = {2014},
	pmid = {24743020},
	keywords = {Algorithms, Humans, Electroencephalography, Epilepsy, Temporal Lobe, Artificial Intelligence, Data Mining, Data mining (knowledge discovery) from medical data, Diagnosis, Computer-Assisted, Diagnosis, Differential, Epilepsies, Partial, Epileptogenic zone identification, Genetics-based classification, Ontology-based classification},
	pages = {79--88},
}

@article{isik_diagnosis_2012,
	title = {Diagnosis of epilepsy from electroencephalography signals using multilayer perceptron and {Elman} {Artificial} {Neural} {Networks} and {Wavelet} {Transform}},
	volume = {36},
	issn = {0148-5598},
	doi = {10.1007/s10916-010-9440-0},
	abstract = {In this study, it has been intended to perform an automatic classification of Electroencephalography (EEG) signals via Artificial Neural Networks (ANN) and to investigate these signals using Wavelet Transform (WT) for diagnosing epilepsy syndrome. EEG signals have been decomposed into frequency sub-bands using WT and a set of feature vectors which were extracted from the sub-bands. Dimensions of these feature vectors have been reduced via Principal Component Analysis (PCA) method and then classified as epileptic or healthy using Multilayer Perceptron (MLP) and ELMAN ANN. Performance evaluation of the used ANN models have been carried out by performing Receiver Operation Characteristic (ROC) analysis.},
	language = {eng},
	number = {1},
	journal = {Journal of Medical Systems},
	author = {I{\c s}ik, Hakan and Sezer, Esma},
	month = feb,
	year = {2012},
	pmid = {20703754},
	keywords = {Humans, Neural Networks, Computer, Epilepsy, Electroencephalography, Signal Processing, Computer-Assisted, Principal Component Analysis, ROC Curve, Wavelet Analysis},
	pages = {1--13},
}

@article{acharya_application_2015,
	title = {Application of entropies for automated diagnosis of epilepsy using {EEG} signals: {A} review},
	volume = {88},
	issn = {0950-7051},
	shorttitle = {Application of entropies for automated diagnosis of epilepsy using {EEG} signals},
	url = {http://www.sciencedirect.com/science/article/pii/S0950705115003081},
	doi = {10.1016/j.knosys.2015.08.004},
	abstract = {Epilepsy is the neurological disorder of the brain which is difficult to diagnose visually using Electroencephalogram (EEG) signals. Hence, an automated detection of epilepsy using EEG signals will be a useful tool in medical field. The automation of epilepsy detection using signal processing techniques such as wavelet transform and entropies may optimise the performance of the system. Many algorithms have been developed to diagnose the presence of seizure in the EEG signals. The entropy is a nonlinear parameter that reflects the complexity of the EEG signal. Many entropies have been used to differentiate normal, interictal and ictal EEG signals. This paper discusses various entropies used for an automated diagnosis of epilepsy using EEG signals. We have presented unique ranges for various entropies used to differentiate normal, interictal, and ictal EEG signals and also ranked them depending on the ability to discrimination ability of three classes. These entropies can be used to classify the different stages of epilepsy and can also be used for other biomedical applications.},
	language = {en},
	urldate = {2020-01-26},
	journal = {Knowledge-Based Systems},
	author = {Acharya, U. Rajendra and Fujita, H. and Sudarshan, Vidya K. and Bhat, Shreya and Koh, Joel E. W.},
	month = nov,
	year = {2015},
	keywords = {Epilepsy, EEG, Entropy, Fuzzy, HOS, Interictal},
	pages = {85--96},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/KPNVB989/S0950705115003081.html:text/html},
}

@article{van_de_vel_non-eeg_2016,
	title = {Non-{EEG} seizure detection systems and potential {SUDEP} prevention: {State} of the art: {Review} and update},
	volume = {41},
	issn = {1532-2688},
	shorttitle = {Non-{EEG} seizure detection systems and potential {SUDEP} prevention},
	doi = {10.1016/j.seizure.2016.07.012},
	abstract = {PURPOSE: Detection of, and alarming for epileptic seizures is increasingly demanded and researched. Our previous review article provided an overview of non-invasive, non-EEG (electro-encephalography) body signals that can be measured, along with corresponding methods, state of the art research, and commercially available systems. Three years later, many more studies and devices have emerged. Moreover, the boom of smart phones and tablets created a new market for seizure detection applications.
METHOD: We performed a thorough literature review and had contact with manufacturers of commercially available devices.
RESULTS: This review article gives an updated overview of body signals and methods for seizure detection, international research and (commercially) available systems and applications. Reported results of non-EEG based detection devices vary between 2.2\% and 100\% sensitivity and between 0 and 3.23 false detections per hour compared to the gold standard video-EEG, for seizures ranging from generalized to convulsive or non-convulsive focal seizures with or without loss of consciousness. It is particularly interesting to include monitoring of autonomic dysfunction, as this may be an important pathophysiological mechanism of SUDEP (sudden unexpected death in epilepsy), and of movement, as many seizures have a motor component.
CONCLUSION: Comparison of research results is difficult as studies focus on different seizure types, timing (night versus day) and patients (adult versus pediatric patients). Nevertheless, we are convinced that the most effective seizure detection systems are multimodal, combining for example detection methods for movement and heart rate, and that devices should especially take into account the user's seizure types and personal preferences.},
	language = {eng},
	journal = {Seizure},
	author = {Van de Vel, Anouk and Cuppens, Kris and Bonroy, Bert and Milosevic, Milica and Jansen, Katrien and Van Huffel, Sabine and Vanrumste, Bart and Cras, Patrick and Lagae, Lieven and Ceulemans, Berten},
	month = oct,
	year = {2016},
	pmid = {27567266},
	keywords = {Humans, Epilepsy, Electroencephalography, Alarm system, Death, Sudden, Non-EEG based seizure detection, Sudden unexpected death, SUDEP},
	pages = {141--153},
}

@article{ramgopal_seizure_2014,
	title = {Seizure detection, seizure prediction, and closed-loop warning systems in epilepsy},
	volume = {37},
	issn = {1525-5069},
	doi = {10.1016/j.yebeh.2014.06.023},
	abstract = {Nearly one-third of patients with epilepsy continue to have seizures despite optimal medication management. Systems employed to detect seizures may have the potential to improve outcomes in these patients by allowing more tailored therapies and might, additionally, have a role in accident and SUDEP prevention. Automated seizure detection and prediction require algorithms which employ feature computation and subsequent classification. Over the last few decades, methods have been developed to detect seizures utilizing scalp and intracranial EEG, electrocardiography, accelerometry and motion sensors, electrodermal activity, and audio/video captures. To date, it is unclear which combination of detection technologies yields the best results, and approaches may ultimately need to be individualized. This review presents an overview of seizure detection and related prediction methods and discusses their potential uses in closed-loop warning systems in epilepsy.},
	language = {eng},
	journal = {Epilepsy \& Behavior: E\&B},
	author = {Ramgopal, Sriram and Thome-Souza, Sigride and Jackson, Michele and Kadish, Navah Ester and S{\'a}nchez Fern{\'a}ndez, Iv{\'a}n and Klehm, Jacquelyn and Bosl, William and Reinsberger, Claus and Schachter, Steven and Loddenkemper, Tobias},
	month = aug,
	year = {2014},
	pmid = {25174001},
	keywords = {Algorithms, Humans, Markov Chains, Adolescent, Sensitivity and Specificity, Epilepsy, Electroencephalography, Seizures, Motion, Predictive Value of Tests, Accelerometry, Artificial neural network, Automated seizure detection, Child, Child, Preschool, Closed-loop methods, ECG-based seizure detection, EEG-based seizure detection, Electrocardiography, Fourier, Higher-order spectra, Markov modeling, Scalp, Support vector machine},
	pages = {291--307},
}

@article{ulate-campos_automated_2016,
	title = {Automated seizure detection systems and their effectiveness for each type of seizure},
	volume = {40},
	issn = {1532-2688},
	doi = {10.1016/j.seizure.2016.06.008},
	abstract = {Epilepsy affects almost 1\% of the population and most of the approximately 20-30\% of patients with refractory epilepsy have one or more seizures per month. Seizure detection devices allow an objective assessment of seizure frequency and a treatment tailored to the individual patient. A rapid recognition and treatment of seizures through closed-loop systems could potentially decrease morbidity and mortality in epilepsy. However, no single detection device can detect all seizure types. Therefore, the choice of a seizure detection device should consider the patient-specific seizure semiologies. This review of the literature evaluates seizure detection devices and their effectiveness for different seizure types. Our aim is to summarize current evidence, offer suggestions on how to select the most suitable seizure detection device for each patient and provide guidance to physicians, families and researchers when choosing or designing seizure detection devices. Further, this review will guide future prospective validation studies.},
	language = {eng},
	journal = {Seizure},
	author = {Ulate-Campos, A. and Coughlin, F. and Ga{\'i}nza-Lein, M. and Fern{\'a}ndez, I. S{\'a}nchez and Pearl, P. L. and Loddenkemper, T.},
	month = aug,
	year = {2016},
	pmid = {27376911},
	keywords = {Humans, Epilepsy, Seizures, SUDEP, Automated seizure detection, Intractable epilepsy, Neurophysiological Monitoring, Seizure alarms, Seizure prediction sensors},
	pages = {88--101},
}

@inproceedings{ahmedt-aristizabal_motion_2019,
	title = {Motion {Signatures} for the {Analysis} of {Seizure} {Evolution} in {Epilepsy}},
	doi = {10.1109/EMBC.2019.8857743},
	abstract = {In epilepsy, semiology refers to the study of patient behavior and movement, and their temporal evolution during epileptic seizures. Understanding semiology provides clues to the cerebral networks underpinning the epileptic episode and is a vital resource in the pre-surgical evaluation. Recent advances in video analytics have been helpful in capturing and quantifying epileptic seizures. Nevertheless, the automated representation of the evolution of semiology, as examined by neurologists, has not been appropriately investigated. From initial seizure symptoms until seizure termination, motion patterns of isolated clinical manifestations vary over time. Furthermore, epileptic seizures frequently evolve from one clinical manifestation to another, and their understanding cannot be overlooked during a presurgery evaluation. Here, we propose a system capable of computing motion signatures from videos of face and hand semiology to provide quantitative information on the motion, and the correlation between motions. Each signature is derived from a sparse saliency representation established by the magnitude of the optical flow field. The developed computer-aided tool provides a novel approach for physicians to analyze semiology as a flow of signals without interfering in the healthcare environment. We detect and quantify semiology using detectors based on deep learning and via a novel signature scheme, which is independent of the amount of data and seizure differences. The system reinforces the benefits of computer vision for non-obstructive clinical applications to quantify epileptic seizures recorded in real-life healthcare conditions.},
	booktitle = {2019 41st {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Ahmedt-Aristizabal, David and Sarfraz, M. Saquib and Denman, Simon and Nguyen, Kien and Fookes, Clinton and Dionisio, Sasha and Stiefelhagen, Rainer},
	month = jul,
	year = {2019},
	note = {ISSN: 1557-170X},
	keywords = {Deep learning, Epilepsy, image motion analysis, medical image processing, biomechanics, cerebral networks, computer-aided tool, deep learning, electroencephalography, epilepsy, epileptic episode, epileptic seizures, Face, hand semiology, health care, medical disorders, Monitoring, motion patterns, motion signatures, neurophysiology, optical flow field, seizure evolution, seizure symptoms, seizure termination, Semiotics, Tools},
	pages = {2099--2105},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/8MPBP8BL/8857743.html:text/html},
}

@article{ahmedt-aristizabal_neural_2019,
	title = {Neural {Memory} {Networks} for {Robust} {Classification} of {Seizure} {Type}},
	url = {http://arxiv.org/abs/1912.04968},
	abstract = {Classification of seizure type is a key step in the clinical process for evaluating an individual who presents with seizures. It determines the course of clinical diagnosis and treatment, and its impact stretches beyond the clinical domain to epilepsy research and the development of novel therapies. Automated identification of seizure type may facilitate understanding of the disease, and seizure detection and prediction has been the focus of recent research that has sought to exploit the benefits of machine learning and deep learning architectures. Nevertheless, there is not yet a definitive solution for automating the classification of seizure type, a task that must currently be performed by an expert epileptologist. Inspired by recent advances in neural memory networks (NMNs), we introduce a novel approach for the classification of seizure type using electrophysiological data. We first explore the performance of traditional deep learning techniques which use convolutional and recurrent neural networks, and enhance these architectures by using external memory modules with trainable neural plasticity. We show that our model achieves a state-of-the-art weighted F1 score of 0.945 for seizure type classification on the TUH EEG Seizure Corpus with the IBM TUSZ preprocessed data. This work highlights the potential of neural memory networks to support the field of epilepsy research, along with biomedical research and signal analysis more broadly.},
	urldate = {2020-01-26},
	journal = {arXiv:1912.04968 [cs, eess, stat]},
	author = {Ahmedt-Aristizabal, David and Fernando, Tharindu and Denman, Simon and Petersson, Lars and Aburn, Matthew J. and Fookes, Clinton},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.04968},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/UCWTE8TW/1912.html:text/html},
}

@article{ahmedt-aristizabal_understanding_2019,
	title = {Understanding {Patients}{\textquoteright} {Behavior}: {Vision}-{Based} {Analysis} of {Seizure} {Disorders}},
	volume = {23},
	issn = {2168-2208},
	shorttitle = {Understanding {Patients}{\textquoteright} {Behavior}},
	doi = {10.1109/JBHI.2019.2895855},
	abstract = {A substantial proportion of patients with functional neurological disorders (FND) are being incorrectly diagnosed with epilepsy because their semiology resembles that of epileptic seizures (ES). Misdiagnosis may lead to unnecessary treatment and its associated complications. Diagnostic errors often result from an overreliance on specific clinical features. Furthermore, the lack of electrophysiological changes in patients with FND can also be seen in some forms of epilepsy, making diagnosis extremely challenging. Therefore, understanding semiology is an essential step for differentiating between ES and FND. Existing sensor-based and marker-based systems require physical contact with the body and are vulnerable to clinical situations such as patient positions, illumination changes, and motion discontinuities. Computer vision and deep learning are advancing to overcome these limitations encountered in the assessment of diseases and patient monitoring; however, they have not been investigated for seizure disorder scenarios. Here, we propose and compare two marker-free deep learning models, a landmark-based and a region-based model, both of which are capable of distinguishing between seizures from video recordings. We quantify semiology by using either a fusion of reference points and flow fields, or through the complete analysis of the body. Average leave-one-subject-out cross-validation accuracies for the landmark-based and region-based approaches of 68.1\% and 79.6\% in our dataset collected from 35 patients, reveal the benefit of video analytics to support automated identification of semiology in the challenging conditions of a hospital setting.},
	number = {6},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Ahmedt-Aristizabal, David and Denman, Simon and Nguyen, Kien and Sridharan, Sridha and Dionisio, Sasha and Fookes, Clinton},
	month = nov,
	year = {2019},
	keywords = {Deep learning, Epilepsy, Cameras, Computer architecture, Feature extraction, Monitoring, Semiotics, Epileptic seizures, functional neurological disorder, quantitative movement analysis},
	pages = {2583--2591},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/4GR7KKGD/8629065.html:text/html},
}

@article{ahmedt-aristizabal_hierarchical_2018,
	title = {A hierarchical multimodal system for motion analysis in patients with~epilepsy},
	volume = {87},
	issn = {1525-5050},
	url = {http://www.sciencedirect.com/science/article/pii/S1525505018304311},
	doi = {10.1016/j.yebeh.2018.07.028},
	abstract = {During seizures, a myriad of clinical manifestations may occur. The analysis of these signs, known as seizure semiology, gives clues to the underlying cerebral networks involved. When patients with drug-resistant epilepsy are monitored to assess their suitability for epilepsy surgery, semiology is a vital component to the presurgical evaluation. Specific patterns of facial movements, head motions, limb posturing and articulations, and hand and finger automatisms may be useful in distinguishing between mesial temporal lobe epilepsy (MTLE) and extratemporal lobe epilepsy (ETLE). However, this analysis is time-consuming and dependent on clinical experience and training. Given this limitation, an automated analysis of semiological patterns, i.e., detection, quantification, and recognition of body movement patterns, has the potential to help increase the diagnostic precision of localization. While a few single modal quantitative approaches are available to assess seizure semiology, the automated quantification of patients' behavior across multiple modalities has seen limited advances in the literature. This is largely due to multiple complicated variables commonly encountered in the clinical setting, such as analyzing subtle physical movements when the patient is covered or room lighting is inadequate. Semiology encompasses the stepwise/temporal progression of signs that is reflective of the integration of connected neuronal networks. Thus, single signs in isolation are far less informative. Taking this into account, here, we describe a novel modular, hierarchical, multimodal system that aims to detect and quantify semiologic signs recorded in 2D monitoring videos. Our approach can jointly learn semiologic features from facial, body, and hand motions based on computer vision and deep learning architectures. A dataset collected from an Australian quaternary referral epilepsy unit analyzing 161 seizures arising from the temporal (n = 90) and extratemporal (n = 71) brain regions has been used in our system to quantitatively classify these types of epilepsy according to the semiology detected. A leave-one-subject-out (LOSO) cross-validation of semiological patterns from the face, body, and hands reached classification accuracies ranging between 12\% and 83.4\%, 41.2\% and 80.1\%, and 32.8\% and 69.3\%, respectively. The proposed hierarchical multimodal system is a potential stepping-stone towards developing a fully automated semiology analysis system to support the assessment of epilepsy.},
	language = {en},
	urldate = {2020-01-26},
	journal = {Epilepsy \& Behavior},
	author = {Ahmedt-Aristizabal, David and Fookes, Clinton and Denman, Simon and Nguyen, Kien and Fernando, Tharindu and Sridharan, Sridha and Dionisio, Sasha},
	month = oct,
	year = {2018},
	keywords = {Deep learning, Computer vision, Facial movements, Hand automatisms, Quantitative movement analysis, Seizure semiology},
	pages = {46--58},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/UIFZHAA4/S1525505018304311.html:text/html},
}

@inproceedings{ahmedt-aristizabal_deep_2018,
	title = {Deep {Motion} {Analysis} for {Epileptic} {Seizure} {Classification}},
	doi = {10.1109/EMBC.2018.8513031},
	abstract = {Visual motion clues such as facial expression and pose are natural semiology features which an epileptologist observes to identify epileptic seizures. However, these cues have not been effectively exploited for automatic detection due to the diverse variations in seizure appearance within and between patients. Here we present a multi-modal analysis approach to quantitatively classify patients with mesial temporal lobe (MTLE) and extra-temporal lobe (ETLE) epilepsy, relying on the fusion of facial expressions and pose dynamics. We propose a new deep learning approach that leverages recent advances in Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks to automatically extract spatiotemporal features from facial and pose semiology using recorded videos. A video dataset from 12 patients with MTLE and 6 patients with ETLEin an Australian hospital has been collected for experiments. Our experiments show that facial semiology and body movements can be effectively recognized and tracked, and that they provide useful evidence to identify the type of epilepsy. A multi-fold cross-validation of the fusion model exhibited an average test accuracy of 92.10\%, while a leave-one-subject-out cross-validation scheme, which is the first in the literature, achieves an accuracy of 58.49\%. The proposed approach is capable of modelling semiology features which effectively discriminate between seizures arising from temporal and extra-temporal brain areas. Our approach can be used as a virtual assistant, which will save time, improve patient safety and provide objective clinical analysis to assist with clinical decision making.},
	booktitle = {2018 40th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Ahmedt-Aristizabal, David and Nguyen, Kien and Denman, Simon and Sridharan, Sridha and Dionisio, Sasha and Fookes, Clinton},
	month = jul,
	year = {2018},
	note = {ISSN: 1557-170X},
	keywords = {Epilepsy, image motion analysis, convolutional neural networks, Feature extraction, video signal processing, brain, feature extraction, medical image processing, epileptic seizures, Face, medical disorders, neurophysiology, Semiotics, Australian hospital, automatic detection, average test accuracy, biomedical electrodes, cross-validation, Data mining, deep learning approach, diseases, diverse variations, epileptic seizure classification, epileptologist, extra-temporal brain areas, extra-temporal lobe, face recognition, facial expression, facial semiology, feedforward neural nets, fusion model, learning (artificial intelligence), long short-term memory networks, Magnetic heads, motion analysis, motion compensation, MTLE, multimodal analysis approach, natural semiology features, objective clinical analysis, patient monitoring, patient safety, recorded videos, seizure appearance, spatiotemporal features, video dataset, visual motion clues},
	pages = {3578--3581},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/VV25BZ9P/8513031.html:text/html},
}

@article{ahmedt-aristizabal_deep_2018-1,
	title = {Deep facial analysis: {A} new phase {I} epilepsy evaluation using computer vision},
	volume = {82},
	issn = {1525-5050},
	shorttitle = {Deep facial analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S1525505017309605},
	doi = {10.1016/j.yebeh.2018.02.010},
	abstract = {Semiology observation and characterization play a major role in the presurgical evaluation of epilepsy. However, the interpretation of patient movements has subjective and intrinsic challenges. In this paper, we develop approaches to attempt to automatically extract and classify semiological patterns from facial expressions. We address limitations of existing computer-based analytical approaches of epilepsy monitoring, where facial movements have largely been ignored. This is an area that has seen limited advances in the literature. Inspired by recent advances in deep learning, we propose two deep learning models, landmark-based and region-based, to quantitatively identify changes in facial semiology in patients with mesial temporal lobe epilepsy (MTLE) from spontaneous expressions during phase I monitoring. A dataset has been collected from the Mater Advanced Epilepsy Unit (Brisbane, Australia) and is used to evaluate our proposed approach. Our experiments show that a landmark-based approach achieves promising results in analyzing facial semiology, where movements can be effectively marked and tracked when there is a frontal face on visualization. However, the region-based counterpart with spatiotemporal features achieves more accurate results when confronted with extreme head positions. A multifold cross-validation of the region-based approach exhibited an average test accuracy of 95.19\% and an average AUC of 0.98 of the ROC curve. Conversely, a leave-one-subject-out cross-validation scheme for the same approach reveals a reduction in accuracy for the model as it is affected by data limitations and achieves an average test accuracy of 50.85\%. Overall, the proposed deep learning models have shown promise in quantifying ictal facial movements in patients with MTLE. In turn, this may serve to enhance the automated presurgical epilepsy evaluation by allowing for standardization, mitigating bias, and assessing key features. The computer-aided diagnosis may help to support clinical decision-making and prevent erroneous localization and surgery.},
	language = {en},
	urldate = {2020-01-26},
	journal = {Epilepsy \& Behavior},
	author = {Ahmedt-Aristizabal, David and Fookes, Clinton and Nguyen, Kien and Denman, Simon and Sridharan, Sridha and Dionisio, Sasha},
	month = may,
	year = {2018},
	keywords = {Convolutional neural network (CNN), Deep learning, Epilepsy evaluation, Facial semiology, Long short-term memory (LSTM), Neuroethology},
	pages = {17--24},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/NTMCJJAR/S1525505017309605.html:text/html},
}

@article{kouwenhoven_measuring_2009,
	title = {Measuring the similarity of target volume delineations independent of the number of observers},
	volume = {54},
	issn = {0031-9155},
	doi = {10.1088/0031-9155/54/9/018},
	abstract = {The variability of target delineations is a topic of interest in radiotherapy. The similarity of delineations is often quantified by use of a conformity index (CI) defined as the ratio of common to encompassing volume. Several forms of CI are in use, but no consensus exists on how to calculate the CI for more than two delineations. This study proposes a generalization of the CI applicable to any number of delineations. The generalization of the CI was developed, unbiased with respect to the number of delineations. Numerical values were calculated for clinical and theoretical cases, and differences with other forms of CI were considered. A simple expression could be derived, applicable to any number of delineations, and is equivalent to the known CI for two delineations. The use of this index is advised, although another frequently used index obtained from averaging the CI between all possible pairs of delineations results in minor differences. The use of the third generalization for the CI which is based upon the volume common to all delineations shows a clear dependence upon the number of delineations and is discouraged.},
	language = {eng},
	number = {9},
	journal = {Physics in Medicine and Biology},
	author = {Kouwenhoven, Erik and Giezen, Marina and Struikmans, Henk},
	month = may,
	year = {2009},
	pmid = {19384002},
	keywords = {Humans, Breast Neoplasms, Carcinoma, Non-Small-Cell Lung, Lung Neoplasms, Observer Variation, Radiotherapy, Conformal, Tomography, X-Ray Computed},
	pages = {2863--2873},
}

@article{isensee_nnu-net_2018,
	title = {{nnU}-{Net}: {Self}-adapting {Framework} for {U}-{Net}-{Based} {Medical} {Image} {Segmentation}},
	shorttitle = {{nnU}-{Net}},
	url = {http://arxiv.org/abs/1809.10486},
	abstract = {The U-Net was presented in 2015. With its straight-forward and successful architecture it quickly evolved to a commonly used benchmark in medical image segmentation. The adaptation of the U-Net to novel problems, however, comprises several degrees of freedom regarding the exact architecture, preprocessing, training and inference. These choices are not independent of each other and substantially impact the overall performance. The present paper introduces the nnU-Net ('no-new-Net'), which refers to a robust and self-adapting framework on the basis of 2D and 3D vanilla U-Nets. We argue the strong case for taking away superfluous bells and whistles of many proposed network designs and instead focus on the remaining aspects that make out the performance and generalizability of a method. We evaluate the nnU-Net in the context of the Medical Segmentation Decathlon challenge, which measures segmentation performance in ten disciplines comprising distinct entities, image modalities, image geometries and dataset sizes, with no manual adjustments between datasets allowed. At the time of manuscript submission, nnU-Net achieves the highest mean dice scores across all classes and seven phase 1 tasks (except class 1 in BrainTumour) in the online leaderboard of the challenge.},
	urldate = {2020-01-25},
	journal = {arXiv:1809.10486 [cs]},
	author = {Isensee, Fabian and Petersen, Jens and Klein, Andre and Zimmerer, David and Jaeger, Paul F. and Kohl, Simon and Wasserthal, Jakob and Koehler, Gregor and Norajitra, Tobias and Wirkert, Sebastian and Maier-Hein, Klaus H.},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.10486},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/HCQ3L3XH/1809.html:text/html},
}

@inproceedings{long_fully_2015,
	title = {Fully convolutional networks for semantic segmentation},
	doi = {10.1109/CVPR.2015.7298965},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build {\textquotedblleft}fully convolutional{\textquotedblright} networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Training, Computer architecture, image classification, learning (artificial intelligence), Adaptation models, contemporary classification networks, Convolution, Deconvolution, fully convolutional networks, image segmentation, Image segmentation, inference, inference mechanisms, learning, NYUDv2, PASCAL VOC, pixels-to-pixels, semantic segmentation, Semantics, SIFT flow, visual models},
	pages = {3431--3440},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/QWFMPMPK/7298965.html:text/html},
}

@article{rosenblatt_perceptron_1958,
	title = {The {Perceptron}: {A} {Probabilistic} {Model} for {Information} {Storage} and {Organization} in {The} {Brain}},
	shorttitle = {The {Perceptron}},
	abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	pages = {65--386},
	file = {Citeseer - Snapshot:/home/fernando/Zotero/storage/WX7SQLAA/summary.html:text/html},
}

@article{mohan_long-term_2018,
	title = {The long-term outcomes of epilepsy surgery},
	volume = {13},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5955551/},
	doi = {10.1371/journal.pone.0196274},
	abstract = {Objective
Despite modern anti-epileptic drug treatment, approximately 30\% of epilepsies remain medically refractory and for these patients, epilepsy surgery may be a treatment option. There have been numerous studies demonstrating good outcome of epilepsy surgery in the short to median term however, there are a limited number of studies looking at the long-term outcomes. The aim of this study was to ascertain the long-term outcome of resective epilepsy surgery in a large neurosurgery hospital in the U.K.

Methods
This a retrospective analysis of prospectively collected data. We used the 2001 International League Against Epilepsy (ILAE) classification system to classify seizure freedom and Kaplan-Meier survival analysis to estimate the probability of seizure freedom.

Results
We included 284 patients who underwent epilepsy surgery (178 anterior temporal lobe resections, 37 selective amygdalohippocampectomies, 33 temporal lesionectomies, 36 extratemporal lesionectomies), and had a prospective median follow-up of 5 years (range 1{\textendash}27). Kaplan-Meier estimates showed that 47\% (95\% CI 40{\textendash}58) remained seizure free (apart from simple partial seizures) at 5 years and 38\% (95\% CI 31{\textendash}45) at 10 years after surgery. 74\% (95\% CI 69{\textendash}80) had a greater than 50\% seizure reduction at 5 years and 70\% (95\% CI 64{\textendash}77) at 10 years. Patients who had an amygdalohippocampectomy were more likely to have seizure recurrence than patients who had an anterior temporal lobe resection (p = 0.006) and temporal lesionectomy (p = 0.029). There was no significant difference between extra temporal and temporal lesionectomies. Hippocampal sclerosis was associated with a good outcome but declined in relative frequency over the years.

Conclusion
The vast majority of patients who were not seizure free experienced at least a substantial and long-lasting reduction in seizure frequency. A positive long-term outcome after epilepsy surgery is possible for many patients and especially those with hippocampal sclerosis or those who had anterior temporal lobe resections.},
	number = {5},
	urldate = {2020-01-24},
	journal = {PLoS ONE},
	author = {Mohan, Midhun and Keller, Simon and Nicolson, Andrew and Biswas, Shubhabrata and Smith, David and Osman Farah, Jibril and Eldridge, Paul and Wieshmann, Udo},
	month = may,
	year = {2018},
	pmid = {29768433},
	pmcid = {PMC5955551},
}

@article{zhou_review_2019,
	title = {A review: {Deep} learning for medical image segmentation using multi-modality fusion},
	volume = {3-4},
	issn = {2590-0056},
	shorttitle = {A review},
	url = {http://www.sciencedirect.com/science/article/pii/S2590005619300049},
	doi = {10.1016/j.array.2019.100004},
	abstract = {Multi-modality is widely used in medical imaging, because it can provide multiinformation about a target (tumor, organ or tissue). Segmentation using multimodality consists of fusing multi-information to improve the segmentation. Recently, deep learning-based approaches have presented the state-of-the-art performance in image classification, segmentation, object detection and tracking tasks. Due to their self-learning and generalization ability over large amounts of data, deep learning recently has also gained great interest in multi-modal medical image segmentation. In this paper, we give an overview of deep learning-based approaches for multi-modal medical image segmentation task. Firstly, we introduce the general principle of deep learning and multi-modal medical image segmentation. Secondly, we present different deep learning network architectures, then analyze their fusion strategies and compare their results. The earlier fusion is commonly used, since it{\textquoteright}s simple and it focuses on the subsequent segmentation network architecture. However, the later fusion gives more attention on fusion strategy to learn the complex relationship between different modalities. In general, compared to the earlier fusion, the later fusion can give more accurate result if the fusion method is effective enough. We also discuss some common problems in medical image segmentation. Finally, we summarize and provide some perspectives on the future research.},
	language = {en},
	urldate = {2020-01-24},
	journal = {Array},
	author = {Zhou, Tongxue and Ruan, Su and Canu, St{\'e}phane},
	month = sep,
	year = {2019},
	keywords = {Deep learning, Medical image segmentation, Multi-modality fusion, Review},
	pages = {100004},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/KERU2VGX/S2590005619300049.html:text/html},
}

@article{chen_deep_2019,
	title = {Deep {Learning} for {Video} {Captioning}: {A} {Review}},
	shorttitle = {Deep {Learning} for {Video} {Captioning}},
	url = {https://www.ijcai.org/Proceedings/2019/877},
	abstract = {Electronic proceedings of IJCAI 2019},
	urldate = {2020-01-24},
	author = {Chen, Shaoxiang and Yao, Ting and Jiang, Yu-Gang},
	year = {2019},
	pages = {6283--6290},
	file = {Snapshot:/home/fernando/Zotero/storage/JU38PU59/877.html:text/html},
}

@article{yao_video_2019,
	title = {Video {Object} {Segmentation} and {Tracking}: {A} {Survey}},
	shorttitle = {Video {Object} {Segmentation} and {Tracking}},
	url = {http://arxiv.org/abs/1904.09172},
	abstract = {Object segmentation and object tracking are fundamental research area in the computer vision community. These two topics are diffcult to handle some common challenges, such as occlusion, deformation, motion blur, and scale variation. The former contains heterogeneous object, interacting object, edge ambiguity, and shape complexity. And the latter suffers from difficulties in handling fast motion, out-of-view, and real-time processing. Combining the two problems of video object segmentation and tracking (VOST) can overcome their respective difficulties and improve their performance. VOST can be widely applied to many practical applications such as video summarization, high definition video compression, human computer interaction, and autonomous vehicles. This article aims to provide a comprehensive review of the state-of-the-art tracking methods, and classify these methods into different categories, and identify new trends. First, we provide a hierarchical categorization existing approaches, including unsupervised VOS, semi-supervised VOS, interactive VOS, weakly supervised VOS, and segmentation-based tracking methods. Second, we provide a detailed discussion and overview of the technical characteristics of the different methods. Third, we summarize the characteristics of the related video dataset, and provide a variety of evaluation metrics. Finally, we point out a set of interesting future works and draw our own conclusions.},
	urldate = {2020-01-24},
	journal = {arXiv:1904.09172 [cs]},
	author = {Yao, Rui and Lin, Guosheng and Xia, Shixiong and Zhao, Jiaqi and Zhou, Yong},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.09172},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/TAE8SF54/1904.html:text/html},
}

@article{jobst_resective_2015,
	title = {Resective epilepsy surgery for drug-resistant focal epilepsy: a review},
	volume = {313},
	issn = {1538-3598},
	shorttitle = {Resective epilepsy surgery for drug-resistant focal epilepsy},
	doi = {10.1001/jama.2014.17426},
	language = {eng},
	number = {3},
	journal = {JAMA},
	author = {Jobst, Barbara C. and Cascino, Gregory D.},
	month = jan,
	year = {2015},
	pmid = {25602999},
	keywords = {Humans, Adult, Seizures, Epilepsies, Partial, Child, Anticonvulsants, Drug Resistance, Quality of Life, Randomized Controlled Trials as Topic, Temporal Lobe},
	pages = {285--293},
}

@article{cunha_neurokinect_2016,
	title = {{NeuroKinect}: {A} {Novel} {Low}-{Cost} {3Dvideo}-{EEG} {System} for {Epileptic} {Seizure} {Motion} {Quantification}},
	volume = {11},
	issn = {1932-6203},
	shorttitle = {{NeuroKinect}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0145669},
	doi = {10.1371/journal.pone.0145669},
	abstract = {Epilepsy is a common neurological disorder which affects 0.5{\textendash}1\% of the world population. Its diagnosis relies both on Electroencephalogram (EEG) findings and characteristic seizure-induced body movements - called seizure semiology. Thus, synchronous EEG and (2D)video recording systems (known as Video-EEG) are the most accurate tools for epilepsy diagnosis. Despite the establishment of several quantitative methods for EEG analysis, seizure semiology is still analyzed by visual inspection, based on epileptologists{\textquoteright} subjective interpretation of the movements of interest (MOIs) that occur during recorded seizures. In this contribution, we present NeuroKinect, a low-cost, easy to setup and operate solution for a novel 3Dvideo-EEG system. It is based on a RGB-D sensor (Microsoft Kinect camera) and performs 24/7 monitoring of an Epilepsy Monitoring Unit (EMU) bed. It does not require the attachment of any reflectors or sensors to the patient{\textquoteright}s body and has a very low maintenance load. To evaluate its performance and usability, we mounted a state-of-the-art 6-camera motion-capture system and our low-cost solution over the same EMU bed. A comparative study of seizure-simulated MOIs showed an average correlation of the resulting 3D motion trajectories of 84.2\%. Then, we used our system on the routine of an EMU and collected 9 different seizures where we could perform 3D kinematic analysis of 42 MOIs arising from the temporal (TLE) (n = 19) and extratemporal (ETE) brain regions (n = 23). The obtained results showed that movement displacement and movement extent discriminated both seizure MOI groups with statistically significant levels (mean = 0.15 m vs. 0.44 m, p{\textless}0.001; mean = 0.068 m3 vs. 0.14 m3, p{\textless}0.05, respectively). Furthermore, TLE MOIs were significantly shorter than ETE (mean = 23 seconds vs 35 seconds, p{\textless}0.01) and presented higher jerking levels (mean = 345 ms-3 vs 172 ms-3, p{\textless}0.05). Our newly implemented 3D approach is faster by 87.5\% in extracting body motion trajectories when compared to a 2D frame by frame tracking procedure. We conclude that this new approach provides a more comfortable (both for patients and clinical professionals), simpler, faster and lower-cost procedure than previous approaches, therefore providing a reliable tool to quantitatively analyze MOI patterns of epileptic seizures in the routine of EMUs around the world. We hope this study encourages other EMUs to adopt similar approaches so that more quantitative information is used to improve epilepsy diagnosis.},
	language = {en},
	number = {1},
	urldate = {2020-01-24},
	journal = {PLOS ONE},
	author = {Cunha, Jo{\~a}o Paulo Silva and Choupina, Hugo Miguel Pereira and Rocha, Ana Patr{\'i}cia and Fernandes, Jos{\'e} Maria and Achilles, Felix and Loesch, Anna Mira and Vollmar, Christian and Hartl, Elisabeth and Noachtar, Soheyl},
	month = jan,
	year = {2016},
	keywords = {Algorithms, Epilepsy, Electroencephalography, Semiotics, Epileptic seizures, Data acquisition, Musculoskeletal system, Velocity},
	pages = {e0145669},
	file = {Snapshot:/home/fernando/Zotero/storage/2F9QHJYI/article.html:text/html},
}

@article{ahmedtaristizabal_automated_2017,
	title = {Automated analysis of seizure semiology and brain electrical activity in presurgery evaluation of epilepsy: {A} focused survey},
	volume = {58},
	copyright = {Wiley Periodicals, Inc. {\textcopyright} 2017 International League Against Epilepsy},
	issn = {1528-1167},
	shorttitle = {Automated analysis of seizure semiology and brain electrical activity in presurgery evaluation of epilepsy},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/epi.13907},
	doi = {10.1111/epi.13907},
	abstract = {Epilepsy being one of the most prevalent neurological disorders, affecting approximately 50 million people worldwide, and with almost 30{\textendash}40\% of patients experiencing partial epilepsy being nonresponsive to medication, epilepsy surgery is widely accepted as an effective therapeutic option. Presurgical evaluation has advanced significantly using noninvasive techniques based on video monitoring, neuroimaging, and electrophysiological and neuropsychological tests; however, certain clinical settings call for invasive intracranial recordings such as stereoelectroencephalography (SEEG), aiming to accurately map the eloquent brain networks involved during a seizure. Most of the current presurgical evaluation procedures focus on semiautomatic techniques, where surgery diagnosis relies immensely on neurologists{\textquoteright} experience and their time-consuming subjective interpretation of semiology or the manifestations of epilepsy and their correlation with the brain's electrical activity. Because surgery misdiagnosis reaches a rate of 30\%, and more than one-third of all epilepsies are poorly understood, there is an evident keen interest in improving diagnostic precision using computer-based methodologies that in the past few years have shown near-human performance. Among them, deep learning has excelled in many biological and medical applications, but has advanced insufficiently in epilepsy evaluation and automated understanding of neural bases of semiology. In this paper, we systematically review the automatic applications in epilepsy for human motion analysis, brain electrical activity, and the anatomoelectroclinical correlation to attribute anatomical localization of the epileptogenic network to distinctive epilepsy patterns. Notably, recent advances in deep learning techniques will be investigated in the contexts of epilepsy to address the challenges exhibited by traditional machine learning techniques. Finally, we discuss and propose future research on epilepsy surgery assessment that can jointly learn across visually observed semiologic patterns and recorded brain electrical activity.},
	language = {en},
	number = {11},
	urldate = {2020-01-24},
	journal = {Epilepsia},
	author = {Ahmedt-Aristizabal, David and Fookes, Clinton and Dionisio, Sasha and Nguyen, Kien and Cunha, Jo{\~a}o Paulo S. and Sridharan, Sridha},
	year = {2017},
	keywords = {Deep learning, Epileptogenic network, Facial expression, Human motion, Machine learning},
	pages = {1817--1831},
	file = {Snapshot:/home/fernando/Zotero/storage/478HAREE/epi.html:text/html},
}

@article{chauvel_emergence_2014,
	title = {Emergence of semiology in epileptic seizures},
	volume = {38},
	issn = {1525-5069},
	doi = {10.1016/j.yebeh.2013.12.003},
	abstract = {Semiology, the manifestation of epilepsy, is dependent upon electrical activity produced by epileptic seizures that are organized within existing neural pathways. Clinical signs evolve as the epileptic discharge spreads in both time and space. Studying the relation between these, of which the temporal component is at least as important as the spatial one, is possible using anatomo-electro-clinical correlations of stereoelectroencephalography (SEEG) data. The period of semiology production occurs with variable time lag after seizure onset and signs then emerge more or less rapidly depending on seizure type (temporal seizures generally propagating more slowly and frontal seizures more quickly). The subset of structures involved in semiological production, the "early spread network", is tightly linked to those constituting the epileptogenic zone. The level of complexity of semiological features varies according to the degree of involvement of the primary or associative cortex, with the former having a direct relation to peripheral sensory and motor systems with production of hallucinations (visual and auditory) or elementary sensorimotor signs. Depending on propagation pattern, these signs can occur in a "march" fashion as described by Jackson. On the other hand, seizures involving the associative cortex, having a less direct relation with the peripheral nervous system, and necessarily involving more widely distributed networks manifest with altered cognitive and/or behavioral signs whose neural substrate involves a network of cortical structures, as has been observed for normal cognitive processes. Other than the anatomical localization of these structures, the frequency of the discharge is a crucial determinant of semiological effect since a fast (gamma) discharge will tend to deactivate normal function, whereas a slower theta discharge can mimic physiological function. In terms of interaction between structures, the degree of synchronization plays a key role in clinical expression, as evidenced, for example, by studies of ictal fear-related behavior (decorrelation of activity between structures inducing "release" phenomena) and of d{\'e}j{\`a} vu (increased synchronization). Studies of functional coupling within networks underlying complex ictal behavior indicate that the clinical semiology of a given seizure depends upon neither the anatomical origin of ictal discharge nor the target areas of its propagation alone but on the dynamic interaction between these. Careful mapping of the ictal network in its full spread offers essential information as to the localization of seizure onset, by deducing that a given network configuration could only be generated by a given area or group of areas.},
	language = {eng},
	journal = {Epilepsy \& Behavior: E\&B},
	author = {Chauvel, Patrick and McGonigal, Aileen},
	month = sep,
	year = {2014},
	pmid = {24424286},
	keywords = {Humans, Epilepsy, Electroencephalography, EEG, Semiology, Cerebral Cortex, Nerve Net, Networks, Stereoelectroencephalography, Synchronization},
	pages = {94--103},
}

@misc{perez-garcia_fepegarhighresnet_2019,
	title = {fepegar/highresnet: {PyTorch} implementation of {HighRes3DNet}},
	shorttitle = {fepegar/highresnet},
	url = {https://zenodo.org/record/3550830#.XiraQKf7RhE},
	abstract = {highresnet PyTorch implementation of HighRes3DNet from Li et al. 2017, On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task. All the information about how the weights were ported from NiftyNet can be found in my submission to the MICCAI Educational Challenge 2019.},
	urldate = {2020-01-24},
	publisher = {Zenodo},
	author = {P{\'e}rez-Garc{\'i}a, Fernando},
	month = nov,
	year = {2019},
	doi = {10.5281/zenodo.3550830},
	file = {Zenodo Snapshot:/home/fernando/Zotero/storage/L33PWGZ2/3550830.html:text/html},
}

@incollection{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	urldate = {2020-01-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch{\'e}-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
	file = {NIPS Snapshot:/home/fernando/Zotero/storage/B74LB6CD/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.html:text/html},
}

@article{sudre_longitudinal_2017,
	title = {Longitudinal segmentation of age-related white matter hyperintensities},
	volume = {38},
	issn = {1361-8415},
	doi = {10.1016/j.media.2017.02.007},
	abstract = {Although white matter hyperintensities evolve in the course of ageing, few solutions exist to consider the lesion segmentation problem longitudinally. Based on an existing automatic lesion segmentation algorithm, a longitudinal extension is proposed. For evaluation purposes, a longitudinal lesion simulator is created allowing for the comparison between the longitudinal and the cross-sectional version in various situations of lesion load progression. Finally, applied to clinical data, the proposed framework demonstrates an increased robustness compared to available cross-sectional methods and findings are aligned with previously reported clinical patterns.},
	language = {en},
	urldate = {2020-01-23},
	journal = {Medical Image Analysis},
	author = {Sudre, Carole H. and Cardoso, M. Jorge and Ourselin, Sebastien},
	month = may,
	year = {2017},
	keywords = {Segmentation, Longitudinal, White matter hyperintensities},
	pages = {50--64},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/2B4JHBSA/S1361841517300257.html:text/html},
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	shorttitle = {Dropout},
	urldate = {2020-01-22},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
	file = {Snapshot:/home/fernando/Zotero/storage/7NXVYPIX/srivastava14a.html:text/html},
}

@article{papadopoulos_uncertainty_2001,
	title = {Uncertainty estimation and {Monte} {Carlo} simulation method},
	volume = {12},
	issn = {0955-5986},
	url = {http://www.sciencedirect.com/science/article/pii/S0955598601000152},
	doi = {10.1016/S0955-5986(01)00015-2},
	abstract = {It has been reported that the Monte Carlo Method has many advantages over conventional methods in the estimation of uncertainty, especially that of complex measurement systems' outputs. The method, superficially, is relatively simple to implement, and is slowly gaining industrial acceptance. Unfortunately, very little has been published on how the method works. To those who are uninitiated, this powerful approach remains a {\textquoteleft}black art{\textquoteright}. This paper demonstrates that the Monte Carlo simulation method is fully compatible with the conventional uncertainty estimation methods for linear systems and systems that have small uncertainties. Monte Carlo simulation has the ability to take account of partial correlated measurement input uncertainties. It also examines the uncertainties of the results of some basic manipulations e.g. addition, multiplication and division, of two input measured variables which may or may not be correlated. For correlated input measurements, the probability distribution of the result could be biased or skewed. These properties cannot be revealed using conventional methods.},
	language = {en},
	number = {4},
	urldate = {2020-01-22},
	journal = {Flow Measurement and Instrumentation},
	author = {Papadopoulos, Christos E. and Yeung, Hoi},
	month = aug,
	year = {2001},
	keywords = {Correlation, Monte Carlo, Uncertainty estimation, Uncertainty propagation},
	pages = {291--298},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/47XCASKJ/S0955598601000152.html:text/html},
}

@article{chen_deeplab_2017,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	shorttitle = {{DeepLab}},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	urldate = {2020-01-22},
	journal = {arXiv:1606.00915 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = may,
	year = {2017},
	note = {arXiv: 1606.00915},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/DQK9JBQK/1606.html:text/html},
}

@article{pezeshk_seamless_2017,
	title = {Seamless lesion insertion for data augmentation in {CAD} training},
	volume = {36},
	issn = {0278-0062},
	doi = {10.1109/TMI.2016.2640180},
	abstract = {The performance of a classifier is largely dependent on the size and representativeness of data used for its training. In circumstances where accumulation and/or labeling of training samples is difficult or expensive, such as medical applications, data augmentation can potentially be used to alleviate the limitations of small datasets. We have previously developed an image blending tool that allows users to modify or supplement an existing CT or mammography dataset by seamlessly inserting a lesion extracted from a source image into a target image. This tool also provides the option to apply various types of transformations to different properties of the lesion prior to its insertion into a new location. In this study, we used this tool to create synthetic samples that appear realistic in chest CT. We then augmented different size training sets with these artificial samples, and investigated the effect of the augmentation on training various classifiers for the detection of lung nodules. Our results indicate that the proposed lesion insertion method can improve classifier performance for small training datasets, and thereby help reduce the need to acquire and label actual patient data.},
	number = {4},
	urldate = {2020-01-22},
	journal = {IEEE Trans Med Imaging},
	author = {Pezeshk, Aria and Petrick, Nicholas and Chen, Weijie and Sahiner, Berkman},
	month = apr,
	year = {2017},
	pmid = {28113310},
	pmcid = {PMC5509514},
	pages = {1005--1015},
}

@article{perez_poisson_2003,
	title = {Poisson image editing},
	volume = {22},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/882262.882269},
	doi = {10.1145/882262.882269},
	abstract = {Using generic interpolation machinery based on solving Poisson equations, a variety of novel tools are introduced for seamless editing of image regions. The first set of tools permits the seamless importation of both opaque and transparent source image regions into a destination region. The second set is based on similar mathematical ideas and allows the user to modify the appearance of the image seamlessly, within a selected region. These changes can be arranged to affect the texture, the illumination, and the color of objects lying in the region, or to make tileable a rectangular selection.},
	number = {3},
	urldate = {2020-01-22},
	journal = {ACM Transactions on Graphics (TOG)},
	author = {P{\'e}rez, Patrick and Gangnet, Michel and Blake, Andrew},
	month = jul,
	year = {2003},
	keywords = {guided interpolation, image gradient, interactive image editing, Poisson equation, seamless cloning, selection editing},
	pages = {313--318},
}

@article{robins_techniques_2017,
	title = {Techniques for virtual lung nodule insertion: volumetric and morphometric comparison of projection-based and image-based methods for quantitative {CT}},
	volume = {62},
	issn = {1361-6560},
	shorttitle = {Techniques for virtual lung nodule insertion},
	doi = {10.1088/1361-6560/aa83f8},
	abstract = {Virtual nodule insertion paves the way towards the development of standardized databases of hybrid CT images with known lesions. The purpose of this study was to assess three methods (an established and two newly developed techniques) for inserting virtual lung nodules into CT images. Assessment was done by comparing virtual nodule volume and shape to the CT-derived volume and shape of synthetic nodules. 24 synthetic nodules (three sizes, four morphologies, two repeats) were physically inserted into the lung cavity of an anthropomorphic chest phantom (KYOTO KAGAKU). The phantom was imaged with and without nodules on a commercial CT scanner (SOMATOM Definition Flash, Siemens) using a standard thoracic CT protocol at two dose levels (1.4 and 22 mGy CTDIvol). Raw projection data were saved and reconstructed with filtered back-projection and sinogram affirmed iterative reconstruction (SAFIRE, strength 5) at 0.6 mm slice thickness. Corresponding 3D idealized, virtual nodule models were co-registered with the CT images to determine each nodule's location and orientation. Virtual nodules were voxelized, partial volume corrected, and inserted into nodule-free CT data (accounting for system imaging physics) using two methods: projection-based Technique A, and image-based Technique B. Also a third Technique C based on cropping a region of interest from the acquired image of the real nodule and blending it into the nodule-free image was tested. Nodule volumes were measured using a commercial segmentation tool (iNtuition, TeraRecon, Inc.) and deformation was assessed using the Hausdorff distance. Nodule volumes and deformations were compared between the idealized, CT-derived and virtual nodules using a linear mixed effects regression model which utilized the mean, standard deviation, and coefficient of variation ([Formula: see text], [Formula: see text] and [Formula: see text] of the regional Hausdorff distance. Overall, there was a close concordance between the volumes of the CT-derived and virtual nodules. Percent differences between them were less than 3\% for all insertion techniques and were not statistically significant in most cases. Correlation coefficient values were greater than 0.97. The deformation according to the Hausdorff distance was also similar between the CT-derived and virtual nodules with minimal statistical significance in the ([Formula: see text]) for Techniques A, B, and C. This study shows that both projection-based and image-based nodule insertion techniques yield realistic nodule renderings with statistical similarity to the synthetic nodules with respect to nodule volume and deformation. These techniques could be used to create a database of hybrid CT images containing nodules of known size, location and morphology.},
	language = {eng},
	number = {18},
	journal = {Physics in Medicine and Biology},
	author = {Robins, Marthony and Solomon, Justin and Sahbaee, Pooyan and Sedlmair, Martin and Roy Choudhury, Kingshuk and Pezeshk, Aria and Sahiner, Berkman and Samei, Ehsan},
	month = aug,
	year = {2017},
	pmid = {28786399},
	pmcid = {PMC5693368},
	keywords = {Humans, Lung Neoplasms, Tomography, X-Ray Computed, Linear Models, Phantoms, Imaging, Radiographic Image Interpretation, Computer-Assisted, Solitary Pulmonary Nodule, Tomography Scanners, X-Ray Computed},
	pages = {7280--7299},
}

@article{cheplygina_not-so-supervised_2019,
	title = {Not-so-supervised: {A} survey of semi-supervised, multi-instance, and transfer learning in medical image analysis},
	volume = {54},
	issn = {1361-8423},
	shorttitle = {Not-so-supervised},
	doi = {10.1016/j.media.2019.03.009},
	abstract = {Machine learning (ML) algorithms have made a tremendous impact in the field of medical imaging. While medical imaging datasets have been growing in size, a challenge for supervised ML algorithms that is frequently mentioned is the lack of annotated data. As a result, various methods that can learn with less/other types of supervision, have been proposed. We give an overview of semi-supervised, multiple instance, and transfer learning in medical imaging, both in diagnosis or segmentation tasks. We also discuss connections between these learning scenarios, and opportunities for future research. A dataset with the details of the surveyed papers is available via https://figshare.com/articles/Database\_of\_surveyed\_literature\_in\_Not-so-supervised\_a\_survey\_of\_semi-supervised\_multi-instance\_and\_transfer\_learning\_in\_medical\_image\_analysis\_/7479416.},
	language = {eng},
	journal = {Medical Image Analysis},
	author = {Cheplygina, Veronika and de Bruijne, Marleen and Pluim, Josien P. W.},
	year = {2019},
	pmid = {30959445},
	keywords = {Medical imaging, Machine learning, Computer aided diagnosis, Multi-task learning, Multiple instance learning, Semi-supervised learning, Transfer learning, Weakly-supervised learning},
	pages = {280--296},
}

@article{minaee_image_2020,
	title = {Image {Segmentation} {Using} {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Image {Segmentation} {Using} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2001.05566},
	abstract = {Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.},
	urldate = {2020-01-21},
	journal = {arXiv:2001.05566 [cs]},
	author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.05566},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/84PVM2HG/2001.html:text/html},
}

@article{lundervold_overview_2019,
	series = {Special {Issue}: {Deep} {Learning} in {Medical} {Physics}},
	title = {An overview of deep learning in medical imaging focusing on {MRI}},
	volume = {29},
	issn = {0939-3889},
	url = {http://www.sciencedirect.com/science/article/pii/S0939388918301181},
	doi = {10.1016/j.zemedi.2018.11.002},
	abstract = {What has happened in machine learning lately, and what does it mean for the future of medical image analysis? Machine learning has witnessed a tremendous amount of attention over the last few years. The current boom started around 2009 when so-called deep artificial neural networks began outperforming other established models on a number of important benchmarks. Deep neural networks are now the state-of-the-art machine learning models across a variety of areas, from image analysis to natural language processing, and widely deployed in academia and industry. These developments have a huge potential for medical imaging technology, medical data analysis, medical diagnostics and healthcare in general, slowly being realized. We provide a short overview of recent advances and some associated challenges in machine learning applied to medical image processing and image analysis. As this has become a very broad and fast expanding field we will not survey the entire landscape of applications, but put particular focus on deep learning in MRI. Our aim is threefold: (i) give a brief introduction to deep learning with pointers to core references; (ii) indicate how deep learning has been applied to the entire MRI processing chain, from acquisition to image retrieval, from segmentation to disease prediction; (iii) provide a starting point for people interested in experimenting and perhaps contributing to the field of deep learning for medical imaging by pointing out good educational resources, state-of-the-art open-source code, and interesting sources of data and problems related medical imaging.},
	language = {en},
	number = {2},
	urldate = {2020-01-21},
	journal = {Zeitschrift f{\"u}r Medizinische Physik},
	author = {Lundervold, Alexander Selvikv{\r a}g and Lundervold, Arvid},
	month = may,
	year = {2019},
	keywords = {Deep learning, Medical imaging, MRI, Machine learning},
	pages = {102--127},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/TWE7YLS5/S0939388918301181.html:text/html},
}

@article{sharma_automated_2010,
	title = {Automated medical image segmentation techniques},
	volume = {35},
	issn = {0971-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2825001/},
	doi = {10.4103/0971-6203.58777},
	abstract = {Accurate segmentation of medical images is a key step in contouring during radiotherapy planning. Computed topography (CT) and Magnetic resonance (MR) imaging are the most widely used radiographic techniques in diagnosis, clinical studies and treatment planning. This review provides details of automated segmentation methods, specifically discussed in the context of CT and MR images. The motive is to discuss the problems encountered in segmentation of CT and MR images, and the relative merits and limitations of methods currently available for segmentation of medical images.},
	number = {1},
	urldate = {2020-01-21},
	journal = {Journal of Medical Physics / Association of Medical Physicists of India},
	author = {Sharma, Neeraj and Aggarwal, Lalit M.},
	year = {2010},
	pmid = {20177565},
	pmcid = {PMC2825001},
	pages = {3--14},
}

@article{desikan_automated_2006,
	title = {An automated labeling system for subdividing the human cerebral cortex on {MRI} scans into gyral based regions of interest},
	volume = {31},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811906000437},
	doi = {10.1016/j.neuroimage.2006.01.021},
	abstract = {In this study, we have assessed the validity and reliability of an automated labeling system that we have developed for subdividing the human cerebral cortex on magnetic resonance images into gyral based regions of interest (ROIs). Using a dataset of 40 MRI scans we manually identified 34 cortical ROIs in each of the individual hemispheres. This information was then encoded in the form of an atlas that was utilized to automatically label ROIs. To examine the validity, as well as the intra- and inter-rater reliability of the automated system, we used both intraclass correlation coefficients (ICC), and a new method known as mean distance maps, to assess the degree of mismatch between the manual and the automated sets of ROIs. When compared with the manual ROIs, the automated ROIs were highly accurate, with an average ICC of 0.835 across all of the ROIs, and a mean distance error of less than 1~mm. Intra- and inter-rater comparisons yielded little to no difference between the sets of ROIs. These findings suggest that the automated method we have developed for subdividing the human cerebral cortex into standard gyral-based neuroanatomical regions is both anatomically valid and reliable. This method may be useful for both morphometric and functional studies of the cerebral cortex as well as for clinical investigations aimed at tracking the evolution of disease-induced changes over time, including clinical trials in which MRI-based measures are used to examine response to treatment.},
	language = {en},
	number = {3},
	urldate = {2020-01-21},
	journal = {NeuroImage},
	author = {Desikan, Rahul S. and S{\'e}gonne, Florent and Fischl, Bruce and Quinn, Brian T. and Dickerson, Bradford C. and Blacker, Deborah and Buckner, Randy L. and Dale, Anders M. and Maguire, R. Paul and Hyman, Bradley T. and Albert, Marilyn S. and Killiany, Ronald J.},
	month = jul,
	year = {2006},
	pages = {968--980},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/EXBHMAXK/S1053811906000437.html:text/html},
}

@article{west_surgery_2019,
	title = {Surgery for epilepsy},
	volume = {6},
	issn = {1469-493X},
	doi = {10.1002/14651858.CD010541.pub3},
	abstract = {BACKGROUND: This is an updated version of the original Cochrane review, published in 2015.Focal epilepsies are caused by a malfunction of nerve cells localised in one part of one cerebral hemisphere. In studies, estimates of the number of individuals with focal epilepsy who do not become seizure-free despite optimal drug therapy vary between at least 20\% and up to 70\%. If the epileptogenic zone can be located, surgical resection offers the chance of a cure with a corresponding increase in quality of life.
OBJECTIVES: The primary objective is to assess the overall outcome of epilepsy surgery according to evidence from randomised controlled trials.Secondary objectives are to assess the overall outcome of epilepsy surgery according to non-randomised evidence, and to identify the factors that correlate with remission of seizures postoperatively.
SEARCH METHODS: For the latest update, we searched the following databases on 11 March 2019: Cochrane Register of Studies (CRS Web), which includes the Cochrane Epilepsy Group Specialized Register and the Cochrane Central Register of Controlled Trials (CENTRAL), MEDLINE (Ovid, 1946 to March 08, 2019), ClinicalTrials.gov, and the World Health Organization (WHO) International Clinical Trials Registry Platform (ICTRP).
SELECTION CRITERIA: Eligible studies were randomised controlled trials (RCTs) that included at least 30 participants in a well-defined population (age, sex, seizure type/frequency, duration of epilepsy, aetiology, magnetic resonance imaging (MRI) diagnosis, surgical findings), with an MRI performed in at least 90\% of cases and an expected duration of follow-up of at least one year, and reporting an outcome related to postoperative seizure control. Cohort studies or case series were included in the previous version of this review.
DATA COLLECTION AND ANALYSIS: Three groups of two review authors independently screened all references for eligibility, assessed study quality and risk of bias, and extracted data. Outcomes were proportions of participants achieving a good outcome according to the presence or absence of each prognostic factor of interest. We intended to combine data with risk ratios (RRs) and 95\% confidence intervals (95\% CIs).
MAIN RESULTS: We identified 182 studies with a total of 16,855 included participants investigating outcomes of surgery for epilepsy. Nine studies were RCTs (including two that randomised participants to surgery or medical treatment (99 participants included in the two trials received medical treatment)). Risk of bias in these RCTs was unclear or high. Most of the remaining 173 non-randomised studies followed a retrospective design. We assessed study quality using the Effective Public Health Practice Project (EPHPP) tool and determined that most studies provided moderate or weak evidence. For 29 studies reporting multivariate analyses, we used the Quality in Prognostic Studies (QUIPS) tool and determined that very few studies were at low risk of bias across domains.In terms of freedom from seizures, two RCTs found surgery (n = 97) to be superior to medical treatment (n = 99); four found no statistically significant differences between anterior temporal lobectomy (ATL) with or without corpus callosotomy (n = 60), between subtemporal or transsylvian approach to selective amygdalohippocampectomy (SAH) (n = 47); between ATL, SAH and parahippocampectomy (n = 43) or between 2.5 cm and 3.5 cm ATL resection (n = 207). One RCT found total hippocampectomy to be superior to partial hippocampectomy (n = 70) and one found ATL to be superior to stereotactic radiosurgery (n = 58); and another provided data to show that for Lennox-Gastaut syndrome, no significant differences in seizure outcomes were evident between those treated with resection of the epileptogenic zone and those treated with resection of the epileptogenic zone plus corpus callosotomy (n = 43). We judged evidence from the nine RCTs to be of moderate to very low quality due to lack of information reported about the randomised trial design and the restricted study populations.Of the 16,756 participants included in this review who underwent a surgical procedure, 10,696 (64\%) achieved a good outcome from surgery; this ranged across studies from 13.5\% to 92.5\%. Overall, we found the quality of data in relation to recording of adverse events to be very poor.In total, 120 studies examined between one and eight prognostic factors in univariate analysis. We found the following prognostic factors to be associated with a better post-surgical seizure outcome: abnormal pre-operative MRI, no use of intracranial monitoring, complete surgical resection, presence of mesial temporal sclerosis, concordance of pre-operative MRI and electroencephalography, history of febrile seizures, absence of focal cortical dysplasia/malformation of cortical development, presence of tumour, right-sided resection, and presence of unilateral interictal spikes. We found no evidence that history of head injury, presence of encephalomalacia, presence of vascular malformation, and presence of postoperative discharges were prognostic factors of outcome.Twenty-nine studies reported multi-variable models of prognostic factors, and showed that the direction of association of factors with outcomes was generally the same as that found in univariate analyses.We observed variability in many of our analyses, likely due to small study sizes with unbalanced group sizes and variation in the definition of seizure outcome, the definition of prognostic factors, and the influence of the site of surgery AUTHORS' CONCLUSIONS: Study design issues and limited information presented in the included studies mean that our results provide limited evidence to aid patient selection for surgery and prediction of likely surgical outcomes. Future research should be of high quality, follow a prospective design, be appropriately powered, and focus on specific issues related to diagnostic tools, the site-specific surgical approach, and other issues such as extent of resection. Researchers should investigate prognostic factors related to the outcome of surgery via multi-variable statistical regression modelling, where variables are selected for modelling according to clinical relevance, and all numerical results of the prognostic models are fully reported. Journal editors should not accept papers for which study authors did not record adverse events from a medical intervention. Researchers have achieved improvements in cancer care over the past three to four decades by answering well-defined questions through the conduct of focused RCTs in a step-wise fashion. The same approach to surgery for epilepsy is required.},
	language = {eng},
	journal = {The Cochrane Database of Systematic Reviews},
	author = {West, Siobhan and Nevitt, Sarah J. and Cotton, Jennifer and Gandhi, Sacha and Weston, Jennifer and Sudan, Ajay and Ramirez, Roberto and Newton, Richard},
	year = {2019},
	pmid = {31237346},
	pmcid = {PMC6591702},
	pages = {CD010541},
}

@article{baumgartner_presurgical_2019,
	title = {Presurgical epilepsy evaluation and epilepsy surgery},
	volume = {8},
	issn = {2046-1402},
	doi = {10.12688/f1000research.17714.1},
	abstract = {With a prevalence of 0.8 to 1.2\%, epilepsy represents one of the most frequent chronic neurological disorders; 30 to 40\% of patients suffer from drug-resistant epilepsy (that is, seizures cannot be controlled adequately with antiepileptic drugs). Epilepsy surgery represents a valuable treatment option for 10 to 50\% of these patients. Epilepsy surgery aims to control seizures by resection of the epileptogenic tissue while avoiding neuropsychological and other neurological deficits by sparing essential brain areas. The most common histopathological findings in epilepsy surgery specimens are hippocampal sclerosis in adults and focal cortical dysplasia in children. Whereas presurgical evaluations and surgeries in patients with mesial temporal sclerosis and benign tumors recently decreased in most centers, non-lesional patients, patients requiring intracranial recordings, and neocortical resections increased. Recent developments in neurophysiological techniques (high-density electroencephalography [EEG], magnetoencephalography, electrical and magnetic source imaging, EEG-functional magnetic resonance imaging [EEG-fMRI], and recording of pathological high-frequency oscillations), structural magnetic resonance imaging (MRI) (ultra-high-field imaging at 7 Tesla, novel imaging acquisition protocols, and advanced image analysis [post-processing] techniques), functional imaging (positron emission tomography and single-photon emission computed tomography co-registered to MRI), and fMRI significantly improved non-invasive presurgical evaluation and have opened the option of epilepsy surgery to patients previously not considered surgical candidates. Technical improvements of resective surgery techniques facilitate successful and safe operations in highly delicate brain areas like the perisylvian area in operculoinsular epilepsy. Novel less-invasive surgical techniques include stereotactic radiosurgery, MR-guided laser interstitial thermal therapy, and stereotactic intracerebral EEG-guided radiofrequency thermocoagulation.},
	language = {eng},
	journal = {F1000Research},
	author = {Baumgartner, Christoph and Koren, Johannes P. and Britto-Arias, Martha and Zoche, Lea and Pirker, Susanne},
	year = {2019},
	pmid = {31700611},
	pmcid = {PMC6820825},
	keywords = {epilepsy, presurgical evaluation, surgery},
}

@article{yushkevich_user-guided_2006,
	title = {User-guided {3D} active contour segmentation of anatomical structures: significantly improved efficiency and reliability},
	volume = {31},
	issn = {1053-8119},
	shorttitle = {User-guided {3D} active contour segmentation of anatomical structures},
	doi = {10.1016/j.neuroimage.2006.01.015},
	abstract = {Active contour segmentation and its robust implementation using level set methods are well-established theoretical approaches that have been studied thoroughly in the image analysis literature. Despite the existence of these powerful segmentation methods, the needs of clinical research continue to be fulfilled, to a large extent, using slice-by-slice manual tracing. To bridge the gap between methodological advances and clinical routine, we developed an open source application called ITK-SNAP, which is intended to make level set segmentation easily accessible to a wide range of users, including those with little or no mathematical expertise. This paper describes the methods and software engineering philosophy behind this new tool and provides the results of validation experiments performed in the context of an ongoing child autism neuroimaging study. The validation establishes SNAP intrarater and interrater reliability and overlap error statistics for the caudate nucleus and finds that SNAP is a highly reliable and efficient alternative to manual tracing. Analogous results for lateral ventricle segmentation are provided.},
	language = {eng},
	number = {3},
	journal = {NeuroImage},
	author = {Yushkevich, Paul A. and Piven, Joseph and Hazlett, Heather Cody and Smith, Rachel Gimpel and Ho, Sean and Gee, James C. and Gerig, Guido},
	month = jul,
	year = {2006},
	pmid = {16545965},
	keywords = {Brain, Humans, Image Processing, Computer-Assisted, Magnetic Resonance Imaging, Caudate Nucleus, Dominance, Cerebral, Imaging, Three-Dimensional, Mathematical Computing, Software, Software Validation, User-Computer Interface},
	pages = {1116--1128},
}

@article{vakharia_computer-assisted_2019,
	title = {Computer-{Assisted} {Planning} for {Stereoelectroencephalography} ({SEEG})},
	issn = {1878-7479},
	url = {https://doi.org/10.1007/s13311-019-00774-9},
	doi = {10.1007/s13311-019-00774-9},
	abstract = {Stereoelectroencephalography (SEEG) is a diagnostic procedure in which multiple electrodes are stereotactically implanted within predefined areas of the brain to identify the seizure onset zone, which needs to be removed to achieve remission of focal epilepsy. Computer-assisted planning (CAP) has been shown to improve trajectory safety metrics and generate clinically feasible trajectories in a fraction of the time needed for manual planning. We report a prospective validation study of the use of EpiNav (UCL, London, UK)~as a clinical decision support software for SEEG. Thirteen consecutive patients (125 electrodes) undergoing SEEG were prospectively recruited. EpiNav was used to generate 3D models of critical structures (including vasculature) and other important regions of interest. Manual planning utilizing the same 3D models was performed in advance of CAP. CAP was subsequently employed to automatically generate a plan for each patient. The treating neurosurgeon was able to modify CAP generated plans based on their preference. The plan with the lowest risk score metric was stereotactically implanted. In all cases (13/13), the final CAP generated plan returned a lower mean risk score and was stereotactically implanted. No complication or adverse event occurred. CAP trajectories were generated in 30\% of the time with significantly lower risk scores compared to manually generated. EpiNav has successfully been integrated as a clinical decision support software (CDSS) into the clinical pathway for SEEG implantations at our institution. To our knowledge, this is the first prospective study of a complex CDSS in stereotactic neurosurgery and provides the highest level of evidence to date.},
	language = {en},
	urldate = {2020-01-20},
	journal = {Neurotherapeutics},
	author = {Vakharia, Vejay N. and Sparks, Rachel and Miserocchi, Anna and Vos, Sjoerd B. and O{\textquoteright}Keeffe, Aidan and Rodionov, Roman and McEvoy, Andrew W. and Ourselin, Sebastien and Duncan, John S.},
	month = aug,
	year = {2019},
}

@inproceedings{ourselin_block_2000,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Block {Matching}: {A}~{General}~{Framework}~to~{Improve} {Robustness} of~{Rigid}~{Registration} of {Medical} {Images}},
	isbn = {978-3-540-40899-4},
	shorttitle = {Block {Matching}},
	doi = {10.1007/978-3-540-40899-4_57},
	abstract = {In order to improve the robustness of rigid registration algorithms in various medical imaging problems, we propose in this article a general framework built on block matching strategies. This framework combines two stages in a multi-scale hierarchy. The first stage consists in finding for each block (or subregion) of the first image, the most similar subregion in the other image, using a similarity criterion which depends on the nature of the images. The second stage consists in finding the global rigid transformation which best explains most of these local correspondances. This is done with a robust procedure which allows up to 50\% of false matches. We show that this approach, besides its simplicity, provides a robust and efficient way to rigidly register images in various situations. This includes for instance the alignment of 2D histological sections for the 3D reconstructions of trimmed organs and tissues, the automatic computation of the mid-sagittal plane in multimodal 3D images of the brain, and the multimodal registration of 3D CT and MR images of the brain. A quantitative evaluation of the results is provided for this last example, as well as a comparison with the classical approaches involving the minimization of a global measure of similarity based on Mutual Information or the Correlation Ratio. This shows a significant improvement of the robustness, for a comparable final accuracy. Although slightly more expensive in terms of computational requirements, the proposed approach can easily be implemented on a parallel architecture, which opens potentialities for real time applications using a large number of processors.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} {\textendash} {MICCAI} 2000},
	publisher = {Springer},
	author = {Ourselin, S. and Roche, A. and Prima, S. and Ayache, N.},
	editor = {Delp, Scott L. and DiGoia, Anthony M. and Jaramaz, Branislav},
	year = {2000},
	keywords = {Block Match, Correlation Ratio, Mutual Information, Registration Problem, Rigid Registration},
	pages = {557--566},
}

@article{iglesias_robust_2011,
	title = {Robust brain extraction across datasets and comparison with publicly available methods},
	volume = {30},
	issn = {1558-254X},
	doi = {10.1109/TMI.2011.2138152},
	abstract = {Automatic whole-brain extraction from magnetic resonance images (MRI), also known as skull stripping, is a key component in most neuroimage pipelines. As the first element in the chain, its robustness is critical for the overall performance of the system. Many skull stripping methods have been proposed, but the problem is not considered to be completely solved yet. Many systems in the literature have good performance on certain datasets (mostly the datasets they were trained/tuned on), but fail to produce satisfactory results when the acquisition conditions or study populations are different. In this paper we introduce a robust, learning-based brain extraction system (ROBEX). The method combines a discriminative and a generative model to achieve the final result. The discriminative model is a Random Forest classifier trained to detect the brain boundary; the generative model is a point distribution model that ensures that the result is plausible. When a new image is presented to the system, the generative model is explored to find the contour with highest likelihood according to the discriminative model. Because the target shape is in general not perfectly represented by the generative model, the contour is refined using graph cuts to obtain the final segmentation. Both models were trained using 92 scans from a proprietary dataset but they achieve a high degree of robustness on a variety of other datasets. ROBEX was compared with six other popular, publicly available methods (BET, BSE, FreeSurfer, AFNI, BridgeBurner, and GCUT) on three publicly available datasets (IBSR, LPBA40, and OASIS, 137 scans in total) that include a wide range of acquisition hardware and a highly variable population (different age groups, healthy/diseased). The results show that ROBEX provides significantly improved performance measures for almost every method/dataset combination.},
	language = {eng},
	number = {9},
	journal = {IEEE Trans Med Imaging},
	author = {Iglesias, Juan Eugenio and Liu, Cheng-Yi and Thompson, Paul M. and Tu, Zhuowen},
	month = sep,
	year = {2011},
	pmid = {21880566},
	keywords = {Algorithms, Brain, Humans, Image Processing, Computer-Assisted, Magnetic Resonance Imaging, Computer Simulation, Reproducibility of Results, Adult, Aged, Female, Male, Middle Aged, Sensitivity and Specificity, Database Management Systems, Databases, Factual, Discriminant Analysis, Electronic Data Processing, Models, Anatomic, Pattern Recognition, Automated, Skull},
	pages = {1617--1634},
}

@article{rorden_stereotaxic_2000,
	title = {Stereotaxic display of brain lesions},
	volume = {12},
	issn = {1875-8584},
	doi = {10.1155/2000/421719},
	abstract = {Traditionally lesion location has been reported using standard templates, text based descriptions or representative raw slices from the patient's CT or MRI scan. Each of these methods has drawbacks for the display of neuroanatomical data. One solution is to display MRI scans in the same stereotaxic space popular with researchers working in functional neuroimaging. Presenting brains in this format is useful as the slices correspond to the standard anatomical atlases used by neuroimagers. In addition, lesion position and volume are directly comparable across patients. This article describes freely available software for presenting stereotaxically aligned patient scans. This article focuses on MRI scans, but many of these tools are also applicable to other modalities (e.g. CT, PET and SPECT). We suggest that this technique of presenting lesions in terms of images normalized to standard stereotaxic space should become the standard for neuropsychological studies.},
	language = {eng},
	number = {4},
	journal = {Behavioural Neurology},
	author = {Rorden, Chris and Brett, Matthew},
	year = {2000},
	pmid = {11568431},
	pages = {191--200},
}

@article{bhavsar_machine_2017,
	title = {Machine {Learning} in {Transportation} {Data} {Analytics}},
	url = {https://www.researchwithnj.com/en/publications/machine-learning-in-transportation-data-analytics},
	doi = {https://doi.org/10.1016/B978-0-12-809715-1.00012-2},
	language = {English (US)},
	urldate = {2020-01-20},
	journal = {Data Analytics for Intelligent Transportation Systems},
	author = {Bhavsar, Parth and Safro, Ilya and Bouaynaya, Nidhal and Polikar, Robi and Dera, Dimah},
	month = apr,
	year = {2017},
	pages = {283--307},
	file = {Snapshot:/home/fernando/Zotero/storage/5ZGJKZL3/machine-learning-in-transportation-data-analytics.html:text/html},
}

@article{brett_spatial_2001,
	title = {Spatial {Normalization} of {Brain} {Images} with {Focal} {Lesions} {Using} {Cost} {Function} {Masking}},
	volume = {14},
	issn = {1053-8119},
	doi = {10.1006/nimg.2001.0845},
	language = {en},
	number = {2},
	urldate = {2020-01-20},
	journal = {NeuroImage},
	author = {Brett, Matthew and Leff, Alexander P. and Rorden, Chris and Ashburner, John},
	month = aug,
	year = {2001},
	pages = {486--500},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/8TJZWGKF/S1053811901908456.html:text/html},
}

@article{otsu_threshold_1979,
	title = {A {Threshold} {Selection} {Method} from {Gray}-{Level} {Histograms}},
	volume = {9},
	issn = {2168-2909},
	doi = {10.1109/TSMC.1979.4310076},
	number = {1},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Otsu, Nobuyuki},
	month = jan,
	year = {1979},
	keywords = {Displays, Gaussian distribution, Histograms, Least squares approximation, Marine vehicles, Q measurement, Radar tracking, Sea measurements, Surveillance, Target tracking},
	pages = {62--66},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/UTBGIDAC/4310076.html:text/html},
}

@article{perlin_image_1985,
	title = {An image synthesizer},
	volume = {19},
	issn = {0097-8930},
	url = {https://doi.org/10.1145/325165.325247},
	doi = {10.1145/325165.325247},
	abstract = {We introduce the concept of a Pixel Stream Editor. This forms the basis for an interactive synthesizer for designing highly realistic Computer Generated Imagery. The designer works in an interactive Very High Level programming environment which provides a very fast concept/implement/view iteration cycle.Naturalistic visual complexity is built up by composition of non-linear functions, as opposed to the more conventional texture mapping or growth model algorithms. Powerful primitives are included for creating controlled stochastic effects. We introduce the concept of "solid texture" to the field of CGI.We have used this system to create very convincing representations of clouds, fire, water, stars, marble, wood, rock, soap films and crystal. The algorithms created with this paradigm are generally extremely fast, highly realistic, and asynchronously parallelizable at the pixel level.},
	number = {3},
	urldate = {2020-01-15},
	journal = {ACM SIGGRAPH Computer Graphics},
	author = {Perlin, Ken},
	month = jul,
	year = {1985},
	keywords = {algorithm development, fire, functional composition, interactive, pixel stream editor, solid texture, space function, stochastic modelling, turbulence, waves},
	pages = {287--296},
}

@misc{noauthor_ixi_nodate,
	title = {{IXI} {Dataset} {\textendash} {Brain} {Development}},
	url = {https://brain-development.org/ixi-dataset/},
	language = {en-US},
	urldate = {2020-01-15},
	file = {Snapshot:/home/fernando/Zotero/storage/I72EPGJ5/ixi-dataset.html:text/html},
}

@inproceedings{fidon_scalable_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Scalable {Multimodal} {Convolutional} {Networks} for {Brain} {Tumour} {Segmentation}},
	isbn = {978-3-319-66179-7},
	doi = {10.1007/978-3-319-66179-7_33},
	abstract = {Brain tumour segmentation plays a key role in computer-assisted surgery. Deep neural networks have increased the accuracy of automatic segmentation significantly, however these models tend to generalise poorly to different imaging modalities than those for which they have been designed, thereby limiting their applications. For example, a network architecture initially designed for brain parcellation of monomodal T1 MRI can not be easily translated into an efficient tumour segmentation network that jointly utilises T1, T1c, Flair and T2 MRI. To tackle this, we propose a novel scalable multimodal deep learning architecture using new nested structures that explicitly leverage deep features within or across modalities. This aims at making the early layers of the architecture structured and sparse so that the final architecture becomes scalable to the number of modalities. We evaluate the scalable architecture for brain tumour segmentation and give evidence of its regularisation effect compared to the conventional concatenation approach.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} - {MICCAI} 2017},
	publisher = {Springer International Publishing},
	author = {Fidon, Lucas and Li, Wenqi and Garcia-Peraza-Herrera, Luis C. and Ekanayake, Jinendra and Kitchen, Neil and Ourselin, Sebastien and Vercauteren, Tom},
	editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D. Louis and Duchesne, Simon},
	year = {2017},
	pages = {285--293},
}

@article{mercier_online_2012,
	title = {Online database of clinical {MR} and ultrasound images of brain tumors},
	volume = {39},
	issn = {0094-2405},
	doi = {10.1118/1.4709600},
	abstract = {PURPOSE: One of the important challenges in the field of medical imaging is finding real clinical images with which to validate new image processing algorithms. This is particularly true for tracked 3D ultrasound images of the brain.
METHODS: In 2010, pre- and postoperative magnetic resonance and intraoperative ultrasound images were acquired from brain tumor patients involved in the authors' imaging study at the Montreal Neurological Institute.
RESULTS: These data are available online at the Montreal Neurological Institute's Brain Images of Tumors for Evaluation database, termed here the MNI BITE database. It contains ultrasound and magnetic resonance images from 14 patients. Each patient underwent a preoperative and a postoperative T1-weighted magnetic resonance scan with gadolinium enhancement, and multiple intraoperative B-mode images were acquired before and after resection. Corresponding features were manually selected in some image pairs for validation. All images are in MINC format, the file format used at the authors' institute for image processing. The MINC tools are available for free download at packages.bic.mni.mcgill.ca.
CONCLUSIONS: This is the first online database of its kind. These images can be used by image processing scientists as well as clinicians wishing to compare findings from magnetic resonance and ultrasound imaging.},
	language = {eng},
	number = {6},
	journal = {Medical Physics},
	author = {Mercier, Laurence and Del Maestro, Rolando F. and Petrecca, Kevin and Araujo, David and Haegelen, Claire and Collins, D. Louis},
	month = jun,
	year = {2012},
	pmid = {22755708},
	keywords = {Humans, Image Processing, Computer-Assisted, Magnetic Resonance Imaging, Adult, Aged, Female, Male, Middle Aged, Young Adult, Ultrasonography, Databases, Factual, Brain Neoplasms, Online Systems, Survival Rate},
	pages = {3253--3261},
}

@article{xiao_retrospective_2017,
	title = {{REtroSpective} {Evaluation} of {Cerebral} {Tumors} ({RESECT}): {A} clinical database of pre-operative {MRI} and intra-operative ultrasound in low-grade glioma surgeries},
	volume = {44},
	copyright = {{\textcopyright} 2017 American Association of Physicists in Medicine},
	issn = {2473-4209},
	shorttitle = {{REtroSpective} {Evaluation} of {Cerebral} {Tumors} ({RESECT})},
	url = {https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.12268},
	doi = {10.1002/mp.12268},
	abstract = {Purpose The advancement of medical image processing techniques, such as image registration, can effectively help improve the accuracy and efficiency of brain tumor surgeries. However, it is often challenging to validate these techniques with real clinical data due to the rarity of such publicly available repositories. Acquisition and validation methods Pre-operative magnetic resonance images (MRI), and intra-operative ultrasound (US) scans were acquired from 23 patients with low-grade gliomas who underwent surgeries at St. Olavs University Hospital between 2011 and 2016. Each patient was scanned by Gadolinium-enhanced T1w and T2-FLAIR MRI protocols to reveal the anatomy and pathology, and series of B-mode ultrasound images were obtained before, during, and after tumor resection to track the surgical progress and tissue deformation. Retrospectively, corresponding anatomical landmarks were identified across US images of different surgical stages, and between MRI and US, and can be used to validate image registration algorithms. Quality of landmark identification was assessed with intra- and inter-rater variability. Data format and access In addition to co-registered MRIs, each series of US scans are provided as a reconstructed 3D volume. All images are accessible in MINC2 and NIFTI formats, and the anatomical landmarks were annotated in MNI tag files. Both the imaging data and the corresponding landmarks are available online as the RESECT database at https://archive.norstore.no (search for {\textquotedblleft}RESECT{\textquotedblright}). Potential impact The proposed database provides real high-quality multi-modal clinical data to validate and compare image registration algorithms that can potentially benefit the accuracy and efficiency of brain tumor resection. Furthermore, the database can also be used to test other image processing methods and neuro-navigation software platforms.},
	language = {en},
	number = {7},
	urldate = {2020-01-15},
	journal = {Medical Physics},
	author = {Xiao, Yiming and Fortin, Maryse and Unsg{\r a}rd, Geirmund and Rivaz, Hassan and Reinertsen, Ingerid},
	year = {2017},
	keywords = {MRI, registration, brain tumor, database, intra-operative ultrasound, low-grade glioma},
	pages = {3875--3882},
	file = {Snapshot:/home/fernando/Zotero/storage/3NHFUTVW/mp.html:text/html},
}

@article{pan_survey_2010,
	title = {A {Survey} on {Transfer} {Learning}},
	volume = {22},
	issn = {2326-3865},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
	number = {10},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	month = oct,
	year = {2010},
	keywords = {Data mining, Machine learning, Transfer learning, data mining, data mining., inductive transfer learning, knowledge engineering, Knowledge engineering, knowledge transfer, Knowledge transfer, Labeling, learning by example, Learning systems, machine learning, Machine learning algorithms, optimisation, Space technology, survey, Testing, Training data, transductive transfer learning, unsupervised learning, unsupervised transfer learning},
	pages = {1345--1359},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/39RPVFBG/5288526.html:text/html},
}

@article{chen_self-supervised_2019,
	title = {Self-supervised learning for medical image analysis using image context restoration},
	volume = {58},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841518304699},
	doi = {10.1016/j.media.2019.101539},
	abstract = {Machine learning, particularly deep learning has boosted medical image analysis over the past years. Training a good model based on deep learning requires large amount of labelled data. However, it is often difficult to obtain a sufficient number of labelled images for training. In many scenarios the dataset in question consists of more unlabelled images than labelled ones. Therefore, boosting the performance of machine learning models by using unlabelled as well as labelled data is an important but challenging problem. Self-supervised learning presents one possible solution to this problem. However, existing self-supervised learning strategies applicable to medical images cannot result in significant performance improvement. Therefore, they often lead to only marginal improvements. In this paper, we propose a novel self-supervised learning strategy based on context restoration in order to better exploit unlabelled images. The context restoration strategy has three major features: 1) it learns semantic image features; 2) these image features are useful for different types of subsequent image analysis tasks; and 3) its implementation is simple. We validate the context restoration strategy in three common problems in medical imaging: classification, localization, and segmentation. For classification, we apply and test it to scan plane detection in fetal 2D ultrasound images; to localise abdominal organs in CT images; and to segment brain tumours in multi-modal MR images. In all three cases, self-supervised learning based on context restoration learns useful semantic features and lead to improved machine learning models for the above tasks.},
	language = {en},
	urldate = {2020-01-13},
	journal = {Medical Image Analysis},
	author = {Chen, Liang and Bentley, Paul and Mori, Kensaku and Misawa, Kazunari and Fujiwara, Michitaka and Rueckert, Daniel},
	month = dec,
	year = {2019},
	keywords = {Medical image analysis, Context restoration, Self-supervised learning},
	pages = {101539},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/SE3WEYMI/S1361841518304699.html:text/html},
}

@article{jacobsen_analysis_2019,
	series = {Special {Issue}: {Deep} {Learning} in {Medical} {Physics}},
	title = {Analysis of intensity normalization for optimal segmentation performance of a fully convolutional neural network},
	volume = {29},
	issn = {0939-3889},
	url = {http://www.sciencedirect.com/science/article/pii/S0939388918301028},
	doi = {10.1016/j.zemedi.2018.11.004},
	abstract = {Introduction
Convolutional neural networks have begun to surpass classical statistical- and atlas based machine learning techniques in medical image segmentation in recent years, proving to be superior in performance and speed. However, a major challenge that the community faces are mismatch between variability within training and evaluation datasets and therefore a dependency on proper data pre-processing. Intensity normalization is a widely applied technique for reducing the variance of the data for which there are several methods available ranging from uniformity transformation to histogram equalization. The current study analyses the influence of intensity normalization on cerebellum segmentation performance of a convolutional neural network (CNN).
Method
The study included three population samples with a total number of 218 datasets, all including a T1w MRI data set acquired at 3T and a ground truth segmentation delineating the cerebellum. A 12 layer deep 3D fully convolutional neural network was trained using 150 datasets from one of the population samples. Four different intensity normalization methods were separately applied to pre-process the data, and the CNN was correspondingly trained four times with respect to the different normalization techniques. A quantitative analysis of the segmentation performance, assessed via the S{\o}rensen{\textendash}Dice similarity coefficient (DSC) of all four CNNs, was performed to investigate the intensity sensitivity of the CNNs. Additionally, the optimal network performance was determined by identifying the best parameter set for intensity normalization.
Results
All four normalization methods led to excellent (mean DSC score=0.96) segmentation results when evaluated using known data; however, the segmentation performance differed depending on the applied intensity normalization method when testing with formerly unseen data, in which case the histogram equalization methods outperformed the unit distribution methods. A detailed, systematic analysis of intensity manipulations revealed, that the distribution of input intensities clearly affected the segmentation performance and that for each input dataset a linear intensity modification (shifting and scaling) existed leading to optimal segmentation results. This was further proven by an optimization analysis to find the optimal adjustment for an individual input evaluation sample within each normalization configuration.
Discussion
The findings suggest that proper preparation of the evaluation data is more crucial than the exact choice of normalization method to prepare the training data. The histogram equalization methods tested in this study were found to perform this task best, although leaving room for further improvements, as shown by the optimization analysis.},
	language = {en},
	number = {2},
	urldate = {2020-01-13},
	journal = {Zeitschrift f{\"u}r Medizinische Physik},
	author = {Jacobsen, Nina and Deistung, Andreas and Timmann, Dagmar and Goericke, Sophia L. and Reichenbach, J{\"u}rgen R. and G{\"u}llmar, Daniel},
	month = may,
	year = {2019},
	keywords = {Deep learning, Convolutional neural network, Segmentation, MRI, Cerebellum, Intensity normalization, Pre-processing},
	pages = {128--138},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/3NF3UTS6/S0939388918301028.html:text/html},
}

@article{meier_automatic_2017,
	title = {Automatic estimation of extent of resection and residual tumor volume of patients with glioblastoma},
	volume = {127},
	issn = {1933-0693},
	doi = {10.3171/2016.9.JNS16146},
	abstract = {OBJECTIVE In the treatment of glioblastoma, residual tumor burden is the only prognostic factor that can be actively influenced by therapy. Therefore, an accurate, reproducible, and objective measurement of residual tumor burden is necessary. This study aimed to evaluate the use of a fully automatic segmentation method-brain tumor image analysis (BraTumIA)-for estimating the extent of resection (EOR) and residual tumor volume (RTV) of contrast-enhancing tumor after surgery. METHODS The imaging data of 19 patients who underwent primary resection of histologically confirmed supratentorial glioblastoma were retrospectively reviewed. Contrast-enhancing tumors apparent on structural preoperative and immediate postoperative MR imaging in this patient cohort were segmented by 4 different raters and the automatic segmentation BraTumIA software. The manual and automatic results were quantitatively compared. RESULTS First, the interrater variabilities in the estimates of EOR and RTV were assessed for all human raters. Interrater agreement in terms of the coefficient of concordance (W) was higher for RTV (W = 0.812; p {\textless} 0.001) than for EOR (W = 0.775; p {\textless} 0.001). Second, the volumetric estimates of BraTumIA for all 19 patients were compared with the estimates of the human raters, which showed that for both EOR (W = 0.713; p {\textless} 0.001) and RTV (W = 0.693; p {\textless} 0.001) the estimates of BraTumIA were generally located close to or between the estimates of the human raters. No statistically significant differences were detected between the manual and automatic estimates. BraTumIA showed a tendency to overestimate contrast-enhancing tumors, leading to moderate agreement with expert raters with respect to the literature-based, survival-relevant threshold values for EOR. CONCLUSIONS BraTumIA can generate volumetric estimates of EOR and RTV, in a fully automatic fashion, which are comparable to the estimates of human experts. However, automated analysis showed a tendency to overestimate the volume of a contrast-enhancing tumor, whereas manual analysis is prone to subjectivity, thereby causing considerable interrater variability.},
	language = {eng},
	number = {4},
	journal = {Journal of Neurosurgery},
	author = {Meier, Raphael and Porz, Nicole and Knecht, Urspeter and Loosli, Tina and Schucht, Philippe and Beck, J{\"u}rgen and Slotboom, Johannes and Wiest, Roland and Reyes, Mauricio},
	month = oct,
	year = {2017},
	pmid = {28059651},
	keywords = {Humans, Retrospective Studies, automatic tumor volumetry, BraTumIA, BraTumIA = brain tumor image analysis, CET = contrast-enhancing tumor, CRET = complete resection of the enhancing tumor, EOR = extent of resection, extent of resection, glioblastoma, Glioblastoma, MPR = multiplanar reconstruction, Neoplasm, Residual, Neurosurgical Procedures, oncology, PRET = partial resection of the enhancing tumor, residual tumor volume, RTV = residual tumor volume, Supratentorial Neoplasms, T1w = T1-weighted, T2w = T2-weighted, Tumor Burden, W = Kendall's coefficient of concordance},
	pages = {798--806},
}

@article{rueckert_nonrigid_1999,
	title = {Nonrigid registration using free-form deformations: application to breast {MR} images},
	volume = {18},
	issn = {1558-254X},
	shorttitle = {Nonrigid registration using free-form deformations},
	doi = {10.1109/42.796284},
	abstract = {In this paper the authors present a new approach for the nonrigid registration of contrast-enhanced breast MRI. A hierarchical transformation model of the motion of the breast has been developed. The global motion of the breast is modeled by an affine transformation while the local breast motion is described by a free-form deformation (FFD) based on B-splines. Normalized mutual information is used as a voxel-based similarity measure which is insensitive to intensity changes as a result of the contrast enhancement. Registration is achieved by minimizing a cost function, which represents a combination of the cost associated with the smoothness of the transformation and the cost associated with the image similarity. The algorithm has been applied to the fully automated registration of three-dimensional (3-D) breast MRI in volunteers and patients. In particular, the authors have compared the results of the proposed nonrigid registration algorithm to those obtained using rigid and affine registration techniques. The results clearly indicate that the nonrigid registration algorithm is much better able to recover the motion and deformation of the breast than rigid or affine registration algorithms.},
	number = {8},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Rueckert, D. and Sonoda, L.I. and Hayes, C. and Hill, D.L.G. and Leach, M.O. and Hawkes, D.J.},
	month = aug,
	year = {1999},
	keywords = {Humans, Image Processing, Computer-Assisted, Magnetic Resonance Imaging, Female, image motion analysis, biomedical MRI, Magnetic resonance imaging, medical image processing, MRI, biomechanics, affine transformation, B-splines, Biomedical imaging, Breast, Breast cancer, breast motion, Breast tissue, Cancer detection, contrast enhancement, Cost function, Cyclic redundancy check, Diseases, free-form deformation, global motion, hierarchical transformation model, image registration, intensity changes, local breast motion, magnetic resonance imaging, mammography, Mammography, medical diagnostic imaging, Medical diagnostic imaging, MR mammography, splines (mathematics), voxel-based similarity measure},
	pages = {712--721},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/P3G2URTF/796284.html:text/html},
}

@article{papademetris_bioimage_2006,
	title = {{BioImage} {Suite}: {An} integrated medical image analysis suite: {An} update},
	volume = {2006},
	issn = {2327-770X},
	shorttitle = {{BioImage} {Suite}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4213804/},
	abstract = {BioImage Suite is an NIH-supported medical image analysis software suite developed at Yale. It leverages both the Visualization Toolkit (VTK) and the Insight Toolkit (ITK) and it includes many additional algorithms for image analysis especially in the areas of segmentation, registration, diffusion weighted image processing and fMRI analysis. BioImage Suite has a user-friendly user interface developed in the Tcl scripting language. A final beta version is freely available for download},
	urldate = {2020-01-12},
	journal = {The insight journal},
	author = {Papademetris, Xenophon and Jackowski, Marcel P. and Rajeevan, Nallakkandi and DiStasio, Marcello and Okuda, Hirohito and Constable, R. Todd and Staib, Lawrence H.},
	year = {2006},
	pmid = {25364771},
	pmcid = {PMC4213804},
	pages = {209},
}

@article{winterstein_partially_2010,
	title = {Partially resected gliomas: diagnostic performance of fluid-attenuated inversion recovery {MR} imaging for detection of progression},
	volume = {254},
	issn = {1527-1315},
	shorttitle = {Partially resected gliomas},
	doi = {10.1148/radiol09090893},
	abstract = {PURPOSE: To assess whether signal intensity (SI) different from that of cerebrospinal fluid (CSF) within the resection cavity during follow-up helps predict tumor progression in partially resected gliomas.
MATERIALS AND METHODS: This retrospective study had local institutional review board approval, with waiver of informed consent. Seventy-five patients with partially resected and irradiated gliomas were evaluated. SI within the resection cavity on fluid-attenuated inversion recovery (FLAIR) magnetic resonance (MR) images was qualitatively and quantitatively assessed during follow-up. Qualitative analysis comprised visual comparison of SI in the resection cavity with SI of normal CSF by two readers. SI of the cavity was quantitatively assessed with region-of-interest (ROI) analysis normalized to background noise, contralateral healthy white matter, and CSF. Normalized SI during follow-up was compared with SI immediately after resection. Tumor progression was defined as increase in longest glioma diameter of at least 20\% (Response Evaluation Criteria in Solid Tumors). Sensitivity and specificity of elevated SI in resection cavities for predicting or indicating tumor progression were calculated. Wilcoxon rank-sum test, Hodges-Lehman estimates, Kaplan-Meier curves, and linear mixed-effect models for repeated-measures data were used for quantitative SI measurements.
RESULTS: Tumor progression at MR was seen in 44 patients (59\%), and median progression-free survival was 4.1 years. Qualitative analysis showed that 25 of 44 patients with progression (57\%) had SI increase in the resection cavity on FLAIR images. In 10 patients with progression (23\%), SI increase was seen a mean of 5 months +/- 3 (standard deviation) before tumor size progression. In 15 patients with progression (34\%), SI increase and tumor size progression were detected on the same MR study. In 19 patients with progressing glioma (43\%), no SI increase was observed qualitatively. Among 31 patients without progression during follow-up (41\%), no SI increase could be observed. Quantitative analysis showed no significant differences in ROI ratios at baseline (after surgery) between progressing and nonprogressing tumors, whereas significant differences in change of ROI ratios at the last measurement could be detected. Overall, SI increase on FLAIR images had specificity of 100\% (95\% confidence interval [CI]: 91\%, 100\%) and sensitivity of 57\% (95\% CI: 42\%, 71\%) for glioma progression.
CONCLUSION: In partially resected gliomas, encapsulation of resection cavity, presumably by tumor cells, manifests as SI increase on FLAIR images and indicates tumor progression with very high specificity. (c) RSNA, 2010.},
	language = {eng},
	number = {3},
	journal = {Radiology},
	author = {Winterstein, Marianne and M{\"u}nter, Marc W. and Burkholder, Iris and Essig, Marco and Kauczor, Hans-Ulrich and Weber, Marc-Andr{\'e}},
	month = mar,
	year = {2010},
	pmid = {20177101},
	keywords = {Humans, Image Processing, Computer-Assisted, Magnetic Resonance Imaging, Adult, Female, Male, Sensitivity and Specificity, Retrospective Studies, Linear Models, Brain Neoplasms, Combined Modality Therapy, Disease Progression, Follow-Up Studies, Glioma, Radiotherapy Dosage, Statistics, Nonparametric, Survival Analysis},
	pages = {907--916},
}

@article{rosenow_presurgical_2001,
	title = {Presurgical evaluation of epilepsy},
	volume = {124},
	issn = {0006-8950},
	doi = {10.1093/brain/124.9.1683},
	abstract = {Abstract.  An overview of the following six cortical zones that have been defined in the presurgical evaluation of candidates for epilepsy surgery is given: the},
	language = {en},
	number = {9},
	urldate = {2020-01-11},
	journal = {Brain},
	author = {Rosenow, Felix and L{\"u}ders, Hans},
	month = sep,
	year = {2001},
	pages = {1683--1700},
	file = {Snapshot:/home/fernando/Zotero/storage/DTRRRGJ9/303186.html:text/html},
}

@book{oppenheim_digital_1975,
	address = {Englewood Cliffs, N.J},
	edition = {US Ed edition},
	title = {Digital {Signal} {Processing}},
	isbn = {978-0-13-214635-7},
	abstract = {An up-to-date and detailed introduction to the fundamentals of processing signals by digital techniques and their applications to practical problems.},
	language = {English},
	publisher = {Pearson},
	author = {Oppenheim, Alan V. and Schafer, Ronald W.},
	month = jan,
	year = {1975},
}

@article{qian_non-parametric_2009,
	title = {A non-parametric vessel detection method for complex vascular structures},
	volume = {13},
	issn = {13618415},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S136184150800056X},
	doi = {10.1016/j.media.2008.05.005},
	abstract = {Modern medical imaging techniques enable the acquisition of in vivo high resolution images of the vascular system. Most common methods for the detection of vessels in these images, such as multiscale Hessian-based operators and matched filters, rely on the assumption that at each voxel there is a single cylinder. Such an assumption is clearly violated at the multitude of branching points that are easily observed in all, but the most focused vascular image studies. In this paper, we propose a novel method for detecting vessels in medical images that relaxes this single cylinder assumption. We directly exploit local neighborhood intensities and extract characteristics of the local intensity profile (in a spherical polar coordinate system) which we term as the polar neighborhood intensity profile. We present a new method to capture the common properties shared by polar neighborhood intensity profiles for all the types of vascular points belonging to the vascular system. The new method enables us to detect vessels even near complex extreme points, including branching points. Our method demonstrates improved performance over standard methods on both 2D synthetic images and 3D animal and clinical vascular images, particularly close to vessel branching regions.},
	language = {en},
	number = {1},
	urldate = {2018-08-19},
	journal = {Medical Image Analysis},
	author = {Qian, Xiaoning and Brennan, Matthew P. and Dione, Donald P. and Dobrucki, Wawrzyniec L. and Jackowski, Marcel P. and Breuer, Christopher K. and Sinusas, Albert J. and Papademetris, Xenophon},
	month = feb,
	year = {2009},
	pages = {49--61},
}

@article{shim_robust_2006,
	title = {Robust segmentation of cerebral arterial segments by a sequential {Monte} {Carlo} method: {Particle} filtering},
	volume = {84},
	issn = {01692607},
	shorttitle = {Robust segmentation of cerebral arterial segments by a sequential {Monte} {Carlo} method},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0169260706001945},
	doi = {10.1016/j.cmpb.2006.09.001},
	abstract = {In this paper a method to extract cerebral arterial segments from CT angiography (CTA) is proposed. The segmentation of cerebral arteries in CTA is a challenging task mainly due to bone contact and vein contamination. The proposed method considers a vessel segment as an ellipse travelling in three-dimensional (3D) space and segments it out by tracking the ellipse in spatial sequence. A particle filter is employed as the main framework for tracking and is equipped with adaptive properties to both bone contact and vein contamination. The proposed tracking method is evaluated by the experiments on both synthetic and actual data. A variety of vessels were synthesized to assess the sensitivity to the axis curvature change, obscure boundaries, and noise. The experimental results showed that the proposed method is also insensitive to parameter settings and requires less user intervention than the conventional vessel tracking methods, which proves its improved robustness.},
	language = {en},
	number = {2-3},
	urldate = {2018-08-19},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Shim, Hackjoon and Kwon, Dongjin and Yun, Il Dong and Lee, Sang Uk},
	month = dec,
	year = {2006},
	pages = {135--145},
}

@article{zhou_segmentation_2013,
	title = {Segmentation of brain magnetic resonance angiography images based on {MAP}{\textendash}{MRF} with multi-pattern neighborhood system and approximation of regularization coefficient},
	volume = {17},
	issn = {13618415},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1361841513001199},
	doi = {10.1016/j.media.2013.08.005},
	abstract = {Existing maximum a posteriori probability and Markov random field (MRF) models have limitations associated with: (1) the ordinary neighborhood system being unable to differentiate subtle changes due to several-to-one correspondence within the neighborhood; and (2) difficulty finding an appropriate parameter to balance between the spatial context and the data likelihood. Aiming at overcoming the limitations and applications to segmentation of cerebral vessels from magnetic resonance angiography images, we have proposed (1) a multi-pattern neighborhood system and corresponding energy equation to enable the MRF model for segmenting fine cerebral vessels with complicated context; and (2) an iterative approximation algorithm based on the maximum pseudo-likelihood and the space coding mode for the automatic parameter estimation of high level model of MRF. In the implementation, two computational strategies have been employed to speed up: the candidate space of cerebral vessels based on a threshold of the response to multi-scale filtering, and parallel computation of major equations. Three phantoms simulating segmentation challenges of vessels have been devised to quantitatively validate the algorithm. In addition, 10 three-dimensional clinical data sets have been used to validate the algorithm qualitatively. It has been shown that the proposed method could yield smaller error, improve the spatial resolution of MRF model, and better balance the smoothing and data likelihood than the traditional trial-and-error estimation. Comparative studies have shown that the proposed method is better than the 3 segmentation algorithms (Hassouna et al., 2006; Hao et al., 2008; Gao et al., 2011) in terms of segmentation accuracy, robustness to noise and varying curvatures as well as radii.},
	language = {en},
	number = {8},
	urldate = {2018-08-19},
	journal = {Medical Image Analysis},
	author = {Zhou, Shoujun and Chen, Wufan and Jia, Fucang and Hu, Qingmao and Xie, Yaoqin and Chen, Mingyang and Wu, Jianhuang},
	month = dec,
	year = {2013},
	pages = {1220--1235},
}

@article{law_segmentation_2013,
	title = {Segmentation of {Intracranial} {Vessels} and {Aneurysms} in {Phase} {Contrast} {Magnetic} {Resonance} {Angiography} {Using} {Multirange} {Filters} and {Local} {Variances}},
	volume = {22},
	issn = {1057-7149, 1941-0042},
	url = {http://ieeexplore.ieee.org/document/6291783/},
	doi = {10.1109/TIP.2012.2216274},
	abstract = {Segmentation of intensity varying and low-contrast structures is an extremely challenging and rewarding task. In computer-aided diagnosis of intracranial aneurysms, segmenting the high-intensity major vessels along with the attached lowcontrast aneurysms is essential to the recognition of this lethal vascular disease. It is particularly helpful in performing early and noninvasive diagnosis of intracranial aneurysms using phase contrast magnetic resonance angiographic (PC-MRA) images. The major challenges of developing a PC-MRA-based segmentation method are the significantly varying voxel intensity inside vessels with different flow velocities and the signal loss in the aneurysmal regions where turbulent flows occur. This paper proposes a novel intensity-based algorithm to segment intracranial vessels and the attached aneurysms. The proposed method can handle intensity varying vasculatures and also the low-contrast aneurysmal regions affected by turbulent flows. It is grounded on the use of multirange filters and local variances to extract intensity-based image features for identifying contrast varying vasculatures. The extremely low-intensity region affected by turbulent flows is detected according to the topology of the structure detected by multirange filters and local variances. The proposed method is evaluated using a phantom image volume with an aneurysm and four clinical cases. It achieves 0.80 dice score in the phantom case. In addition, different components of the proposed method{\textemdash}the multirange filters, local variances, and topology-based detection{\textemdash}are evaluated in the comparison between the proposed method and its lower complexity variants. Owing to the analogy between these variants and existing vascular segmentation methods, this comparison also exemplifies the advantage of the proposed method over the existing approaches. It analyzes the weaknesses of these existing approaches and justifies the use of every component involved in the proposed method. It is shown that the proposed method is capable of segmenting blood vessels and the attached aneurysms on PC-MRA images.},
	language = {en},
	number = {3},
	urldate = {2018-08-19},
	journal = {IEEE Transactions on Image Processing},
	author = {Law, Max W. K. and Chung, Albert C. S.},
	month = mar,
	year = {2013},
	pages = {845--859},
}

@article{flaaris_method_2004,
	title = {Method for modelling cerebral blood vessels and their bifurcations using circular, homogeneous, generalised cylinders},
	volume = {42},
	issn = {0140-0118, 1741-0444},
	url = {http://link.springer.com/10.1007/BF02344628},
	doi = {10.1007/BF02344628},
	abstract = {A method for automatic mode/ring of blood vessels and their bifurcations from 3D scans of the brain is presented. The method is a three-step procedure. First, a skeleton of the cerebra/ blood vessels is developed, and then the surfaces of the blood vessels are located using an active contour approach. The active contour approach uses circular homogeneous genera/ised cylinders (CHGCs) to mode/ the thin, elongated blood vessels. Finally, a novel method for mode/ring the surfaces of the bifurcations in a vessel tree is presented. The method was tested on simulated data: a computed tomography angiography (CTA) and four magnetic resonance angiography (MRA) volumes. Furthermore, the method was tested on ten magnetic resonance images (MR/s) to demonstrate its robustness. The test on the simulated data indicated that the approach for the surface modelling of vessels had a mean radius error of less than O.l m m and a mean Iocalisation error of O.l mm. Surface models evaluated by an expert in vascular neurosurgery were found to have a smooth appearance and generally agreed with the image data. The test on the MR/ scans indicated that the method performed well in noisy environments.},
	language = {en},
	number = {2},
	urldate = {2018-08-19},
	journal = {Medical \& Biological Engineering \& Computing},
	author = {Flaaris, J. J. and Volden, M. and Haase, J. and {\O}stergaard, L. R.},
	month = mar,
	year = {2004},
	pages = {171--177},
}

@article{salehi_auto-context_2017,
	title = {Auto-context {Convolutional} {Neural} {Network} ({Auto}-{Net}) for {Brain} {Extraction} in {Magnetic} {Resonance} {Imaging}},
	url = {http://arxiv.org/abs/1703.02083},
	abstract = {Brain extraction or whole brain segmentation is an important first step in many of the neuroimage analysis pipelines. The accuracy and robustness of brain extraction, therefore, is crucial for the accuracy of the entire brain analysis process. With the aim of designing a learning-based, geometry-independent and registration-free brain extraction tool in this study, we present a technique based on an auto-context convolutional neural network (CNN), in which intrinsic local and global image features are learned through 2D patches of different window sizes. In this architecture three parallel 2D convolutional pathways for three different directions (axial, coronal, and sagittal) implicitly learn 3D image information without the need for computationally expensive 3D convolutions. Posterior probability maps generated by the network are used iteratively as context information along with the original image patches to learn the local shape and connectedness of the brain, to extract it from non-brain tissue. The brain extraction results we have obtained from our algorithm are superior to the recently reported results in the literature on two publicly available benchmark datasets, namely LPBA40 and OASIS, in which we obtained Dice overlap coefficients of 97.42\% and 95.40\%, respectively. Furthermore, we evaluated the performance of our algorithm in the challenging problem of extracting arbitrarily-oriented fetal brains in reconstructed fetal brain magnetic resonance imaging (MRI) datasets. In this application our algorithm performed much better than the other methods (Dice coefficient: 95.98\%), where the other methods performed poorly due to the non-standard orientation and geometry of the fetal brain in MRI. Our CNN-based method can provide accurate, geometry-independent brain extraction in challenging applications.},
	urldate = {2018-08-19},
	journal = {arXiv:1703.02083 [cs]},
	author = {Salehi, Seyed Sadegh Mohseni and Erdogmus, Deniz and Gholipour, Ali},
	month = mar,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{pereira_brain_2016,
	title = {Brain {Tumor} {Segmentation} {Using} {Convolutional} {Neural} {Networks} in {MRI} {Images}},
	volume = {35},
	issn = {0278-0062},
	doi = {10.1109/TMI.2016.2538465},
	abstract = {Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 {\texttimes}3 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.},
	number = {5},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Pereira, S. and Pinto, A. and Alves, V. and Silva, C. A.},
	month = may,
	year = {2016},
	keywords = {Humans, Magnetic Resonance Imaging, data augmentation, Machine Learning, Training, convolutional neural networks, biomedical MRI, brain, cancer, Magnetic resonance imaging, medical image processing, tumours, deep learning, neurophysiology, image segmentation, Image segmentation, Brain Neoplasms, magnetic resonance imaging, Glioma, automatic segmentation, automatic segmentation methods, Brain modeling, Brain tumor, brain tumor segmentation, clinical practice, CNN-based segmentation methods, Computer-Assisted, Context, Dice similarity coefficient metrics, glioma, gliomas, Image Interpretation, imaging technique, intensity normalization, Kernel, kernels, manual segmentation, MRI images, Neural Networks (Computer), on-site BRATS 2015 Challenge, oncological patients, online evaluation platform, precise quantitative measurements, preprocessing step, quality-of-life, reliable segmentation methods, spatial variability, structural variability, Tumors},
	pages = {1240--1251},
}

@article{salehi_tversky_2017,
	title = {Tversky loss function for image segmentation using {3D} fully convolutional deep networks},
	url = {http://arxiv.org/abs/1706.05721},
	abstract = {Fully convolutional deep neural networks carry out excellent potential for fast and accurate image segmentation. One of the main challenges in training these networks is data imbalance, which is particularly problematic in medical imaging applications such as lesion segmentation where the number of lesion voxels is often much lower than the number of non-lesion voxels. Training with unbalanced data can lead to predictions that are severely biased towards high precision but low recall (sensitivity), which is undesired especially in medical applications where false negatives are much less tolerable than false positives. Several methods have been proposed to deal with this problem including balanced sampling, two step training, sample re-weighting, and similarity loss functions. In this paper, we propose a generalized loss function based on the Tversky index to address the issue of data imbalance and achieve much better trade-off between precision and recall in training 3D fully convolutional deep neural networks. Experimental results in multiple sclerosis lesion segmentation on magnetic resonance images show improved F2 score, Dice coefficient, and the area under the precision-recall curve in test data. Based on these results we suggest Tversky loss function as a generalized framework to effectively train deep neural networks.},
	urldate = {2018-08-18},
	journal = {arXiv:1706.05721 [cs]},
	author = {Salehi, Seyed Sadegh Mohseni and Erdogmus, Deniz and Gholipour, Ali},
	month = jun,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@book{schroeder_visualization_2006,
	address = {Clifton Park, NY},
	edition = {4th edition},
	title = {Visualization {Toolkit}: {An} {Object}-{Oriented} {Approach} to {3D} {Graphics}, 4th {Edition}},
	isbn = {978-1-930934-19-1},
	shorttitle = {Visualization {Toolkit}},
	abstract = {The theory and practice of visualization using the VTK Visualization Toolkit software. This textbook describes techniques for scalar, vector, and tensor field visualization, as well as texture-map based and modeling techniques. The book includes such important algorithms as color mapping, marching cubes, vector warping and coloring, polygon decimation and smoothing, streamline generation, modeling with implicit surfaces, boolean textures, hyperstreamlines, Delaunay triangulation, volume rendering and many more. Extensive descriptions of data structures and API's, and a succinct description of computer graphics for visualization are also covered. Each chapter contains complete references and exercises (the book is used in many college-level visualization and graphics courses), and algorithms are demonstrated using working VTK code (updated for VTK version 5 and published by Kitware).},
	language = {English},
	publisher = {Kitware},
	author = {Schroeder, Will and Martin, Ken and Lorensen, Bill},
	month = dec,
	year = {2006},
}

@inproceedings{scorza_safe_2017,
	title = {Safe electrode trajectory planning in {SEEG} via {MIP}-based vessel segmentation},
	volume = {10135},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10135/101352C/Safe-electrode-trajectory-planning-in-SEEG-via-MIP-based-vessel/10.1117/12.2254474.short},
	doi = {10.1117/12.2254474},
	abstract = {Stereo-ElectroEncephaloGraphy (SEEG) is a surgical procedure that allows brain exploration of patients affected by focal epilepsy by placing intra-cerebral multi-lead electrodes. The electrode trajectory planning is challenging and time consuming. Various constraints have to be taken into account simultaneously, such as absence of vessels at the electrode Entry Point (EP), where bleeding is more likely to occur. In this paper, we propose a novel framework to help clinicians in defining a safe trajectory and focus our attention on EP. For each electrode, a Maximum Intensity Projection (MIP) image was obtained from Computer Tomography Angiography (CTA) slices of the brain first centimeter measured along the electrode trajectory. A Gaussian Mixture Model (GMM), modified to include neighborhood prior through Markov Random Fields (GMM-MRF), is used to robustly segment vessels and deal with the noisy nature of MIP images. Results are compared with simple GMM and manual global Thresholding (Th) by computing sensitivity, specificity, accuracy and Dice similarity index against manual segmentation performed under the supervision of an expert surgeon. In this work we present a novel framework which can be easily integrated into manual and automatic planner to help surgeon during the planning phase. GMM-MRF qualitatively showed better performance over GMM in reproducing the connected nature of brain vessels also in presence of noise and image intensity drops typical of MIP images. With respect Th, it is a completely automatic method and it is not influenced by inter-subject variability.},
	urldate = {2018-08-16},
	booktitle = {Medical {Imaging} 2017: {Image}-{Guided} {Procedures}, {Robotic} {Interventions}, and {Modeling}},
	publisher = {International Society for Optics and Photonics},
	author = {Scorza, Davide and Moccia, Sara and Luca, Giuseppe De and Plaino, Lisa and Cardinale, Francesco and Mattos, Leonardo S. and Kabongo, Luis and Momi, Elena De},
	month = mar,
	year = {2017},
	pages = {101352C},
}

@article{momi_automatic_2013,
	title = {Automatic {Trajectory} {Planner} for {StereoElectroEncephaloGraphy} {Procedures}: {A} {Retrospective} {Study}},
	volume = {60},
	issn = {0018-9294},
	shorttitle = {Automatic {Trajectory} {Planner} for {StereoElectroEncephaloGraphy} {Procedures}},
	doi = {10.1109/TBME.2012.2231681},
	abstract = {In StereoElectroEncephaloGraphy (SEEG) procedures, intracerebral electrodes are implanted in order to identify the epileptogenic zone in drug-resistant epileptic patients. This paper presents an automatic multitrajectory planner that computes the best trajectory in terms of distance from vessels and guiding screws angle, once the candidate entry and target regions are quickly and roughly defined. The planning process is designed also to spare some brain structures, such as cella media and trigone of the lateral ventricles and brain stem. The planner was retrospectively evaluated on 15 patients who had previously undergone SEEG investigation. Quantitative comparison was performed computing for each patient and for each electrode trajectory 1) the Euclidean distance from the closest vessel; 2) the trajectory incidence angle (guiding screws angle); and 3) the sulcality value. The automatic planner proved to satisfy the clinical requirements, planning safe trajectories in a clinical-compatible timeframe. Qualitative evaluation performed by three neurosurgeons showed that the automatically computed trajectories would have been accepted by them.},
	number = {4},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Momi, E. De and Caborni, C. and Cardinale, F. and Castana, L. and Casaceli, G. and Cossu, M. and Antiga, L. and Ferrigno, G.},
	month = apr,
	year = {2013},
	keywords = {Algorithms, Brain, Humans, Adolescent, Adult, Epilepsy, Electroencephalography, Retrospective Studies, electroencephalography, medical disorders, biomedical electrodes, Neurosurgical Procedures, Computer-Assisted, automatic trajectory planner, blood vessels, Computer-assisted robotic surgery, constrained multiparametric optimization, drug resistant epileptic patients, Electrodes, epilepsy surgery, epileptogenic zone, Euclidean distance, guiding screws angle, Image Processing, intracerebral electrodes, Manuals, Neurosurgery, neurosurgical probes trajectory planning, Planning, Robotics, SEEG procedures, StereoElectroEncephaloGraphy, StereoElectroEncephaloGrapy (SEEG), stereotaxis, Surgery, Trajectory, vessels},
	pages = {986--993},
}

@inproceedings{marrocco_mammogram_2018,
	title = {Mammogram denoising to improve the calcification detection performance of convolutional nets},
	volume = {10718},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10718/107180W/Mammogram-denoising-to-improve-the-calcification-detection-performance-of-convolutional/10.1117/12.2318069.short},
	doi = {10.1117/12.2318069},
	abstract = {Recently, Convolutional Neural Networks (CNNs) have been successfully used to detect microcalcifications in mammograms. An important step in CNN-based detection is image preprocessing that, in raw mammograms, is usually employed to equalize or remove the intensity-dependent quantum noise. In this work, we show how removing the noise can significantly improve the microcalcification detection performance of a CNN. To this end, we describe the quantum noise with a uniform square-root model. Under this assumption, the generalized Anscombe transformation is applied to the raw mammograms by estimating the noise characteristics from the image at hand. In the Anscombe domain, noise is filtered through an adaptive Wiener filter. The denoised images are recovered with an appropriate inverse transformation and are then used to train the CNN-based detector. Experiments were performed on 1,066 mammograms acquired with GE Senographe systems. MC detection performance of a CNN on noise-free mammograms was statistically significantly higher than on unprocessed mammograms. Results were also superior in comparison with a nonparametric noise-equalizing transformation previously proposed for digital mammograms.},
	urldate = {2018-08-15},
	booktitle = {14th {International} {Workshop} on {Breast} {Imaging} ({IWBI} 2018)},
	publisher = {International Society for Optics and Photonics},
	author = {Marrocco, Claudio and Bria, Alessandro and Sano, Valerio Di and Borges, Lucas R. and Savelli, Benedetta and Molinara, Mario and Mordang, Jan-Jurre and Karssemeijer, Nico and Tortorella, Francesco},
	month = jul,
	year = {2018},
	pages = {107180W},
}

@article{chawla_smote_2002,
	title = {{SMOTE}: {Synthetic} {Minority} {Over}-sampling {Technique}},
	volume = {16},
	issn = {1076-9757},
	shorttitle = {{SMOTE}},
	url = {https://jair.org/index.php/jair/article/view/10302},
	doi = {10.1613/jair.953},
	language = {en-US},
	urldate = {2018-08-16},
	journal = {Journal of Artificial Intelligence Research},
	author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
	month = jun,
	year = {2002},
	pages = {321--357},
}

@article{taha_metrics_2015,
	title = {Metrics for evaluating {3D} medical image segmentation: analysis, selection, and tool},
	volume = {15},
	issn = {1471-2342},
	shorttitle = {Metrics for evaluating {3D} medical image segmentation},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4533825/},
	doi = {10.1186/s12880-015-0068-x},
	abstract = {Background Medical Image segmentation is an important image processing step. Comparing images to evaluate the quality of segmentation is an essential part of measuring progress in this research area. Some of the challenges in evaluating medical segmentation are: metric selection, the use in the literature of multiple definitions for certain metrics, inefficiency of the metric calculation implementations leading to difficulties with large volumes, and lack of support for fuzzy segmentation by existing metrics. Result First we present an overview of 20 evaluation metrics selected based on a comprehensive literature review. For fuzzy segmentation, which shows the level of membership of each voxel to multiple classes, fuzzy definitions of all metrics are provided. We present a discussion about metric properties to provide a guide for selecting evaluation metrics. Finally, we propose an efficient evaluation tool implementing the 20 selected metrics. The tool is optimized to perform efficiently in terms of speed and required memory, also if the image size is extremely large as in the case of whole body MRI or CT volume segmentation. An implementation of this tool is available as an open source project. Conclusion We propose an efficient evaluation tool for 3D medical image segmentation using 20 evaluation metrics and provide guidelines for selecting a subset of these metrics that is suitable for the data and the segmentation task.},
	urldate = {2018-08-15},
	journal = {BMC Medical Imaging},
	author = {Taha, Abdel Aziz and Hanbury, Allan},
	month = aug,
	year = {2015},
	pmid = {26263899},
	pmcid = {PMC4533825},
}

@article{langerak_label_2010,
	title = {Label {Fusion} in {Atlas}-{Based} {Segmentation} {Using} a {Selective} and {Iterative} {Method} for {Performance} {Level} {Estimation} ({SIMPLE})},
	volume = {29},
	issn = {0278-0062},
	doi = {10.1109/TMI.2010.2057442},
	abstract = {In a multi-atlas based segmentation procedure, propagated atlas segmentations must be combined in a label fusion process. Some current methods deal with this problem by using atlas selection to construct an atlas set either prior to or after registration. Other methods estimate the performance of propagated segmentations and use this performance as a weight in the label fusion process. This paper proposes a selective and iterative method for performance level estimation (SIMPLE), which combines both strategies in an iterative procedure. In subsequent iterations the method refines both the estimated performance and the set of selected atlases. For a dataset of 100 MR images of prostate cancer patients, we show that the results of SIMPLE are significantly better than those of several existing methods, including the STAPLE method and variants of weighted majority voting.},
	number = {12},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Langerak, T. R. and Heide, U. A. van der and Kotte, A. N. T. J. and Viergever, M. A. and Vulpen, M. van and Pluim, J. P. W.},
	month = dec,
	year = {2010},
	keywords = {Algorithms, Humans, Magnetic Resonance Imaging, Markov Chains, Reproducibility of Results, Robustness, Male, biomedical MRI, cancer, medical image processing, Observer Variation, image segmentation, Image segmentation, Biomedical imaging, image registration, Computer-Assisted, Image Processing, Atlas selection, atlas-based segmentation, biological organs, classifier combination, image fusion, iterative methods, Iterative methods, label fusion, Medical treatment, MR images, Permission, Process planning, Prostate, prostate cancer, Prostate cancer, Prostatic Neoplasms, selective and iterative method for performance level estimation, SIMPLE, STAPLE, Voting},
	pages = {2000--2008},
}

@article{warfield_simultaneous_2004,
	title = {Simultaneous truth and performance level estimation ({STAPLE}): an algorithm for the validation of image segmentation},
	volume = {23},
	issn = {0278-0062},
	shorttitle = {Simultaneous truth and performance level estimation ({STAPLE})},
	doi = {10.1109/TMI.2004.828354},
	abstract = {Characterizing the performance of image segmentation approaches has been a persistent challenge. Performance analysis is important since segmentation algorithms often have limited accuracy and precision. Interactive drawing of the desired segmentation by human raters has often been the only acceptable approach, and yet suffers from intra-rater and inter-rater variability. Automated algorithms have been sought in order to remove the variability introduced by raters, but such algorithms must be assessed to ensure they are suitable for the task. The performance of raters (human or algorithmic) generating segmentations of medical images has been difficult to quantify because of the difficulty of obtaining or estimating a known true segmentation for clinical data. Although physical and digital phantoms can be constructed for which ground truth is known or readily estimated, such phantoms do not fully reflect clinical images due to the difficulty of constructing phantoms which reproduce the full range of imaging characteristics and normal and pathological anatomical variability observed in clinical data. Comparison to a collection of segmentations by raters is an attractive alternative since it can be carried out directly on the relevant clinical imaging data. However, the most appropriate measure or set of measures with which to compare such segmentations has not been clarified and several measures are used in practice. We present here an expectation-maximization algorithm for simultaneous truth and performance level estimation (STAPLE). The algorithm considers a collection of segmentations and computes a probabilistic estimate of the true segmentation and a measure of the performance level represented by each segmentation. The source of each segmentation in the collection may be an appropriately trained human rater or raters, or may be an automated segmentation algorithm. The probabilistic estimate of the true segmentation is formed by estimating an optimal combination of the segmentations, weighting each segmentation depending upon the estimated performance level, and incorporating a prior model for the spatial distribution of structures being segmented as well as spatial homogeneity constraints. STAPLE is straightforward to apply to clinical imaging data, it readily enables assessment of the performance of an automated image segmentation algorithm, and enables direct comparison of human rater and algorithm performance.},
	number = {7},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Warfield, S. K. and Zou, K. H. and Wells, W. M.},
	month = jul,
	year = {2004},
	keywords = {Algorithms, Brain, Humans, Magnetic Resonance Imaging, Markov Chains, Reproducibility of Results, Image Enhancement, Sensitivity and Specificity, medical image processing, Observer Variation, image segmentation, Image segmentation, Biomedical imaging, Computer-Assisted, Image Interpretation, Artificial intelligence, Computer science, Decision Making, digital phantoms, expectation-maximization algorithm, ground truth, Hospitals, Imaging, Imaging phantoms, Infant, inter-rater variability, intra-rater variability, maximum likelihood estimation, medical images, Models, Newborn, Pathology, Performance analysis, performance level estimation, phantoms, Phantoms, physical phantoms, Radiology, spatial homogeneity constraints, Statistical},
	pages = {903--921},
}

@article{cabezas_review_2011,
	title = {A review of atlas-based segmentation for magnetic resonance brain images},
	volume = {104},
	issn = {0169-2607},
	url = {http://www.sciencedirect.com/science/article/pii/S0169260711002033},
	doi = {10.1016/j.cmpb.2011.07.015},
	abstract = {Normal and abnormal brains can be segmented by registering the target image with an atlas. Here, an atlas is defined as the combination of an intensity image (template) and its segmented image (the atlas labels). After registering the atlas template and the target image, the atlas labels are propagated to the target image. We define this process as atlas-based segmentation. In recent years, researchers have investigated registration algorithms to match atlases to query subjects and also strategies for atlas construction. In this paper we present a review of the automated approaches for atlas-based segmentation of magnetic resonance brain images. We aim to point out the strengths and weaknesses of atlas-based methods and suggest new research directions. We use two different criteria to present the methods. First, we refer to the algorithms according to their atlas-based strategy: label propagation, multi-atlas methods, and probabilistic techniques. Subsequently, we classify the methods according to their medical target: the brain and its internal structures, tissue segmentation in healthy subjects, tissue segmentation in fetus, neonates and elderly subjects, and segmentation of damaged brains. A quantitative comparison of the results reported in the literature is also presented.},
	number = {3},
	urldate = {2018-08-13},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Cabezas, Mariano and Oliver, Arnau and Llad{\'o}, Xavier and Freixenet, Jordi and Bach Cuadra, Meritxell},
	month = dec,
	year = {2011},
	keywords = {Brain, Segmentation, Magnetic resonance imaging, Atlas, Automated methods},
	pages = {e158--e177},
}

@article{duncan_brain_2016,
	title = {Brain imaging in the assessment for epilepsy surgery},
	volume = {15},
	issn = {1474-4422},
	url = {http://www.sciencedirect.com/science/article/pii/S147444221500383X},
	doi = {10.1016/S1474-4422(15)00383-X},
	abstract = {Summary
Brain imaging has a crucial role in the presurgical assessment of patients with epilepsy. Structural imaging reveals most cerebral lesions underlying focal epilepsy. Advances in MRI acquisitions including diffusion-weighted imaging, post-acquisition image processing techniques, and quantification of imaging data are increasing the accuracy of lesion detection. Functional MRI can be used to identify areas of the cortex that are essential for language, motor function, and memory, and tractography can reveal white matter tracts that are vital for these functions, thus reducing the risk of epilepsy surgery causing new morbidities. PET, SPECT, simultaneous EEG and functional MRI, and electrical and magnetic source imaging can be used to infer the localisation of epileptic foci and assist in the design of intracranial EEG recording strategies. Progress in semi-automated methods to register imaging data into a common space is enabling the creation of multimodal three-dimensional patient-specific datasets. These techniques show promise for the demonstration of the complex relations between normal and abnormal structural and functional data and could be used to direct precise intracranial navigation and surgery for individual patients.},
	number = {4},
	urldate = {2017-10-20},
	journal = {The Lancet Neurology},
	author = {Duncan, John S and Winston, Gavin P and Koepp, Matthias J and Ourselin, Sebastien},
	month = apr,
	year = {2016},
	pages = {420--433},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/45Y99XLA/S147444221500383X.html:text/html},
}

@article{wang_temporal_2019,
	title = {Temporal {Segment} {Networks} for {Action} {Recognition} in {Videos}},
	volume = {41},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2018.2868668},
	abstract = {We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structure with a new segment-based sampling and aggregation scheme. This unique design enables the TSN framework to efficiently learn action models by using the whole video. The learned models could be easily deployed for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the implementation of the TSN framework given limited training samples. Our approach obtains the state-the-of-art performance on five challenging action recognition benchmarks: HMDB51 (71.0 percent), UCF101 (94.9 percent), THUMOS14 (80.1 percent), ActivityNet v1.2 (89.6 percent), and Kinetics400 (75.7 percent). In addition, using the proposed RGB difference as a simple motion representation, our method can still achieve competitive accuracy on UCF101 (91.0 percent) while running at 340 FPS. Furthermore, based on the proposed TSN framework, we won the video classification track at the ActivityNet challenge 2016 among 24 teams.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wang, L. and Xiong, Y. and Wang, Z. and Qiao, Y. and Lin, D. and Tang, X. and Gool, L. Van},
	month = nov,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {image motion analysis, Training, Visualization, image classification, video signal processing, feature extraction, learning (artificial intelligence), Adaptation models, image segmentation, Histograms, action models, Action recognition, action recognition benchmarks, ActivityNet, aggregation scheme, Analytical models, ConvNets, gesture recognition, good practices, image representation, long-range temporal structure, Microsoft Windows, segment-based sampling, temporal modeling, temporal segment networks, trimmed videos, TSN framework, untrimmed videos, video classification track, video-level framework, Videos},
	pages = {2740--2755},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/S2ITE7PQ/8454294.html:text/html},
}

@article{jenssen_how_2006,
	title = {How {Long} {Do} {Most} {Seizures} {Last}? {A} {Systematic} {Comparison} of {Seizures} {Recorded} in the {Epilepsy} {Monitoring} {Unit}},
	volume = {47},
	issn = {1528-1167},
	shorttitle = {How {Long} {Do} {Most} {Seizures} {Last}?},
	doi = {https://doi.org/10.1111/j.1528-1167.2006.00622.x},
	abstract = {Summary: Purpose: More information is needed regarding how long seizures typically last, since this influences treatment decisions. Seizure type and other factors could influence seizure duration. Methods: Data were collected from a random sample of patients being evaluated with continuous video and scalp EEG. Seizure duration was defined as time from early sign of seizure (clinical or EEG) until the end of seizure on EEG. Seizures were categorized as simple partial (SPS), complex partial (CPS), secondarily generalized tonic{\textendash}clonic (SGTCS), primary generalized tonic{\textendash}clonic (PGTCS) and tonic (TS). SGTCS were divided into a complex partial part (SGTCS/CP) and a tonic{\textendash}clonic part (SGTCS/TC). Median and longest duration of each seizure type in each individual were used. Comparisons of seizure types, first and last seizure, area of onset, and state of onset were performed. Results: Five hundred seventy-nine seizures were recorded in 159 adult patients. Seizures with partial onset spreading to both hemispheres had the longest duration. SGTCS were unlikely to last more than 660 s, CPS more than 600 s, and SPS more than 240 s. PGTCS and TS had shorter durations, but the number of subjects with those two types was small. CPS did not differ in duration according to sleep state at onset nor side of origin. Conclusion: A working definition of status epilepticus in adults with cryptogenic or symptomatic epilepsy can be drawn from these data for purposes of future epidemiologic research. More information is needed for the idiopathic epilepsies and in children.},
	language = {en},
	number = {9},
	urldate = {2020-12-08},
	journal = {Epilepsia},
	author = {Jenssen, Sigmund and Gracely, Edward J. and Sperling, Michael R.},
	year = {2006},
	keywords = {Seizure classification, Seizure duration, Seizure termination, Status epilepticus},
	pages = {1499--1503},
	file = {Snapshot:/home/fernando/Zotero/storage/6QAMUIU7/j.1528-1167.2006.00622.html:text/html},
}

@inproceedings{szegedy_going_2015,
	title = {Going {Deeper} with {Convolutions}},
	url = {http://arxiv.org/abs/1409.4842},
	urldate = {2020-12-08},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	year = {2015},
}

@article{ahmedt-aristizabal_aberrant_2019,
	title = {Aberrant epileptic seizure identification: {A} computer vision perspective},
	volume = {65},
	issn = {1059-1311, 1532-2688},
	shorttitle = {Aberrant epileptic seizure identification},
	doi = {10.1016/j.seizure.2018.12.017},
	language = {English},
	urldate = {2020-12-08},
	journal = {Seizure - European Journal of Epilepsy},
	author = {Ahmedt-Aristizabal, David and Fookes, Clinton and Denman, Simon and Nguyen, Kien and Sridharan, Sridha and Dionisio, Sasha},
	month = feb,
	year = {2019},
	pmid = {30616221},
	note = {Publisher: Elsevier},
	pages = {65--71},
	file = {Snapshot:/home/fernando/Zotero/storage/7U3INKYY/abstract.html:text/html},
}

@article{achilles_convolutional_2018,
	title = {Convolutional neural networks for real-time epileptic seizure detection},
	volume = {6},
	issn = {2168-1163},
	doi = {10.1080/21681163.2016.1141062},
	abstract = {Epileptic seizures constitute a serious neurological condition for patients and, if untreated, considerably decrease their quality of life. Early and correct diagnosis by semiological seizure analysis provides the main approach to treat and improve the patients{\textquoteright} condition. To obtain reliable and quantifiable information, medical professionals perform seizure detection and subsequent analysis using expensive video-EEG systems in specialized epilepsy monitoring units. However, the detection of seizures, especially under difficult circumstances such as occlusion by the blanket or in the absence of predictive EEG patterns, is highly subjective and should therefore be supported by automated systems. In this work, we conjecture that features learned via a convolutional neural network provide the ability to distinctively detect seizures from video, and even allow our system to generalize to different seizure types. By comparing our method to the state of the art we show the superior performance of learned features for epileptic seizure detection.},
	number = {3},
	urldate = {2020-12-03},
	journal = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging \& Visualization},
	author = {Achilles, Felix and Tombari, Federico and Belagiannis, Vasileios and Loesch, Anna Mira and Noachtar, Soheyl and Navab, Nassir},
	month = may,
	year = {2018},
	keywords = {image processing and analysis, convolutional neural networks, deep learning, Computer aided diagnosis, epileptic seizure detection, therapy and treatment},
	pages = {264--269},
	file = {Snapshot:/home/fernando/Zotero/storage/PR9MT5MG/21681163.2016.html:text/html},
}

@inproceedings{karacsony_deep_2020,
	title = {A {Deep} {Learning} {Architecture} for {Epileptic} {Seizure} {Classification} {Based} on {Object} and {Action} {Recognition}},
	doi = {10.1109/ICASSP40776.2020.9054649},
	abstract = {Epilepsy affects approximately 1\% of the world's population. Semi-ology of epileptic seizures contain major clinical signs to classify epilepsy syndromes currently evaluated by epileptologists by simple visual inspection of video. There is a necessity to create automatic and semiautomatic methods for seizure detection and classification to better support patient monitoring management and diagnostic decisions. One of the current promising approaches are the marker-less computer-vision techniques. In this paper an end-to-end deep learning approach is proposed for binary classification of Frontal vs. Temporal Lobe Epilepsies based solely on seizure videos. The system utilizes infrared (IR) videos of the seizures as it is used 24/7 in hospitals' epilepsy monitoring units. The architecture employs transfer learning from large object detection "static" and human action recognition "dynamic" datasets such as ImageNet and Kinectics-400, to extract and classify the clinically known spatiotemporal features of seizures. The developed classification architecture achieves a 5-fold cross-validation f1-score of 0.844{\textpm}0.042. This architecture has the potential to support physicians with diagnostic decisions and might be applied for online applications in epilepsy monitoring units. Furthermore, it may be jointly used in the near future with synchronized scene depth 3D information and EEG from the seizures.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Kar{\'a}csony, T. and Loesch-Biffar, A. M. and Vollmar, C. and Noachtar, S. and Cunha, J. P. S.},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {Deep learning, image classification, feature extraction, medical image processing, electroencephalography, epileptic seizures, medical disorders, neurophysiology, diseases, epileptic seizure classification, learning (artificial intelligence), Action recognition, automatic methods, binary classification, clinical signs, clinically known spatiotemporal features, Computer Vision, cross-validation f1-score, current promising approaches, developed classification architecture, diagnostic decisions, Diagnostic support, end-to-end deep learning approach, epilepsy monitoring units, epilepsy syndromes, Epileptic seizure semiology, hospitals, human action recognition dynamic datasets, marker-less computer-vision techniques, medical signal detection, medical signal processing, object detection, seizure detection, seizure videos, semiautomatic methods, semiology, support patient monitoring management, Temporal Lobe Epilepsies, transfer learning, world},
	pages = {4117--4121},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/Q8UUQ7VM/9054649.html:text/html},
}

@inproceedings{maia_epileptic_2019,
	title = {Epileptic seizure classification using the {NeuroMov} database},
	doi = {10.1109/ENBENG.2019.8692465},
	abstract = {Epilepsy is one of the most common neurological disorders, affecting up to 1\% of the world population. In cases where epileptics are not responsive to medication, resective surgery has been found to be an effective treatment for seizure freedom or increase in control. For this, epilepsy must be correctly diagnosed. That evaluation is typically performed by VideoEEG monitoring by physicians, a subjective task that can easily be aided by movement quantification and pattern recognition techniques. Regarding their onset location, seizures can be classified as having extratemporal or temporal origin. This contribution contains an analysis of infrared data from 143 seizures from 31 different patients, recorded in the Epilepsy Monitoring Unit of University of Munich, for seizure classification. Regarding classification, using the seizures' duration and the existence of movements of interest, an 86\% {\textpm} 17\% AUC is obtained using 10-fold cross validation and a Support Vector Machine model. Using the video recordings, the region of interest (bed) was first detected with 88\% correct detections and 22\% overdetections using a threshold-based method, and all beds were rotated to a vertical position for consistency. Lastly, seizures were classified with a Convolutional Neural Network and a Multilayer Perceptron, obtaining a 65\% AUC and showing that the model is better at classifying extratemporal seizures, mostly due to class imbalance. The developed approach shows potential for clinical decision support using a non-intrusive and low cost solution.},
	booktitle = {2019 {IEEE} 6th {Portuguese} {Meeting} on {Bioengineering} ({ENBENG})},
	author = {Maia, P. and Hartl, E. and Vollmar, C. and Noachtar, S. and Cunha, J. P. Silva},
	month = feb,
	year = {2019},
	keywords = {Epilepsy, Cameras, Feature extraction, electroencephalography, medical disorders, Monitoring, neurophysiology, epileptic seizure classification, support vector machines, convolutional neural network, medical signal processing, Classification algorithms, convolutional neural nets, epileptics, infrared data, movement quantification, multilayer perceptron, multilayer perceptrons, neurological disorders, NeuroMov database, pattern classification, pattern recognition techniques, resective surgery, seizure classification, seizure duration, Sensitivity, signal classification, Support Vector Machine model, Support vector machines, video recordings, VideoEEG monitoring},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/G9L3S39Y/8692465.html:text/html},
}

@article{tufenkjian_seizure_2012,
	title = {Seizure {Semiology}: {Its} {Value} and {Limitations} in {Localizing} the {Epileptogenic} {Zone}},
	volume = {8},
	issn = {1738-6586},
	shorttitle = {Seizure {Semiology}},
	doi = {10.3988/jcn.2012.8.4.243},
	abstract = {Epilepsy surgery has become an important treatment option in patients with medically refractory epilepsy. The ability to precisely localize the epileptogenic zone is crucial for surgical success. The tools available for localization of the epileptogenic zone are limited. Seizure semiology is a simple and cost effective tool that allows localization of the symptomatogenic zone which either overlaps or is in close proximity of the epileptogenic zone. This becomes particularly important in cases of MRI negative focal epilepsy. The ability to video record seizures made it possible to discover new localizing signs and quantify the sensitivity and specificity of others. Ideally the signs used for localization should fulfill these criteria; 1) Easy to identify and have a high inter-rater reliability, 2) It has to be the first or one of the earlier components of the seizure in order to have localizing value. Later symptoms or signs are more likely to be due to ictal spread and may have only a lateralizing value. 3) The symptomatogenic zone corresponding to the recorded ictal symptom has to be clearly defined and well documented. Reproducibility of the initial ictal symptoms with cortical stimulation identifies the corresponding symptomatogenic zone. Unfortunately, however, not all ictal symptoms can be reproduced by focal cortical stimulation. Therefore, the problem the clinician faces is trying to deduce the epileptogenic zone from the seizure semiology. The semiological classification system is particularly useful in this regard. We present the known localizing and lateralizing signs based on this system.},
	number = {4},
	urldate = {2020-12-03},
	journal = {Journal of Clinical Neurology (Seoul, Korea)},
	author = {Tufenkjian, Krikor and L{\"u}ders, Hans O.},
	month = dec,
	year = {2012},
	pmid = {23323131},
	pmcid = {PMC3540282},
	pages = {243--250},
}

@article{bleasel_lateralizing_1997,
	title = {Lateralizing {Value} and {Semiology} of {Ictal} {Limb} {Posturing} and {Version} in {Temporal} {Lobe} and {Extratemporal} {Epilepsy}},
	volume = {38},
	issn = {1528-1167},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1528-1157.1997.tb01093.x},
	doi = {https://doi.org/10.1111/j.1528-1157.1997.tb01093.x},
	abstract = {Summary: Purpose: Unilateral dystonic limb posturing in partial seizures has been shown to be an accurate lateralizing sign indicating seizure onset in the contralateral hemisphere. However, its clinical utility may be reduced by confusion with other lateralized ictal motor phenomena. In this study, the ictal phenomena of dystonic limb posturing, tonic limb posturing, unilateral immobile limb, and version were distinguished and examined in patients with temporal and extratemporal seizures. Methods: Partial seizures in 54 patients, successfully treated by surgery (34 temporal, 20 extratemporal; 14 frontal, 3 parietal, and 3 occipital), were analyzed blindly by 3 reviewers. Interobserver agreement was tested with kappa indexes and positive predictive value (PPV) was determined for each sign. Results: In patients with temporal lobe epilepsy (TLE), dystonic posturing occurred in 35.3\% (kappa 0.78, positive predictive value (PPV) for the sign being contralateral to seizure onset 92\%); tonic limb posturing occurred in 17.7\% (kappa 0.36, PPV 40\%); unilateral immobile limb occurred in 11.8\% (kappa 0.23, PPV 100\%); and version occurred in 35.3\% (kappa 0.77, PPV 100\%). In patients with extratemporal epilepsy, dystonic posturing occurred in 20.0\% (kappa 0.31, PPV 100\%); tonic limb posturing occurred in 15.0\% (kappa 0.08, PPV 67\%); and version occurred in 40.0\% (kappa 0.54, PPV 100\%). The higher kappa indexes were significant for dystonic posturing (p {\textless} 0.001) and tonic limb posturing (p = 0.032) in TLE. Dystonic posturing (p = 0.034), tonic posturing (p = 0.07), and version (p = 0.0038) occurred earlier in extratemporal seizures than in temporal seizures. Conclusions: Of the limb ictal motor phenomena, only dystonic posturing was accurate and had good interobserver agreement.},
	language = {en},
	number = {2},
	urldate = {2020-12-03},
	journal = {Epilepsia},
	author = {Bleasel, Andrew and Kotagal, Prakash and Kankirawatana, Pongkiat and Rybicki, Lisa},
	year = {1997},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1528-1157.1997.tb01093.x},
	keywords = {Dystonic posturing, Lateralizing signs, Partial seizures},
	pages = {168--174},
	file = {Snapshot:/home/fernando/Zotero/storage/NIC32D9B/j.1528-1157.1997.tb01093.html:text/html},
}

@article{noachtar_semiology_2009,
	title = {Semiology of epileptic seizures: a critical review},
	volume = {15},
	issn = {1525-5069},
	shorttitle = {Semiology of epileptic seizures},
	doi = {10.1016/j.yebeh.2009.02.029},
	abstract = {Epileptic seizures are characterized by a variety of symptoms. Their typical semiology served for a long time as the major tool to classify epilepsy syndromes. The signs and symptoms of epileptic seizures include the following spheres: sensorial sphere, consciousness, motor and autonomic spheres. Most seizures involve more than one sphere, however, some like for instance aura (sensorial sphere) or dialeptic seizures (consciousness) involve only one sphere. The predominant clinical features of a seizure determines the seizure classification. The following review gives an introduction into the semiological seizure classification. This approach enables us to better identify the epileptogenic zone of our patients and to choose the most effective medical or surgical treatment.},
	language = {eng},
	number = {1},
	journal = {Epilepsy \& Behavior: E\&B},
	author = {Noachtar, Soheyl and Peters, Astrid S.},
	month = may,
	year = {2009},
	pmid = {19236941},
	keywords = {Humans, Epilepsy},
	pages = {2--9},
}

@inproceedings{cunha_movement_2003,
	title = {Movement quantification during epileptic seizures: a new technical contribution to the evaluation of seizure semiology},
	volume = {1},
	shorttitle = {Movement quantification during epileptic seizures},
	doi = {10.1109/IEMBS.2003.1279851},
	abstract = {In epilepsy, seizure semiology analysis is routinely used for diagnostic purpose. The behavior of the patient during seizures is usually evaluated by expert qualitative observation where several signs are identified. In the clinical literature, several ictal phenomena are described but still involved in controversy. In this paper, we present our effort to establish a quantified movement analysis method to be widely used as an additional tool to clarify this controversy.},
	booktitle = {Proceedings of the 25th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({IEEE} {Cat}. {No}.{03CH37439})},
	author = {Cunha, J. P. S. and Vollmar, C. and Li, Z. and Fernandes, J. and Feddersen, B. and Noachtar, S.},
	month = sep,
	year = {2003},
	note = {ISSN: 1094-687X},
	keywords = {Epilepsy, Cameras, image motion analysis, biomechanics, electroencephalography, epileptic seizures, patient monitoring, biomedical optical imaging, CCD image sensors, Charge coupled devices, Charge-coupled image sensors, ictal phenomena, Inspection, Nervous system, Patient monitoring, quantified movement analysis, seizure semiology, Solid modeling, Temporal lobe, Video equipment},
	pages = {671--673 Vol.1},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/6YPPB5T4/1279851.html:text/html},
}

@article{odwyer_lateralizing_2007,
	title = {Lateralizing significance of quantitative analysis of head movements before secondary generalization of seizures of patients with temporal lobe epilepsy},
	volume = {48},
	issn = {0013-9580},
	doi = {10.1111/j.1528-1167.2006.00967.x},
	abstract = {PURPOSE: To quantitatively evaluate the lateralizing significance of ictal head movements of patients with temporal lobe epilepsy (TLE).
METHODS: We investigated EEG-video recorded seizures of patients with TLE, in which the camera position was perpendicular to the head facing the camera in an upright position and bilateral head movement was recorded. Thirty-eight seizures (31 patients) with head movement in both directions were investigated. Ipsilateral and contralateral head movements were defined according to ictal EEG. Head movements were quantified by selecting the movement of the nose in relation to a defined point on the thorax (25/s) in a defined plane facing the camera. The duration of the head version was determined independently of the camera angle. The angle, duration, and angular speed of the head movements were computed and inter and intrasubject analyses were performed (Wilcoxon rank sum).
RESULTS: Ipsilateral movement always preceded contralateral movement. The positive predictive value was 100\% for movement in both directions. The duration of contralateral head version was significantly longer than ipsilateral head movement (6.4 +/- 4.1 s vs. 3.9 +/- 3.1 s, p{\textless}0.001). The angular speed of both movements was similar (15.5 +/- 12.1 deg/s vs. 17.3 +/- 13.0 deg/s).
CONCLUSION: The quantitative analysis shows the importance of sequence in the seizure's evolution and duration, but not angular speed for correct lateralization of versive head movement. This quantitative method shows the high lateralizing value of ictal lateral head movements in TLE.},
	language = {eng},
	number = {3},
	journal = {Epilepsia},
	author = {O'Dwyer, Rebecca and Silva Cunha, Joao P. and Vollmar, Christian and Mauerer, Cordula and Feddersen, Berend and Burgess, Richard C. and Ebner, Alois and Noachtar, Soheyl},
	month = mar,
	year = {2007},
	pmid = {17326791},
	keywords = {Humans, Image Processing, Computer-Assisted, Adolescent, Adult, Female, Male, Middle Aged, Electroencephalography, Seizures, Epilepsy, Temporal Lobe, Predictive Value of Tests, Child, Functional Laterality, Head, Videotape Recording},
	pages = {524--530},
}

@article{li_z_movement_2002,
	title = {Movement quantification in epileptic seizures: a new approach to video-{EEG} analysis},
	volume = {49},
	issn = {1558-2531},
	shorttitle = {Movement quantification in epileptic seizures},
	doi = {10.1109/TBME.2002.1001971},
	abstract = {It is common that epileptic seizures induce uncoordinated movement in a patient's body. This movement is a relevant clinical factor in seizure identification. Nevertheless, quantification of this information has not been an object of much attention from the scientific community. In this paper, we present our effort in developing a new approach to the quantification of movement patterns in patients during epileptic seizures. We attach markers at landmark points of a patient's body and use a camera and a commercial video-electroencephalogram (EEG) system to synchronously register EEG and video during seizures. Then, we apply image-processing techniques to analyze the video frames and extract the trajectories of those points that represent the course of the quantified movement of different body parts. This information may help clinicians in seizure classification. We describe the framework of our system and a method of analyzing video in order to achieve the proposed goal. Our experimental results show that our method can reflect quantified motion patterns of epileptic seizures, which cannot be accessed by means of traditional visual inspection of video recordings. We were able, for the first time, to quantify the movement of different parts of a convulsive human body in the course of an epileptic seizure. This result represents an enhanced value to clinicians in studying seizures for reaching a diagnosis.},
	number = {6},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {{Li, Z.} and Silva, A. M. da and Cunha, J. P. S.},
	month = jun,
	year = {2002},
	note = {Conference Name: IEEE Transactions on Biomedical Engineering},
	keywords = {Algorithms, Humans, Epilepsy, Electroencephalography, Epilepsy, Generalized, Cameras, image motion analysis, medical image processing, Movement, Signal Processing, Computer-Assisted, biomechanics, electroencephalography, epileptic seizures, Data mining, diseases, Hospitals, medical signal processing, seizure classification, video recordings, Inspection, body parts, clinicians, convulsive human body, electrodiagnostics, Image analysis, landmark points, Models, Biological, Neurophysiology, quantified motion patterns, television applications, trajectories extraction, uncoordinated movement induction, video frames analysis, Video recording, video-EEG analysis, Videodisc Recording},
	pages = {565--573},
}

@article{fisher_epileptic_2005,
	title = {Epileptic seizures and epilepsy: definitions proposed by the {International} {League} {Against} {Epilepsy} ({ILAE}) and the {International} {Bureau} for {Epilepsy} ({IBE})},
	volume = {46},
	issn = {0013-9580},
	shorttitle = {Epileptic seizures and epilepsy},
	doi = {10.1111/j.0013-9580.2005.66104.x},
	abstract = {The International League Against Epilepsy (ILAE) and the International Bureau for Epilepsy (IBE) have come to consensus definitions for the terms epileptic seizure and epilepsy. An epileptic seizure is a transient occurrence of signs and/or symptoms due to abnormal excessive or synchronous neuronal activity in the brain. Epilepsy is a disorder of the brain characterized by an enduring predisposition to generate epileptic seizures and by the neurobiologic, cognitive, psychological, and social consequences of this condition. The definition of epilepsy requires the occurrence of at least one epileptic seizure.},
	language = {eng},
	number = {4},
	journal = {Epilepsia},
	author = {Fisher, Robert S. and van Emde Boas, Walter and Blume, Warren and Elger, Christian and Genton, Pierre and Lee, Phillip and Engel, Jerome},
	month = apr,
	year = {2005},
	pmid = {15816939},
	keywords = {Brain, Humans, Epilepsy, Consensus, International Agencies, Terminology as Topic},
	pages = {470--472},
}

@article{fisher_operational_2017,
	title = {Operational classification of seizure types by the {International} {League} {Against} {Epilepsy}: {Position} {Paper} of the {ILAE} {Commission} for {Classification} and {Terminology}},
	volume = {58},
	issn = {1528-1167},
	shorttitle = {Operational classification of seizure types by the {International} {League} {Against} {Epilepsy}},
	doi = {10.1111/epi.13670},
	abstract = {The International League Against Epilepsy (ILAE) presents a revised operational classification of seizure types. The purpose of such a revision is to recognize that some seizure types can have either a focal or generalized onset, to allow classification when the onset is unobserved, to include some missing seizure types, and to adopt more transparent names. Because current knowledge is insufficient to form a scientifically based classification, the 2017 Classification is operational (practical) and based on the 1981 Classification, extended in 2010. Changes include the following: (1) "partial" becomes "focal"; (2) awareness is used as a classifier of focal seizures; (3) the terms dyscognitive, simple partial, complex partial, psychic, and secondarily generalized are eliminated; (4) new focal seizure types include automatisms, behavior arrest, hyperkinetic, autonomic, cognitive, and emotional; (5) atonic, clonic, epileptic spasms, myoclonic, and tonic seizures can be of either focal or generalized onset; (6) focal to bilateral tonic-clonic seizure replaces secondarily generalized seizure; (7) new generalized seizure types are absence with eyelid myoclonia, myoclonic absence, myoclonic-atonic, myoclonic-tonic-clonic; and (8) seizures of unknown onset may have features that can still be classified. The new classification does not represent a fundamental change, but allows greater flexibility and transparency in naming seizure types.},
	language = {eng},
	number = {4},
	journal = {Epilepsia},
	author = {Fisher, Robert S. and Cross, J. Helen and French, Jacqueline A. and Higurashi, Norimichi and Hirsch, Edouard and Jansen, Floor E. and Lagae, Lieven and Mosh{\'e}, Solomon L. and Peltola, Jukka and Roulet Perez, Eliane and Scheffer, Ingrid E. and Zuberi, Sameer M.},
	year = {2017},
	pmid = {28276060},
	keywords = {Humans, Epilepsy, Seizures, Classification, Taxonomy, International Agencies, Terminology as Topic, Focal, Generalized, Societies, Medical},
	pages = {522--530},
}

@article{jungo_pymia_2021,
	title = {pymia: {A} {Python} package for data handling and evaluation in deep learning-based medical image analysis},
	volume = {198},
	issn = {0169-2607},
	shorttitle = {pymia},
	url = {http://www.sciencedirect.com/science/article/pii/S0169260720316291},
	doi = {10.1016/j.cmpb.2020.105796},
	abstract = {Background and Objective: Deep learning enables tremendous progress in medical image analysis. One driving force of this progress are open-source frameworks like TensorFlow and PyTorch. However, these frameworks rarely address issues specific to the domain of medical image analysis, such as 3-D data handling and distance metrics for evaluation. pymia, an open-source Python package, tries to address these issues by providing flexible data handling and evaluation independent of the deep learning framework. Methods: The pymia package provides data handling and evaluation functionalities. The data handling allows flexible medical image handling in every commonly used format (e.g., 2-D, 2.5-D, and 3-D; full- or patch-wise). Even data beyond images like demographics or clinical reports can easily be integrated into deep learning pipelines. The evaluation allows stand-alone result calculation and reporting, as well as performance monitoring during training using a vast amount of domain-specific metrics for segmentation, reconstruction, and regression. Results: The pymia package is highly flexible, allows for fast prototyping, and reduces the burden of implementing data handling routines and evaluation methods. While data handling and evaluation are independent of the deep learning framework used, they can easily be integrated into TensorFlow and PyTorch pipelines. The developed package was successfully used in a variety of research projects for segmentation, reconstruction, and regression. Conclusions: The pymia package fills the gap of current deep learning frameworks regarding data handling and evaluation in medical image analysis. It is available at https://github.com/rundherum/pymia and can directly be installed from the Python Package Index using pip install pymia.},
	language = {en},
	urldate = {2020-12-01},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Jungo, Alain and Scheidegger, Olivier and Reyes, Mauricio and Balsiger, Fabian},
	month = jan,
	year = {2021},
	keywords = {Deep learning, Medical image analysis, Data handling, Evaluation, Metrics},
	pages = {105796},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/J9DZZANX/S0169260720316291.html:text/html},
}

@article{modat_fast_2010,
	title = {Fast free-form deformation using graphics processing units},
	volume = {98},
	issn = {1872-7565},
	doi = {10.1016/j.cmpb.2009.09.002},
	abstract = {A large number of algorithms have been developed to perform non-rigid registration and it is a tool commonly used in medical image analysis. The free-form deformation algorithm is a well-established technique, but is extremely time consuming. In this paper we present a parallel-friendly formulation of the algorithm suitable for graphics processing unit execution. Using our approach we perform registration of T1-weighted MR images in less than 1 min and show the same level of accuracy as a classical serial implementation when performing segmentation propagation. This technology could be of significant utility in time-critical applications such as image-guided interventions, or in the processing of large data sets.},
	language = {eng},
	number = {3},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Modat, Marc and Ridgway, Gerard R. and Taylor, Zeike A. and Lehmann, Manja and Barnes, Josephine and Hawkes, David J. and Fox, Nick C. and Ourselin, S{\'e}bastien},
	month = jun,
	year = {2010},
	pmid = {19818524},
	keywords = {Algorithms, Image Processing, Computer-Assisted, Software, Computer Graphics, Diagnostic Imaging},
	pages = {278--284},
}

@article{jia_constrained_2017,
	title = {Constrained {Deep} {Weak} {Supervision} for {Histopathology} {Image} {Segmentation}},
	volume = {36},
	issn = {1558-254X},
	doi = {10.1109/TMI.2017.2724070},
	abstract = {In this paper, we develop a new weakly supervised learning algorithm to learn to segment cancerous regions in histopathology images. This paper is under a multiple instance learning (MIL) framework with a new formulation, deep weak supervision (DWS); we also propose an effective way to introduce constraints to our neural networks to assist the learning process. The contributions of our algorithm are threefold: 1) we build an end-to-end learning system that segments cancerous regions with fully convolutional networks (FCNs) in which image-to-image weakly-supervised learning is performed; 2) we develop a DWS formulation to exploit multi-scale learning under weak supervision within FCNs; and 3) constraints about positive instances are introduced in our approach to effectively explore additional weakly supervised information that is easy to obtain and enjoy a significant boost to the learning process. The proposed algorithm, abbreviated as DWS-MIL, is easy to implement and can be trained efficiently. Our system demonstrates the state-of-the-art results on large-scale histopathology image data sets and can be applied to various applications in medical imaging beyond histopathology images, such as MRI, CT, and ultrasound images.},
	number = {11},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Jia, Z. and Huang, X. and Chang, E. I. and Xu, Y.},
	month = nov,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Algorithms, Humans, Image Processing, Computer-Assisted, Convolutional neural networks, Training, neural nets, cancer, Cancer, medical image processing, Supervised learning, learning (artificial intelligence), fully convolutional networks, image segmentation, Image segmentation, Databases, Factual, neural networks, Biomedical imaging, Neural Networks (Computer), Neural networks, cancerous regions, Colon, Colonic Neoplasms, constrained deep weak supervision, DWS-MIL, end-to-end learning system, Histocytochemistry, histopathology image segmentation, MIL framework, multiple instance learning, multiple instance learning framework, Prediction algorithms, Supervised Machine Learning, Tissue Array Analysis, weakly supervised learning},
	pages = {2376--2388},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/S5Q38PFT/7971941.html:text/html},
}

@inproceedings{feichtenhofer_x3d_2020,
	title = {{X3D}: {Expanding} {Architectures} for {Efficient} {Video} {Recognition}},
	shorttitle = {{X3D}},
	urldate = {2020-11-27},
	author = {Feichtenhofer, Christoph},
	year = {2020},
	pages = {203--213},
	file = {Snapshot:/home/fernando/Zotero/storage/78SJMHDA/Feichtenhofer_X3D_Expanding_Architectures_for_Efficient_Video_Recognition_CVPR_2020_paper.html:text/html},
}

@inproceedings{feichtenhofer_slowfast_2019,
	title = {{SlowFast} {Networks} for {Video} {Recognition}},
	doi = {10.1109/ICCV.2019.00630},
	abstract = {We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github.com/facebookresearch/SlowFast.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Feichtenhofer, C. and Fan, H. and Malik, J. and He, K.},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {image motion analysis, image classification, Spatial resolution, video signal processing, learning (artificial intelligence), Semantics, Biomedical optical imaging, channel capacity, Channel capacity, fast pathway, high frame rate, image capture, Image color analysis, low frame rate, Optical imaging, slow pathway, slowfast networks, Spatiotemporal phenomena, temporal information, temporal resolution, video recognition benchmarks},
	pages = {6201--6210},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/6HTKJEE6/9008780.html:text/html},
}

@inproceedings{simonyan_two-stream_2014,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'14},
	title = {Two-stream convolutional networks for action recognition in videos},
	abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
	urldate = {2020-11-27},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {MIT Press},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = dec,
	year = {2014},
	pages = {568--576},
}

@inproceedings{suzuki_self-organization_1988,
	title = {Self-{Organization} of {Associative} {Database} and {Its} {Applications}},
	url = {https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf},
	urldate = {2020-11-27},
	booktitle = {Neural {Information} {Processing} {Systems}},
	publisher = {American Institute of Physics},
	author = {Suzuki, Hisashi and Arimoto, Suguru},
	editor = {Anderson, D.},
	year = {1988},
	pages = {767--774},
	file = {NIPS Snapshot:/home/fernando/Zotero/storage/MHTEMB8F/00ec53c4682d36f5c4359f4ae7bd7ba1-Abstract.html:text/html},
}

@inproceedings{carreira_quo_2017,
	title = {Quo {Vadis}, {Action} {Recognition}? {A} {New} {Model} and the {Kinetics} {Dataset}},
	shorttitle = {Quo {Vadis}, {Action} {Recognition}?},
	doi = {10.1109/CVPR.2017.502},
	abstract = {The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2\% on HMDB-51 and 97.9\% on UCF-101.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Carreira, J. and Zisserman, A.},
	month = jul,
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {image motion analysis, Feature extraction, image classification, spatiotemporal phenomena, video signal processing, feature extraction, learning (artificial intelligence), Kernel, Three-dimensional displays, Videos, Solid modeling, action classification, action recognition, deep image classification ConvNets, image recognition, Kinetic theory, kinetics dataset, successful ImageNet architecture designs, Two dimensional displays, Two-Stream Inflated 3D ConvNet},
	pages = {4724--4733},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/G4XZNLR6/8099985.html:text/html},
}

@inproceedings{tran_learning_2015,
	title = {Learning {Spatiotemporal} {Features} with {3D} {Convolutional} {Networks}},
	doi = {10.1109/ICCV.2015.510},
	abstract = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8\% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Tran, D. and Bourdev, L. and Fergus, R. and Torresani, L. and Paluri, M.},
	month = dec,
	year = {2015},
	note = {ISSN: 2380-7504},
	keywords = {Training, Feature extraction, image classification, neural nets, spatiotemporal phenomena, Streaming media, video signal processing, feature extraction, learning (artificial intelligence), Convolution, Kernel, Three-dimensional displays, Solid modeling, 3D ConvNets, 3D convolutional networks, C3D features, convolution kernels, convolutional 3D, deep 3-dimensional convolutional networks, homogeneous architecture, large scale supervised video dataset, linear classifier, spatiotemporal feature learning, UCF101 dataset},
	pages = {4489--4497},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/PCCH3VSE/7410867.html:text/html},
}

@article{wang_aleatoric_2019,
	title = {Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks},
	volume = {338},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231219301961},
	doi = {10.1016/j.neucom.2019.01.103},
	abstract = {Despite the state-of-the-art performance for medical image segmentation, deep convolutional neural networks (CNNs) have rarely provided uncertainty estimations regarding their segmentation outputs, e.g., model (epistemic) and image-based (aleatoric) uncertainties. In this work, we analyze these different types of uncertainties for CNN-based 2D and 3D medical image segmentation tasks at both pixel level and structure level. We additionally propose a test-time augmentation-based aleatoric uncertainty to analyze the effect of different transformations of the input image on the segmentation output. Test-time augmentation has been previously used to improve segmentation accuracy, yet not been formulated in a consistent mathematical framework. Hence, we also propose a theoretical formulation of test-time augmentation, where a distribution of the prediction is estimated by Monte Carlo simulation with prior distributions of parameters in an image acquisition model that involves image transformations and noise. We compare and combine our proposed aleatoric uncertainty with model uncertainty. Experiments with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic Resonance Images (MRI) showed that 1) the test-time augmentation-based aleatoric uncertainty provides a better uncertainty estimation than calculating the test-time dropout-based model uncertainty alone and helps to reduce overconfident incorrect predictions, and 2) our test-time augmentation outperforms a single-prediction baseline and dropout-based multiple predictions.},
	language = {en},
	urldate = {2020-11-23},
	journal = {Neurocomputing},
	author = {Wang, Guotai and Li, Wenqi and Aertsen, Michael and Deprest, Jan and Ourselin, S{\'e}bastien and Vercauteren, Tom},
	month = apr,
	year = {2019},
	keywords = {Data augmentation, Convolutional neural networks, Medical image segmentation, Uncertainty estimation},
	pages = {34--45},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/XCYJDFRY/S0925231219301961.html:text/html},
}

@article{moshkov_test-time_2020,
	title = {Test-time augmentation for deep learning-based cell segmentation on microscopy images},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-61808-3},
	doi = {10.1038/s41598-020-61808-3},
	abstract = {Recent advancements in deep learning have revolutionized the way microscopy images of cells are processed. Deep learning network architectures have a large number of parameters, thus, in order to reach high accuracy, they require a massive amount of annotated data. A common way of improving accuracy builds on the artificial increase of the training set by using different augmentation techniques. A less common way relies on test-time augmentation (TTA) which yields transformed versions of the image for prediction and the results are merged. In this paper we describe how we have incorporated the test-time argumentation prediction method into two major segmentation approaches utilized in the single-cell analysis of microscopy images. These approaches are semantic segmentation based on the U-Net, and instance segmentation based on the Mask R-CNN models. Our findings show that even if only simple test-time augmentations (such as rotation or flipping and proper merging methods) are applied, TTA can significantly improve prediction accuracy. We have utilized images of tissue and cell cultures from the Data Science Bowl (DSB) 2018 nuclei segmentation competition and other sources. Additionally, boosting the highest-scoring method of the DSB with TTA, we could further improve prediction accuracy, and our method has reached an ever-best score at the DSB.},
	language = {en},
	number = {1},
	urldate = {2020-11-23},
	journal = {Scientific Reports},
	author = {Moshkov, Nikita and Mathe, Botond and Kertesz-Farkas, Attila and Hollandi, Reka and Horvath, Peter},
	month = mar,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {5068},
	file = {Snapshot:/home/fernando/Zotero/storage/EYTTAHC6/s41598-020-61808-3.html:text/html},
}

@inproceedings{billot_partial_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Partial {Volume} {Segmentation} of {Brain} {MRI} {Scans} of {Any} {Resolution} and {Contrast}},
	isbn = {978-3-030-59728-3},
	doi = {10.1007/978-3-030-59728-3_18},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} {\textendash} {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Billot, Benjamin and Robinson, Eleanor and Dalca, Adrian V. and Iglesias, Juan Eugenio},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {Brain MRI, Partial volume segmentation},
	pages = {177--187},
}

@inproceedings{billot_learning_2020,
	title = {A {Learning} {Strategy} for {Contrast}-agnostic {MRI} {Segmentation}},
	language = {en},
	urldate = {2020-11-23},
	booktitle = {Medical {Imaging} with {Deep} {Learning}},
	publisher = {PMLR},
	author = {Billot, Benjamin and Greve, Douglas N. and Leemput, Koen Van and Fischl, Bruce and Iglesias, Juan Eugenio and Dalca, Adrian},
	month = sep,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {75--93},
	file = {Snapshot:/home/fernando/Zotero/storage/TJMXRWIH/billot20a.html:text/html;Snapshot:/home/fernando/Zotero/storage/3S3RZRF9/billot20a.html:text/html},
}

@article{waibel_phoneme_1989,
	title = {Phoneme recognition using time-delay neural networks},
	volume = {37},
	issn = {0096-3518},
	doi = {10.1109/29.21701},
	abstract = {The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5\% correct while the rate obtained by the best of the HMMs was only 93.7\%.{\textless}{\textgreater}},
	number = {3},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Waibel, A. and Hanazawa, T. and Hinton, G. and Shikano, K. and Lang, K. J.},
	month = mar,
	year = {1989},
	note = {Conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing},
	keywords = {neural nets, Computer science, Neural networks, Acoustic testing, Backpropagation, Character recognition, Computer networks, computing units, error backpropagation, hidden Markov models, Hidden Markov models, Loudspeakers, nonlinear decision surfaces, phoneme recognition, Psychology, speech, speech recognition, Speech recognition, temporal shifts, testing tokens, three-layer, time-delay neural networks},
	pages = {328--339},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/3PYFK56J/21701.html:text/html},
}

@article{collobert_natural_2011,
	title = {Natural {Language} {Processing} (almost) from {Scratch}},
	url = {http://arxiv.org/abs/1103.0398},
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
	urldate = {2020-10-30},
	journal = {arXiv:1103.0398 [cs]},
	author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	month = mar,
	year = {2011},
	note = {arXiv: 1103.0398},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/2YVDPFVZ/1103.html:text/html},
}

@article{kalchbrenner_convolutional_2014,
	title = {A {Convolutional} {Neural} {Network} for {Modelling} {Sentences}},
	url = {http://arxiv.org/abs/1404.2188},
	abstract = {The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25\% error reduction in the last task with respect to the strongest baseline.},
	urldate = {2020-10-30},
	journal = {arXiv:1404.2188 [cs]},
	author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
	month = apr,
	year = {2014},
	note = {arXiv: 1404.2188},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/G7DZ79PB/1404.html:text/html},
}

@article{kim_convolutional_2014,
	title = {Convolutional {Neural} {Networks} for {Sentence} {Classification}},
	url = {http://arxiv.org/abs/1408.5882},
	abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
	urldate = {2020-10-30},
	journal = {arXiv:1408.5882 [cs]},
	author = {Kim, Yoon},
	month = sep,
	year = {2014},
	note = {arXiv: 1408.5882},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/U5IVYLK6/1408.html:text/html},
}

@inproceedings{cardoso_scale_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Scale {Factor} {Point} {Spread} {Function} {Matching}: {Beyond} {Aliasing} in {Image} {Resampling}},
	isbn = {978-3-319-24571-3},
	shorttitle = {Scale {Factor} {Point} {Spread} {Function} {Matching}},
	doi = {10.1007/978-3-319-24571-3_81},
	abstract = {Imaging devices exploit the Nyquist-Shannon sampling theorem to avoid both aliasing and redundant oversampling by design. Conversely, in medical image resampling, images are considered as continuous functions, are warped by a spatial transformation, and are then sampled on a regular grid. In most cases, the spatial warping changes the frequency characteristics of the continuous function and no special care is taken to ensure that the resampling grid respects the conditions of the sampling theorem. This paper shows that this oversight introduces artefacts, including aliasing, that can lead to important bias in clinical applications. One notable exception to this common practice is when multi-resolution pyramids are constructed, with low-pass {\textquotedblright}anti-aliasing{\textquotedblright} filters being applied prior to downsampling. In this work, we illustrate why similar caution is needed when resampling images under general spatial transformations and propose a novel method that is more respectful of the sampling theorem, minimising aliasing and loss of information. We introduce the notion of scale factor point spread function (sfPSF) and employ Gaussian kernels to achieve a computationally tractable resampling scheme that can cope with arbitrary non-linear spatial transformations and grid sizes. Experiments demonstrate significant (p {\textless} 10- 4) technical and clinical implications of the proposed method.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} -- {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Cardoso, M. Jorge and Modat, Marc and Vercauteren, Tom and Ourselin, Sebastien},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro},
	year = {2015},
	keywords = {Image Registration, Point Spread Function, Sampling Theorem, Source Image, Spatial Transformation},
	pages = {675--683},
}

@article{larobina_medical_2014,
	title = {Medical {Image} {File} {Formats}},
	volume = {27},
	issn = {0897-1889},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3948928/},
	doi = {10.1007/s10278-013-9657-9},
	abstract = {Image file format is often a confusing aspect for someone wishing to process medical images. This article presents a demystifying overview of the major file formats currently used in medical imaging: Analyze, Neuroimaging Informatics Technology Initiative (Nifti), Minc, and Digital Imaging and Communications in Medicine (Dicom). Concepts common to all file formats, such as pixel depth, photometric interpretation, metadata, and pixel data, are first presented. Then, the characteristics and strengths of the various formats are discussed. The review concludes with some predictive considerations about the future trends in medical image file formats.},
	number = {2},
	urldate = {2020-08-05},
	journal = {Journal of Digital Imaging},
	author = {Larobina, Michele and Murino, Loredana},
	month = apr,
	year = {2014},
	pmid = {24338090},
	pmcid = {PMC3948928},
	pages = {200--206},
}

@article{mancolo_eisen_2020,
	title = {Eisen: a python package for solid deep learning},
	shorttitle = {Eisen},
	url = {http://arxiv.org/abs/2004.02747},
	abstract = {Eisen is an open source python package making the implementation of deep learning methods easy. It is specifically tailored to medical image analysis and computer vision tasks, but its flexibility allows extension to any application. Eisen is based on PyTorch and it follows the same architecture of other packages belonging to the PyTorch ecosystem. This simplifies its use and allows it to be compatible with modules provided by other packages. Eisen implements multiple dataset loading methods, I/O for various data formats, data manipulation and transformation, full implementation of training, validation and test loops, implementation of losses and network architectures, automatic export of training artifacts, summaries and logs, visual experiment building, command line interface and more. Furthermore, it is open to user contributions by the community. Documentation, examples and code can be downloaded from http://eisen.ai.},
	urldate = {2020-07-30},
	journal = {arXiv:2004.02747 [cs, eess]},
	author = {Mancolo, Frank},
	month = mar,
	year = {2020},
	note = {arXiv: 2004.02747},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/9F3W2G3E/2004.html:text/html},
}

@article{cai_spatio-temporal_2020,
	title = {Spatio-temporal visual attention modelling of standard biometry plane-finding navigation},
	volume = {65},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841520301262},
	doi = {10.1016/j.media.2020.101762},
	abstract = {We present a novel multi-task neural network called Temporal SonoEyeNet (TSEN) with a primary task to describe the visual navigation process of sonographers by learning to generate visual attention maps of ultrasound images around standard biometry planes of the fetal abdomen, head (trans-ventricular plane) and femur. TSEN has three components: a feature extractor, a temporal attention module (TAM), and an auxiliary video classification module (VCM). A soft dynamic time warping (sDTW) loss function is used to improve visual attention modelling. Variants of the model are trained on a dataset of 280 video clips, each containing one of the three biometry planes and lasting 3{\textendash}7 seconds, with corresponding real-time recorded gaze tracking data of an experienced sonographer. We report the performances of the different variants of TSEN for visual attention prediction at standard biometry plane detection. The best model performance is achieved using bi-directional convolutional long-short term memory (biCLSTM) in both TAM and VCM, and it outperforms a previous spatial model on all static and dynamic saliency metrics. As an auxiliary task to validate the clinical relevance of the visual attention modelling, the predicted visual attention maps were used to guide standard biometry plane detection in consecutive US video frames. All spatio-temporal TSEN models achieve higher scores compared to a spatial-only baseline; the best performing TSEN model achieves F1 scores on these standard biometry planes of 83.7\%, 89.9\% and 81.1\%, respectively.},
	language = {en},
	urldate = {2020-07-22},
	journal = {Medical Image Analysis},
	author = {Cai, Yifan and Droste, Richard and Sharma, Harshita and Chatelain, Pierre and Drukker, Lior and Papageorghiou, Aris T. and Noble, J. Alison},
	month = oct,
	year = {2020},
	keywords = {Multi-task learning, Fetal ultrasound, Gaze tracking, Saliency prediction, Standard plane detection},
	pages = {101762},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/IR3TUHAL/S1361841520301262.html:text/html},
}

@inproceedings{wang_revisiting_2018,
	title = {Revisiting {Video} {Saliency}: {A} {Large}-{Scale} {Benchmark} and a {New} {Model}},
	shorttitle = {Revisiting {Video} {Saliency}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Revisiting_Video_Saliency_CVPR_2018_paper.html},
	urldate = {2020-07-22},
	author = {Wang, Wenguan and Shen, Jianbing and Guo, Fang and Cheng, Ming-Ming and Borji, Ali},
	year = {2018},
	pages = {4894--4903},
	file = {Snapshot:/home/fernando/Zotero/storage/IXQAQVFN/Wang_Revisiting_Video_Saliency_CVPR_2018_paper.html:text/html},
}

@inproceedings{tian_automated_2020,
	title = {Automated {Analysis} of {Seizure} {Behavior} in {Video}: {Methods} and {Challenges}},
	shorttitle = {Automated {Analysis} of {Seizure} {Behavior} in {Video}},
	doi = {10.1109/WSAI49636.2020.9143279},
	abstract = {Automated analysis of seizure behavior in video using intelligent video analytics technology has significant applications in healthcare industry, since it can provide accurate and quantitative measurement of human seizure behavior for assisting diagnosis. This paper presents a brief survey on intelligent video analytics for automated seizure behavior analysis, including both conventional motion analysis based approaches and the state-of-the-art machine learning based approaches. Furthermore, a new automated video analytics framework is proposed in this paper, by exploiting the machine learning approach to build a seizure motion model and performing automatic detection of seizure events in the surveillance video in real time. This paper also discusses the preliminary experimental results and deployment of the proposed framework, as well as the future research challenges in this area.},
	booktitle = {2020 2nd {World} {Symposium} on {Artificial} {Intelligence} ({WSAI})},
	author = {Tian, Jing and Yu, Weiyu and Chen, Jinquan and Lin, Junke and Wen, Mingfeng and Li, Yingxin and Zhong, Jianxin and Chen, Keqiang and Feng, Xuchu},
	month = jun,
	year = {2020},
	keywords = {motion analysis, machine learning, analytics, seizure behavior},
	pages = {34--37},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/SDCVBIIB/9143279.html:text/html},
}

@misc{preston-werner_semantic_2020,
	title = {Semantic {Versioning} 2.0.0},
	url = {https://semver.org/},
	abstract = {Semantic Versioning spec and website},
	language = {en},
	urldate = {2020-05-20},
	journal = {Semantic Versioning},
	author = {Preston-Werner, Tom},
	year = {2020},
	note = {Library Catalog: semver.org},
	file = {Snapshot:/home/fernando/Zotero/storage/HZRJC276/semver.org.html:text/html},
}

@article{shorten_survey_2019,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	language = {en},
	number = {1},
	urldate = {2020-05-20},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	month = jul,
	year = {2019},
	pages = {60},
}

@book{chollet_keras_2015,
	title = {Keras},
	url = {https://keras.io},
	author = {Chollet, Fran{\c c}ois and {others}},
	year = {2015},
}

@book{bibb_medical_2015,
	title = {Medical {Modelling}: {The} {Application} of {Advanced} {Design} and {Rapid} {Prototyping} {Techniques} in {Medicine} ({Woodhead} {Publishing} {Series} in {Biomaterials})},
	isbn = {978-1-78242-300-3},
	shorttitle = {Medical {Modelling}},
	url = {https://repository.cardiffmet.ac.uk/handle/10369/9337},
	abstract = {Medical modelling and the principles of medical imaging, Computer Aided Design (CAD) and Rapid Prototyping (also known as Additive Manufacturing and 3D Printing) are important techniques relating to various disciplines - from biomaterials engineering to surgery. Medical Modelling: The application of Advanced Design and Rapid Prototyping techniques in medicine provides readers with a revised edition of the original text, along with key information on innovative imaging techniques, Rapid Prototyping technologies and case studies. Following an overview of medical imaging for Rapid Prototyping, the book goes on to discuss working with medical scan data and techniques for Rapid Prototyping. In this second edition there is an extensive section of peer-reviewed case studies, describing the practical applications of advanced design technologies in surgical, prosthetic, orthotic, dental and research applications.},
	language = {en},
	urldate = {2020-05-04},
	publisher = {Woodhead Publishing (Elsevier)},
	author = {Bibb, Richard and Eggbeer, Dominic and Paterson, Abby},
	year = {2015},
	note = {Accepted: 2018-03-09T11:21:05Z},
	file = {Snapshot:/home/fernando/Zotero/storage/32QM9NMD/9337.html:text/html},
}

@inproceedings{drobny_handling_2015,
	address = {Berlin, Heidelberg},
	series = {Informatik aktuell},
	title = {Handling {Non}-{Corresponding} {Regions} in {Image} {Registration}},
	isbn = {978-3-662-46224-9},
	doi = {10.1007/978-3-662-46224-9\_20},
	abstract = {Image registration is particularly challenging if the images to be aligned contain non-corresponding regions. Using state-of-the-art algorithms typically leads to unwanted and unrealistic deformations in these regions. There are various approaches handling this problem which improve registration results, however each with a focus on specific applications. In this note we describe a general approach which can be applied on different mono-modal registration problems. We show the effects of this approach compared to a standard registration algorithm on the basis of five 3D CT lung image pairs where synthetic tumors have been added. We show that our approach significantly reduces unwanted deformation of a non-corresponding tumor. The average volume decrease is 9\% compared to 66\% for the standard approach while the overall accuracy based on landmark error is retained.},
	language = {en},
	booktitle = {Bildverarbeitung f{\"u}r die {Medizin} 2015},
	publisher = {Springer},
	author = {Drobny, David and Carolus, Heike and Kabus, Sven and Modersitzki, Jan},
	editor = {Handels, Heinz and Deserno, Thomas Martin and Meinzer, Hans-Peter and Tolxdorff, Thomas},
	year = {2015},
	keywords = {Image Registration, Deformable Image Registration, Extended Algorithm, Peripheral Tumor, Template Image},
	pages = {107--112},
}

@article{van_engelen_survey_2020,
	title = {A survey on semi-supervised learning},
	volume = {109},
	issn = {1573-0565},
	doi = {10.1007/s10994-019-05855-6},
	abstract = {Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
	language = {en},
	number = {2},
	urldate = {2020-03-14},
	journal = {Machine Learning},
	author = {van Engelen, Jesper E. and Hoos, Holger H.},
	month = feb,
	year = {2020},
	pages = {373--440},
}

@article{jing_self-supervised_2019,
	title = {Self-supervised {Visual} {Feature} {Learning} with {Deep} {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Self-supervised {Visual} {Feature} {Learning} with {Deep} {Neural} {Networks}},
	abstract = {Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the main components and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.},
	urldate = {2020-03-14},
	journal = {arXiv:1902.06162 [cs]},
	author = {Jing, Longlong and Tian, Yingli},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06162},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/S32TNQRY/1902.html:text/html},
}

@article{rohlfing_shape-based_2007,
	title = {Shape-{Based} {Averaging}},
	volume = {16},
	issn = {1941-0042},
	doi = {10.1109/TIP.2006.884936},
	abstract = {A new method for averaging multidimensional images is presented, which is based on signed Euclidean distance maps computed for each of the pixel values. We refer to the algorithm as "shape-based averaging" (SBA) because of its similarity to Raya and Udupa's shape-based interpolation method. The new method does not introduce pixel intensities that were not present in the input data, which makes it suitable for averaging nonnumerical data such as label maps (segmentations). Using segmented human brain magnetic resonance images, SBA is compared to label voting for the purpose of averaging image segmentations in a multiclassifier fashion. SBA, on average, performed as well as label voting in terms of recognition rates of the averaged segmentations. SBA produced more regular and contiguous structures with less fragmentation than did label voting. SBA also was more robust for small numbers of atlases and for low atlas resolutions, in particular, when combined with shape-based interpolation. We conclude that SBA improves the contiguity and accuracy of averaged image segmentations},
	number = {1},
	journal = {IEEE Transactions on Image Processing},
	author = {Rohlfing, Torsten and Maurer, Calvin R.},
	month = jan,
	year = {2007},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Algorithms, Humans, Image Enhancement, image resolution, Magnetic resonance, Signal Processing, Computer-Assisted, Artificial Intelligence, image segmentation, Image segmentation, Pattern Recognition, Automated, Euclidean distance, Voting, Image Interpretation, Computer-Assisted, image recognition, Combination of segmentations, Error correction, Euclidean distance maps, image segmentations, Information Storage and Retrieval, interpolation, Interpolation, multidimensional images, Multidimensional systems, Pixel, pixel values, recognition rates, shape-based averaging, shape-based averaging (SBA), shape-based interpolation (SBI), shape-based interpolation method, signed Euclidean distance transform, Subtraction Technique, Uncertainty},
	pages = {153--161},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/8QUY8PFM/4032827.html:text/html},
}

@inproceedings{chen_deformable_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deformable {Image} {Registration} with {Automatic} {Non}-{Correspondence} {Detection}},
	isbn = {978-3-319-18461-6},
	doi = {10.1007/978-3-319-18461-6\_29},
	abstract = {Image registration aims at establishing pointwise correspondences between given images. However, in many practical applications, no correspondences can be established in certain parts of the images. A typical example is the tumor resection area in pre- and post-operative medical images. In this paper, we introduce a novel variational framework that combines registration with an automatic detection of non-correspondence regions. The formulation of the proposed approach is simple but efficient, and compatible with a large class of image registration similarity measures and regularizers. The resulting minimization problem is solved numerically with a non-alternating gradient flow scheme. Furthermore, the method is validated on synthetic data as well as axial slices of pre-, post- and intra-operative MR T1 head scans.},
	language = {en},
	booktitle = {Scale {Space} and {Variational} {Methods} in {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Chen, Kanglin and Derksen, Alexander and Heldmann, Stefan and Hallmann, Marc and Berkels, Benjamin},
	editor = {Aujol, Jean-Fran{\c c}ois and Nikolova, Mila and Papadakis, Nicolas},
	year = {2015},
	keywords = {Segmentation, Image registration, Joint method, Lesion, Level set, Non-correspondence detection, Resection},
	pages = {360--371},
}

@article{yang_glioma_2018,
	title = {Glioma {Grading} on {Conventional} {MR} {Images}: {A} {Deep} {Learning} {Study} {With} {Transfer} {Learning}},
	volume = {12},
	issn = {1662-453X},
	shorttitle = {Glioma {Grading} on {Conventional} {MR} {Images}},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2018.00804/full},
	doi = {10.3389/fnins.2018.00804},
	abstract = {Background: Accurate glioma grading before surgery is of the utmost importance in treatment planning and prognosis prediction. But previous studies on MRI images were not effective enough. According to the remarkable performance of convolutional neural network (CNN) in medical domain, we hypothesized that a deep learning algorithm can achieve high accuracy in distinguishing the World Health Organization (WHO) low grade and high grade gliomas. Methods: One hundred and thirteen glioma patients were retrospectively included. Tumor images were segmented with a rectangular region of interest (ROI), which contained about 80\% of the tumor. Then, 20\% data were randomly selected and leaved out at patient-level as test dataset. AlexNet and GoogLeNet were both trained from scratch and fine-tuned from models that pre-trained on the large scale natural image database, ImageNet, to magnetic resonance images. The classification task was evaluated with five-fold cross-validation (CV) on patient-level split. Results: The performance measures, including validation accuracy, test accuracy and test area under curve (AUC), averaged from five-fold CV of GoogLeNet which trained from scratch were 0.867, 0.909 and 0.939, respectively. With transfer learning and fine-tuning, better performances were obtained for both AlexNet and GoogLeNet, especially for AlexNet. Meanwhile, GoogLeNet performed better than AlexNet no matter trained from scratch or learned from pre-trained model. Conclusion: In conclusion, we demonstrated that the application of CNN, especially trained with transfer learning and fine-tuning, to preoperative glioma grading improves the performance, compared with either the performance of traditional machine learning method based on hand-crafted features, or even the CNNs trained from scratch.},
	language = {English},
	urldate = {2020-03-05},
	journal = {Frontiers in Neuroscience},
	author = {Yang, Yang and Yan, Lin-Feng and Zhang, Xin and Han, Yu and Nan, Hai-Yan and Hu, Yu-Chuan and Hu, Bo and Yan, Song-Lin and Zhang, Jin and Cheng, Dong-Liang and Ge, Xiang-Wei and Cui, Guang-Bin and Zhao, Di and Wang, Wen},
	year = {2018},
	keywords = {Magnetic Resonance Imaging, Convolutional neural network (CNN), deep learning (DL), Glioma grading, transfer learning (TL)},
}

@article{gomez_reversible_2017,
	title = {The {Reversible} {Residual} {Network}: {Backpropagation} {Without} {Storing} {Activations}},
	shorttitle = {The {Reversible} {Residual} {Network}},
	url = {http://arxiv.org/abs/1707.04585},
	abstract = {Deep residual networks (ResNets) have significantly pushed forward the state-of-the-art on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck, as one needs to store the activations in order to calculate gradients using backpropagation. We present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backpropagation. We demonstrate the effectiveness of RevNets on CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification accuracy to equally-sized ResNets, even though the activation storage requirements are independent of depth.},
	urldate = {2020-03-05},
	journal = {arXiv:1707.04585 [cs]},
	author = {Gomez, Aidan N. and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B.},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.04585},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/Z2XTCVN7/1707.html:text/html},
}

@article{chen_training_2016,
	title = {Training {Deep} {Nets} with {Sublinear} {Memory} {Cost}},
	url = {http://arxiv.org/abs/1604.06174},
	abstract = {We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O(sqrt(n)) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory - giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(log n) with as little as O(n log n) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences.},
	urldate = {2020-03-05},
	journal = {arXiv:1604.06174 [cs]},
	author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.06174},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/7NAGMEFB/1604.html:text/html},
}

@article{hendriks_preoperative_2019,
	title = {Preoperative {Resectability} {Estimates} of {Nonenhancing} {Glioma} by {Neurosurgeons} and a {Resection} {Probability} {Map}},
	volume = {85},
	issn = {0148-396X},
	url = {https://academic.oup.com/neurosurgery/article/85/2/E304/5178544},
	doi = {10.1093/neuros/nyy487},
	abstract = {AbstractBACKGROUND.  Preoperative interpretation of resectability of diffuse nonenhancing glioma is primarily based on individual surgical expertise.OBJECTIVE.},
	language = {en},
	number = {2},
	urldate = {2020-03-05},
	journal = {Neurosurgery},
	author = {Hendriks, Eef J. and Idema, Sander and Hervey-Jumper, Shawn L. and Bernat, Anne-Laure and Zwinderman, Aeilko H. and Barkhof, Frederik and Vandertop, W. Peter and Mandonnet, Emmanuel and Duffau, Hugues and Berger, Mitchel S. and De Witt Hamer, Philip C.},
	month = aug,
	year = {2019},
	pages = {E304--E313},
	file = {Snapshot:/home/fernando/Zotero/storage/T6EKMQMB/5178544.html:text/html},
}

@article{cooley_algorithm_1965,
	title = {An {Algorithm} for the {Machine} {Calculation} of {Complex} {Fourier} {Series}},
	volume = {19},
	issn = {0025-5718},
	url = {https://www.jstor.org/stable/2003354},
	doi = {10.2307/2003354},
	number = {90},
	urldate = {2020-03-05},
	journal = {Mathematics of Computation},
	author = {Cooley, James W. and Tukey, John W.},
	year = {1965},
	pages = {297--301},
}

@misc{jung_imgaug_2020,
	title = {imgaug},
	url = {https://github.com/aleju/imgaug},
	author = {Jung, Alexander B. and Wada, Kentaro and Crall, Jon and Tanaka, Satoshi and Graving, Jake and Reinders, Christoph and Yadav, Sarthak and Banerjee, Joy and Vecsei, G{\'a}bor and Kraft, Adam and Rui, Zheng and Borovec, Jirka and Vallentin, Christian and Zhydenko, Semen and Pfeiffer, Kilian and Cook, Ben and Fern{\'a}ndez, Ismael and De Rainville, Fran{\c c}ois-Michel and Weng, Chi-Hung and Ayala-Acevedo, Abner and Meudec, Raphael and Laporte, Matias and {others}},
	year = {2020},
}

@book{russell_artificial_2009,
	address = {Upper Saddle River, NJ, USA},
	edition = {3rd},
	title = {Artificial {Intelligence}: {A} {Modern} {Approach}},
	isbn = {978-0-13-604259-4},
	shorttitle = {Artificial {Intelligence}},
	abstract = {The long-anticipated revision of this \#1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.},
	publisher = {Prentice Hall Press},
	author = {Russell, Stuart and Norvig, Peter},
	year = {2009},
}

@article{nakagawa_wall--lumen_2016,
	title = {Wall-to-lumen ratio of intracranial arteries measured by indocyanine green angiography},
	volume = {11},
	issn = {1793-5482},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4974959/},
	doi = {10.4103/1793-5482.175637},
	abstract = {Background: The wall-to-lumen ratio (WLR) is an important parameter in vascular medicine because it indicates the character of vascular wall as well as the degree of stenosis. Despite the advances in medical imaging technologies, it is still difficult to measure the thin-walled normal intracranial arteries, and the reports on the WLR of normal intracranial artery are limited. It might be possible to calculate the WLR using the indocyanine green (ICG) angiography, which is used to observe intracranial vessels during microsurgery. Purpose: To evaluate the WLR of normal intracranial arteries using ICG angiography. Materials and Methods: From the three cases in which ICG angiography was recorded with a ruler during microsurgery, 20 measurement points were chosen for the analysis. The ICG was injected intravenously with a dose of 0.2 mg/kg, and the vessels were inspected at high magnification using an operating microscope equipped with near-infrared illumination system. The vessel outer diameter and the luminal diameter were measured using the images before and after the ICG arrival based on the pixel ratio method using a ruler as reference, respectively. The WLR was calculated as 0.5 {\texttimes} (vessel outer diameter - vessel luminal diameter). Results: The WLR (mean {\textpm} standard deviation) of normal intracranial arteries was 0.086 {\textpm} 0.022. The WLR tended to be high in small arteries. Conclusion: The WLR of normal intracranial arteries calculated using ICG angiography was consistent with the WLR reported in the previous reports based on human autopsy.},
	number = {4},
	urldate = {2018-08-09},
	journal = {Asian Journal of Neurosurgery},
	author = {Nakagawa, Daichi and Shojima, Masaaki and Yoshino, Masanori and Kin, Taichi and Imai, Hideaki and Nomura, Seiji and Saito, Toki and Nakatomi, Hirofumi and Oyama, Hiroshi and Saito, Nobuhito},
	year = {2016},
	pmid = {27695538},
	pmcid = {PMC4974959},
	pages = {361--364},
}

@article{marin-padilla_human_2012,
	title = {The human brain intracerebral microvascular system: development and structure},
	volume = {6},
	issn = {1662-5129},
	shorttitle = {The human brain intracerebral microvascular system},
	url = {https://www.frontiersin.org/articles/10.3389/fnana.2012.00038/full},
	doi = {10.3389/fnana.2012.00038},
	abstract = {The capillary from the meningeal inner pial lamella play a crucial role in the development and structural organization of the cerebral cortex extrinsic and intrinsic microvascular compartments. Only pial capillaries are capable of perforating through the cortex external glial limiting membrane (EGLM) to enter into the nervous tissue, although incapable of perforating the membrane to exit the brain. Circulatory dynamics and functional demands determine which capillaries become arterial and which capillaries become venous. The perforation of the cortex EGLM by pial capillaries is a complex process characterized by three fundamental stages: a) pial capillary contact with the EGLM with fusion of vascular and glial basal laminae at the contact site, b) endothelial cell filopodium penetration through the fussed laminae with the formation of a funnel between them that accompanies it into the nervous tissue while remaining open to the meningeal interstitium and, c) penetration of the whole capillary carrying the open funnel with it and establishing an extravascular Virchow-Robin Compartment (V-RC) that maintains the perforating vessel extrinsic (outside) the nervous tissue through its entire length. The V-RC is walled internally by the vascular basal lamina and externally by the basal lamina of joined glial cells endfeet. The VRC outer glial wall appear as an extension of the cortex superficial EGLM. All the perforating vessels within the V-RCs constitute the cerebral cortex extrinsic microvascular compartment. These perforating vessels are the only one capable of responding to inflammatory insults. The V-RC remains open (for life) to the meningeal interstitium permitting the exchanges of fluid and of cells between brain and meninges. The V-RC function as the brain sole drainage (prelymphatic) system in both physiological as well as pathological situations.},
	language = {English},
	urldate = {2018-08-09},
	journal = {Frontiers in Neuroanatomy},
	author = {Mar{\'i}n-Padilla, Miguel},
	year = {2012},
	keywords = {EGLM, endothelial cell filopodium, human brain, Intracerebral Microvascular System, meningeal inner pial lamella},
}

@article{he_learning_2009,
	title = {Learning from {Imbalanced} {Data}},
	volume = {21},
	issn = {1041-4347},
	doi = {10.1109/TKDE.2008.239},
	abstract = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.},
	number = {9},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {He, H. and Garcia, E. A.},
	month = sep,
	year = {2009},
	keywords = {Large-scale systems, classification, learning (artificial intelligence), learning, Surveillance, data mining, active learning, assessment metrics., Availability, complex systems, cost-sensitive learning, Data analysis, data availability, data engineering, Data engineering, Data security, decision making, Decision making, Finance, imbalanced data, Imbalanced learning, IP networks, kernel-based learning, knowledge discovery, Knowledge representation, large-scale systems, networked systems, sampling methods},
	pages = {1263--1284},
}

@book{james_introduction_2013,
	address = {New York},
	series = {Springer {Texts} in {Statistics}, {Springer} {Nature} {Textbooks} {\textendash} {HE} site},
	title = {An {Introduction} to {Statistical} {Learning}: with {Applications} in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An {Introduction} to {Statistical} {Learning}},
	url = {//www.springer.com/gb/book/9781461471370},
	abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
	language = {en},
	urldate = {2018-08-08},
	publisher = {Springer-Verlag},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
}

@article{havaei_brain_2017,
	title = {Brain tumor segmentation with {Deep} {Neural} {Networks}},
	volume = {35},
	issn = {1361-8415},
	doi = {10.1016/j.media.2016.05.004},
	urldate = {2018-08-06},
	journal = {Medical Image Analysis},
	author = {Havaei, Mohammad and Davy, Axel and Warde-Farley, David and Biard, Antoine and Courville, Aaron and Bengio, Yoshua and Pal, Chris and Jodoin, Pierre-Marc and Larochelle, Hugo},
	month = jan,
	year = {2017},
	keywords = {Convolutional neural networks, Brain tumor segmentation, Cascaded convolutional neural networks, Deep neural networks},
	pages = {18--31},
}

@article{bakas_advancing_2017,
	title = {Advancing {The} {Cancer} {Genome} {Atlas} glioma {MRI} collections with expert segmentation labels and radiomic features},
	volume = {4},
	issn = {2052-4463},
	doi = {10.1038/sdata.2017.117},
	abstract = {Gliomas belong to a group of central nervous system tumors, and consist of various sub-regions. Gold standard labeling of these sub-regions in radiographic imaging is essential for both clinical and computational studies, including radiomic and radiogenomic analyses. Towards this end, we release segmentation labels and radiomic features for all pre-operative multimodal magnetic resonance imaging (MRI) (n=243) of the multi-institutional glioma collections of The Cancer Genome Atlas (TCGA), publicly available in The Cancer Imaging Archive (TCIA). Pre-operative scans were identified in both glioblastoma (TCGA-GBM, n=135) and low-grade-glioma (TCGA-LGG, n=108) collections via radiological assessment. The glioma sub-region labels were produced by an automated state-of-the-art method and manually revised by an expert board-certified neuroradiologist. An extensive panel of radiomic features was extracted based on the manually-revised labels. This set of labels and features should enable i) direct utilization of the TCGA/TCIA glioma collections towards repeatable, reproducible and comparative quantitative studies leading to new predictive, prognostic, and diagnostic assessments, as well as ii) performance evaluation of computer-aided segmentation methods, and comparison to our state-of-the-art method.},
	language = {eng},
	journal = {Scientific Data},
	author = {Bakas, Spyridon and Akbari, Hamed and Sotiras, Aristeidis and Bilello, Michel and Rozycki, Martin and Kirby, Justin S. and Freymann, John B. and Farahani, Keyvan and Davatzikos, Christos},
	year = {2017},
	pmid = {28872634},
	pmcid = {PMC5685212},
	keywords = {Humans, Magnetic Resonance Imaging, Multimodal Imaging, Brain Neoplasms, Glioma, Computer-Assisted, Image Interpretation, DNA, Neoplasm},
	pages = {170117},
}

@article{menze_multimodal_2015,
	title = {The {Multimodal} {Brain} {Tumor} {Image} {Segmentation} {Benchmark} ({BRATS})},
	volume = {34},
	issn = {1558-254X},
	doi = {10.1109/TMI.2014.2377694},
	abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients-manually annotated by up to four raters-and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74\%-85\%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
	language = {eng},
	number = {10},
	journal = {IEEE transactions on medical imaging},
	author = {Menze, Bjoern H. and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc-Andr{\'e} and Arbel, Tal and Avants, Brian B. and Ayache, Nicholas and Buendia, Patricia and Collins, D. Louis and Cordier, Nicolas and Corso, Jason J. and Criminisi, Antonio and Das, Tilak and Delingette, Herv{\'e} and Demiralp, {\c C}a{\u g}atay and Durst, Christopher R. and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M. and Jena, Raj and John, Nigel M. and Konukoglu, Ender and Lashkari, Danial and Mariz, Jos{\'e} Antoni{\'o} and Meier, Raphael and Pereira, S{\'e}rgio and Precup, Doina and Price, Stephen J. and Raviv, Tammy Riklin and Reza, Syed M. S. and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo-Chang and Shotton, Jamie and Silva, Carlos A. and Sousa, Nuno and Subbanna, Nagesh K. and Szekely, Gabor and Taylor, Thomas J. and Thomas, Owen M. and Tustison, Nicholas J. and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and Van Leemput, Koen},
	month = oct,
	year = {2015},
	pmid = {25494501},
	pmcid = {PMC4833122},
	keywords = {Algorithms, Humans, Magnetic Resonance Imaging, Neuroimaging, Glioma, Benchmarking},
	pages = {1993--2024},
}

@article{rajchl_neuronet_2018,
	title = {{NeuroNet}: {Fast} and {Robust} {Reproduction} of {Multiple} {Brain} {Image} {Segmentation} {Pipelines}},
	shorttitle = {{NeuroNet}},
	url = {https://openreview.net/forum?id=Hks1TRisM},
	abstract = {NeuroNet is a deep convolutional neural network mimicking multiple popular and state-of-the-art brain segmentation tools including FSL, SPM, and MALPEM. The network is trained on 5,000 T1-weighted...},
	urldate = {2018-08-08},
	author = {Rajchl, Martin and Pawlowski, Nick and Rueckert, Daniel and Matthews, Paul M. and Glocker, Ben},
	month = apr,
	year = {2018},
}

@article{simonyan_very_2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2018-08-06},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = sep,
	year = {2014},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{nair_rectified_2010,
	address = {USA},
	series = {{ICML}'10},
	title = {Rectified {Linear} {Units} {Improve} {Restricted} {Boltzmann} {Machines}},
	isbn = {978-1-60558-907-7},
	url = {http://dl.acm.org/citation.cfm?id=3104322.3104425},
	abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
	urldate = {2018-08-08},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Nair, Vinod and Hinton, Geoffrey E.},
	year = {2010},
	pages = {807--814},
}

@inproceedings{brosch_deep_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep {Convolutional} {Encoder} {Networks} for {Multiple} {Sclerosis} {Lesion} {Segmentation}},
	isbn = {978-3-319-24573-7 978-3-319-24574-4},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-24574-4_1},
	doi = {10.1007/978-3-319-24574-4_1},
	abstract = {We propose a novel segmentation approach based on deep convolutional encoder networks and apply it to the segmentation of multiple sclerosis (MS) lesions in magnetic resonance images. Our model is a neural network that has both convolutional and deconvolutional layers, and combines feature extraction and segmentation prediction in a single model. The joint training of the feature extraction and prediction layers allows the model to automatically learn features that are optimized for accuracy for any given combination of image types. In contrast to existing automatic feature learning approaches, which are typically patch-based, our model learns features from entire images, which eliminates patch selection and redundant calculations at the overlap of neighboring patches and thereby speeds up the training. Our network also uses a novel objective function that works well for segmenting underrepresented classes, such as MS lesions. We have evaluated our method on the publicly available labeled cases from the MS lesion segmentation challenge 2008 data set, showing that our method performs comparably to the state-of-theart. In addition, we have evaluated our method on the images of 500 subjects from an MS clinical trial and varied the number of training samples from 5 to 250 to show that the segmentation performance can be greatly improved by having a representative data set.},
	language = {en},
	urldate = {2018-08-06},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} {\textendash} {MICCAI} 2015},
	publisher = {Springer, Cham},
	author = {Brosch, Tom and Yoo, Youngjin and Tang, Lisa Y. W. and Li, David K. B. and Traboulsee, Anthony and Tam, Roger},
	month = oct,
	year = {2015},
	pages = {3--11},
}

@article{maninis_deep_2016,
	title = {Deep {Retinal} {Image} {Understanding}},
	volume = {9901},
	url = {http://arxiv.org/abs/1609.01103},
	doi = {10.1007/978-3-319-46723-8_17},
	abstract = {This paper presents Deep Retinal Image Understanding (DRIU), a unified framework of retinal image analysis that provides both retinal vessel and optic disc segmentation. We make use of deep Convolutional Neural Networks (CNNs), which have proven revolutionary in other fields of computer vision such as object detection and image classification, and we bring their power to the study of eye fundus images. DRIU uses a base network architecture on which two set of specialized layers are trained to solve both the retinal vessel and optic disc segmentation. We present experimental validation, both qualitative and quantitative, in four public datasets for these tasks. In all of them, DRIU presents super-human performance, that is, it shows results more consistent with a gold standard than a second human annotator used as control.},
	urldate = {2018-08-06},
	journal = {arXiv:1609.01103 [cs]},
	author = {Maninis, Kevis-Kokitsi and Pont-Tuset, Jordi and Arbel{\'a}ez, Pablo and Van Gool, Luc},
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {140--148},
}

@article{rodionov_feasibility_2013,
	title = {Feasibility of multimodal {3D} neuroimaging to guide implantation of intracranial {EEG} electrodes},
	volume = {107},
	issn = {0920-1211},
	url = {http://www.sciencedirect.com/science/article/pii/S0920121113001964},
	doi = {10.1016/j.eplepsyres.2013.08.002},
	abstract = {Summary Background Since intracranial electrode implantation has limited spatial sampling and carries significant risk, placement has to be effective and efficient. Structural and functional imaging of several different modalities contributes to localising the seizure onset zone (SoZ) and eloquent cortex. There is a need to summarise and present this information throughout the pre/intra/post-surgical course. Methods We developed and implemented a multimodal 3D neuroimaging (M3N) pipeline to guide implantation of intracranial EEG (icEEG) electrodes. We report the implementation of the pipeline for operative planning and a description of its use in clinical decision-making. Results The results of intraoperative application of the M3N pipeline demonstrated clinical benefits in all 15 implantation surgeries assessed. The M3N software was used to simulate placement of intracranial electrodes in 2 cases. The key benefits of using the M3N pipeline are illustrated in 3 representative case reports. Conclusion We have demonstrated feasibility of the developed intraoperative M3N pipeline which serves as a prototype for clinical implementation. Further validity studies with larger sample groups are required to determine the utility of M3N in routine surgical practice},
	number = {1},
	urldate = {2018-08-06},
	journal = {Epilepsy Research},
	author = {Rodionov, Roman and Vollmar, Christian and Nowell, Mark and Miserocchi, Anna and Wehner, Tim and Micallef, Caroline and Zombori, Gergely and Ourselin, Sebastien and Diehl, Beate and McEvoy, Andrew W. and Duncan, John S.},
	month = nov,
	year = {2013},
	keywords = {Neuroimaging, Epilepsy, Surgery, Multimodal},
	pages = {91--100},
}

@article{lorigo_curves_2001,
	title = {{CURVES}: {Curve} evolution for vessel segmentation},
	volume = {5},
	issn = {1361-8415},
	shorttitle = {{CURVES}},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841501000408},
	doi = {10.1016/S1361-8415(01)00040-8},
	abstract = {The vasculature is of utmost importance in neurosurgery. Direct visualization of images acquired with current imaging modalities, however, cannot provide a spatial representation of small vessels. These vessels, and their branches which show considerable variations, are most important in planning and performing neurosurgical procedures. In planning they provide information on where the lesion draws its blood supply and where it drains. During surgery the vessels serve as landmarks and guidelines to the lesion. The more minute the information is, the more precise the navigation and localization of computer guided procedures. Beyond neurosurgery and neurological study, vascular information is also crucial in cardiovascular surgery, diagnosis, and research. This paper addresses the problem of automatic segmentation of complicated curvilinear structures in three-dimensional imagery, with the primary application of segmenting vasculature in magnetic resonance angiography (MRA) images. The method presented is based on recent curve and surface evolution work in the computer vision community which models the object boundary as a manifold that evolves iteratively to minimize an energy criterion. This energy criterion is based both on intensity values in the image and on local smoothness properties of the object boundary, which is the vessel wall in this application. In particular, the method handles curves evolving in 3D, in contrast with previous work that has dealt with curves in 2D and surfaces in 3D. Results are presented on cerebral and aortic MRA data as well as lung computed tomography (CT) data.},
	number = {3},
	urldate = {2018-08-06},
	journal = {Medical Image Analysis},
	author = {Lorigo, L. M. and Faugeras, O. D. and Grimson, W. E. L. and Keriven, R. and Kikinis, R. and Nabavi, A. and Westin, C. -F.},
	month = sep,
	year = {2001},
	keywords = {Deformable models, Volumetric vascular segmentation},
	pages = {195--206},
}

@article{kaufmann_complications_2007,
	title = {Complications of {Diagnostic} {Cerebral} {Angiography}: {Evaluation} of 19 826 {Consecutive} {Patients}},
	volume = {243},
	issn = {0033-8419},
	shorttitle = {Complications of {Diagnostic} {Cerebral} {Angiography}},
	url = {https://pubs.rsna.org/doi/full/10.1148/radiol.2433060536},
	doi = {10.1148/radiol.2433060536},
	abstract = {Purpose: To retrospectively evaluate the complications of diagnostic cerebral catheter angiography in 19 826 consecutive patients.Materials and Methods: This HIPAA-compliant study had institutional review board approval, with waiver of informed consent. Demographic, procedural, and complication data in 19 826 consecutive patients undergoing diagnostic cerebral angiography at one institution from 1981 through 2003 were retrospectively reviewed. Neurologic, systemic, and local complications were recorded on the basis of clinical follow-up results after each angiographic examination. Events that occurred within 24 hours of angiography were considered to be complications of the procedure. Multivariable analysis was employed to identify patient and procedural factors significantly associated with neurologic complications.Results: Neurologic complications occurred in 522 examinations (2.63\%), and 27 of these (0.14\%) were strokes with permanent disability. Twelve deaths occurred (0.06\%). Access-site hematoma was the most common complication overall (4.2\%). Factors independently associated with an increased risk of neurologic complication included the indication of atherosclerotic cerebrovascular disease (odds ratio [OR], 2.494), the indication of subarachnoid hemorrhage (OR, 2.523), and the comorbidity of frequent transient ischemic attack (OR, 1.674). Factors independently associated with a decreased risk of neurologic complication were increasing chronologic year in which the procedure was performed (OR, 0.659 per 5-year interval) and involvement of a trainee in the procedure (OR, 0.710).Conclusion: In this review, diagnostic catheter cerebral angiography was found to have relatively low complication rates.{\textcopyright} RSNA, 2007},
	number = {3},
	urldate = {2018-08-05},
	journal = {Radiology},
	author = {Kaufmann, Timothy J. and Huston, John and Mandrekar, Jay N. and Schleck, Cathy D. and Thielen, Kent R. and Kallmes, David F.},
	month = jun,
	year = {2007},
	pages = {812--819},
}

@article{miyazaki_nonenhanced_2008,
	title = {Nonenhanced {MR} {Angiography}},
	volume = {248},
	issn = {0033-8419},
	url = {https://pubs.rsna.org/doi/10.1148/radiol.2481071497},
	doi = {10.1148/radiol.2481071497},
	abstract = {While nonenhanced magnetic resonance (MR) angiographic methods have been available since the earliest days of MR imaging, prolonged acquisition times and image artifacts have generally limited their use in favor of gadolinium-enhanced MR angiographic techniques. However, the combination of recent technical advances and new concerns about the safety of gadolinium-based contrast agents has spurred a resurgence of interest in methods that do not require exogenous contrast material. After a review of basic considerations in vascular imaging, the established methods for nonenhanced MR angiographic techniques, such as time of flight and phase contrast, are considered and their advantages and disadvantages are discussed. This article then focuses on new techniques that are becoming commercially available, such as electrocardiographically gated partial-Fourier fast spin-echo methods and balanced steady-state free precession imaging both with and without arterial spin labeling. Challenges facing these methods and possible solutions are considered. Since different imaging techniques rely on different mechanisms of image contrast, recommendations are offered for which strategies may work best for specific angiographic applications. Developments on the horizon include techniques that provide time-resolved imaging for assessment of flow dynamics by using nonenhanced approaches.{\textcopyright} RSNA, 2008},
	number = {1},
	urldate = {2018-08-06},
	journal = {Radiology},
	author = {Miyazaki, Mitsue and Lee, Vivian S.},
	month = jul,
	year = {2008},
	pages = {20--43},
}

@article{antiga_image-based_2008,
	title = {An image-based modeling framework for patient-specific computational hemodynamics},
	volume = {46},
	issn = {0140-0118, 1741-0444},
	url = {https://link.springer.com/article/10.1007/s11517-008-0420-1},
	doi = {10.1007/s11517-008-0420-1},
	abstract = {We present a modeling framework designed for patient-specific computational hemodynamics to be performed in the context of large-scale studies. The framework takes advantage of the integration of image processing, geometric analysis and mesh generation techniques, with an accent on full automation and high-level interaction. Image segmentation is performed using implicit deformable models taking advantage of a novel approach for selective initialization of vascular branches, as well as of a strategy for the segmentation of small vessels. A robust definition of centerlines provides objective geometric criteria for the automation of surface editing and mesh generation. The framework is available as part of an open-source effort, the Vascular Modeling Toolkit, a first step towards the sharing of tools and data which will be necessary for computational hemodynamics to play a role in evidence-based medicine.},
	language = {en},
	number = {11},
	urldate = {2018-08-03},
	journal = {Medical \& Biological Engineering \& Computing},
	author = {Antiga, Luca and Piccinelli, Marina and Botti, Lorenzo and Ene-Iordache, Bogdan and Remuzzi, Andrea and Steinman, David A.},
	month = nov,
	year = {2008},
	pages = {1097},
}

@article{kingma_adam_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2018-08-01},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	keywords = {Computer Science - Machine Learning},
}

@article{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	urldate = {2018-08-01},
	journal = {arXiv:1502.01852 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@book{cignoni_meshlab_2008,
	title = {{MeshLab}: an {Open}-{Source} {Mesh} {Processing} {Tool}},
	isbn = {978-3-905673-68-5},
	shorttitle = {{MeshLab}},
	url = {https://diglib.eg.org:443/handle/10.2312/LocalChapterEvents.ItalChap.ItalianChapConf2008.129-136},
	abstract = {The paper presents MeshLab, an open source, extensible, mesh processing system that has been developed at the Visual Computing Lab of the ISTI-CNR with the helps of tens of students. We will describe the MeshLab architecture, its main features and design objectives discussing what strategies have been used to support its development. Various examples of the practical uses of MeshLab in research and professional frameworks are reported to show the various capabilities of the presented system.},
	language = {en},
	urldate = {2018-08-02},
	publisher = {The Eurographics Association},
	author = {Cignoni, Paolo and Callieri, Marco and Corsini, Massimiliano and Dellepiane, Matteo and Ganovelli, Fabio and Ranzuglia, Guido},
	year = {2008},
	doi = {10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136},
}

@article{clarkson_niftk_2015,
	title = {The {NifTK} software platform for image-guided interventions: platform overview and {NiftyLink} messaging},
	volume = {10},
	issn = {1861-6410, 1861-6429},
	shorttitle = {The {NifTK} software platform for image-guided interventions},
	url = {https://link.springer.com/article/10.1007/s11548-014-1124-7},
	doi = {10.1007/s11548-014-1124-7},
	abstract = {PurposeTo perform research in image-guided interventions, researchers need a wide variety of software components, and assembling these components into a flexible and reliable system can be a challenging task. In this paper, the NifTK software platform is presented. A key focus has been high-performance streaming of stereo laparoscopic video data, ultrasound data and tracking data simultaneously.MethodsA new messaging library called NiftyLink is introduced that uses the OpenIGTLink protocol and provides the user with easy-to-use asynchronous two-way messaging, high reliability and comprehensive error reporting. A small suite of applications called NiftyGuide has been developed, containing lightweight applications for grabbing data, currently from position trackers and ultrasound scanners. These applications use NiftyLink to stream data into NiftyIGI, which is a workstation-based application, built on top of MITK, for visualisation and user interaction. Design decisions, performance characteristics and initial applications are described in detail. NiftyLink was tested for latency when transmitting images, tracking data, and interleaved imaging and tracking data.ResultsNiftyLink can transmit tracking data at 1,024 frames per second (fps) with latency of 0.31 milliseconds, and 512 KB images with latency of 6.06 milliseconds at 32 fps. NiftyIGI was tested, receiving stereo high-definition laparoscopic video at 30 fps, tracking data from 4 rigid bodies at 20{\textendash}30 fps and ultrasound data at 20 fps with rendering refresh rates between 2 and 20 Hz with no loss of user interaction.ConclusionThese packages form part of the NifTK platform and have proven to be successful in a variety of image-guided surgery projects. Code and documentation for the NifTK platform are available from http://www.niftk.org. NiftyLink is provided open-source under a BSD license and available from http://github.com/NifTK/NiftyLink. The code for this paper is tagged IJCARS-2014.},
	language = {en},
	number = {3},
	urldate = {2018-08-02},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Clarkson, Matthew J. and Zombori, Gergely and Thompson, Steve and Totz, Johannes and Song, Yi and Espak, Miklos and Johnsen, Stian and Hawkes, David and Ourselin, S{\'e}bastien},
	month = mar,
	year = {2015},
	pages = {301--316},
}

@article{sudre_generalised_2017,
	title = {Generalised {Dice} overlap as a deep learning loss function for highly unbalanced segmentations},
	volume = {10553},
	url = {http://arxiv.org/abs/1707.03237},
	doi = {10.1007/978-3-319-67558-9_28},
	abstract = {Deep-learning has proved in recent years to be a powerful tool for image analysis and is now widely used to segment both 2D and 3D medical images. Deep-learning segmentation frameworks rely not only on the choice of network architecture but also on the choice of loss function. When the segmentation process targets rare observations, a severe class imbalance is likely to occur between candidate labels, thus resulting in sub-optimal performance. In order to mitigate this issue, strategies such as the weighted cross-entropy function, the sensitivity function or the Dice loss function, have been proposed. In this work, we investigate the behavior of these loss functions and their sensitivity to learning rate tuning in the presence of different rates of label imbalance across 2D and 3D segmentation tasks. We also propose to use the class re-balancing properties of the Generalized Dice overlap, a known metric for segmentation assessment, as a robust and accurate deep-learning loss function for unbalanced tasks.},
	urldate = {2018-08-01},
	journal = {arXiv:1707.03237 [cs]},
	author = {Sudre, Carole H. and Li, Wenqi and Vercauteren, Tom and Ourselin, S{\'e}bastien and Cardoso, M. Jorge},
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {240--248},
}

@article{chen_y-net_2017,
	title = {Y-net: {3D} intracranial artery segmentation using a convolutional autoencoder},
	shorttitle = {Y-net},
	url = {http://arxiv.org/abs/1712.07194},
	abstract = {Automated segmentation of intracranial arteries on magnetic resonance angiography (MRA) allows for quantification of cerebrovascular features, which provides tools for understanding aging and pathophysiological adaptations of the cerebrovascular system. Using a convolutional autoencoder (CAE) for segmentation is promising as it takes advantage of the autoencoder structure in effective noise reduction and feature extraction by representing high dimensional information with low dimensional latent variables. In this report, an optimized CAE model (Y-net) was trained to learn a 3D segmentation model of intracranial arteries from 49 cases of MRA data. The trained model was shown to perform better than the three traditional segmentation methods in both binary classification and visual evaluation.},
	urldate = {2018-07-30},
	journal = {arXiv:1712.07194 [cs, eess]},
	author = {Chen, Li and Xie, Yanjun and Sun, Jie and Balu, Niranjan and Mossa-Basha, Mahmud and Pimentel, Kristi and Hatsukami, Thomas S. and Hwang, Jenq-Neng and Yuan, Chun},
	month = dec,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{law_three_2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Three {Dimensional} {Curvilinear} {Structure} {Detection} {Using} {Optimally} {Oriented} {Flux}},
	isbn = {978-3-540-88692-1 978-3-540-88693-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-88693-8_27},
	doi = {10.1007/978-3-540-88693-8_27},
	abstract = {This paper proposes a novel curvilinear structure detector, called Optimally Oriented Flux (OOF). OOF finds an optimal axis on which image gradients are projected in order to compute the image gradient flux. The computation of OOF is localized at the boundaries of local spherical regions. It avoids considering closely located adjacent structures. The main advantage of OOF is its robustness against the disturbance induced by closely located adjacent objects. Moreover, the analytical formulation of OOF introduces no additional computation load as compared to the calculation of the Hessian matrix which is widely used for curvilinear structure detection. It is experimentally demonstrated that OOF delivers accurate and stable curvilinear structure detection responses under the interference of closely located adjacent structures as well as image noise.},
	language = {en},
	urldate = {2018-07-31},
	booktitle = {Computer {Vision} {\textendash} {ECCV} 2008},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Law, Max W. K. and Chung, Albert C. S.},
	month = oct,
	year = {2008},
	pages = {368--382},
}

@article{tetteh_deepvesselnet_2018,
	title = {{DeepVesselNet}: {Vessel} {Segmentation}, {Centerline} {Prediction}, and {Bifurcation} {Detection} in 3-{D} {Angiographic} {Volumes}},
	shorttitle = {{DeepVesselNet}},
	url = {http://arxiv.org/abs/1803.09340},
	abstract = {We present DeepVesselNet, an architecture tailored to the challenges to be addressed when extracting vessel networks and corresponding features in 3-D angiography using deep learning. We discuss the problems of low execution speed and high memory requirements associated with full 3-D convolutional networks, high class imbalance arising from low percentage (less than 3\%) of vessel voxels, and unavailability of accurately annotated training data - and offer solutions that are the building blocks of DeepVesselNet. First, we formulate 2-D orthogonal cross-hair filters which make use of 3-D context information. Second, we introduce a class balancing cross-entropy score with false positive rate correction to handle the high class imbalance and high false positive rate problems associated with existing loss functions. Finally, we generate synthetic dataset using a computational angiogenesis model, capable of generating vascular networks under physiological constraints on local network structure and topology, and use these data for transfer learning. DeepVesselNet is optimized for segmenting vessels, predicting centerlines, and localizing bifurcations. We test the performance on a range of angiographic volumes including clinical Time-of-Flight MRA data of the human brain, as well as synchrotron radiation X-ray tomographic microscopy scans of the rat brain. Our experiments show that, by replacing 3-D filters with 2-D orthogonal cross-hair filters in our network, speed is improved by 23\% while accuracy is maintained. Our class balancing metric is crucial for training the network and pre-training with synthetic data helps in early convergence of the training process.},
	urldate = {2018-07-30},
	journal = {arXiv:1803.09340 [cs]},
	author = {Tetteh, Giles and Efremov, Velizar and Forkert, Nils D. and Schneider, Matthias and Kirschke, Jan and Weber, Bruno and Zimmer, Claus and Piraud, Marie and Menze, Bjoern H.},
	month = mar,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{haft-javaherian_deep_2018,
	title = {Deep convolutional neural networks for segmenting {3D} in vivo multiphoton images of vasculature in {Alzheimer} disease mouse models},
	url = {http://arxiv.org/abs/1801.00880},
	abstract = {The health and function of tissue rely on its vasculature network to provide reliable blood perfusion. Volumetric imaging approaches, such as multiphoton microscopy, are able to generate detailed 3D images of blood vessels that could contribute to our understanding of the role of vascular structure in normal physiology and in disease mechanisms. The segmentation of vessels, a core image analysis problem, is a bottleneck that has prevented the systematic comparison of 3D vascular architecture across experimental populations. We explored the use of convolutional neural networks to segment 3D vessels within volumetric in vivo images acquired by multiphoton microscopy. We evaluated different network architectures and machine learning techniques in the context of this segmentation problem. We show that our optimized convolutional neural network architecture, which we call DeepVess, yielded a segmentation accuracy that was better than both the current state-of-the-art and a trained human annotator, while also being orders of magnitude faster. To explore the effects of aging and Alzheimer's disease on capillaries, we applied DeepVess to 3D images of cortical blood vessels in young and old mouse models of Alzheimer's disease and wild type littermates. We found little difference in the distribution of capillary diameter or tortuosity between these groups, but did note a decrease in the number of longer capillary segments (\${\textbackslash}textgreater75{\textbackslash}textbackslashmu m\$) in aged animals as compared to young, in both wild type and Alzheimer's disease mouse models.},
	urldate = {2018-07-30},
	journal = {arXiv:1801.00880 [cs]},
	author = {Haft-Javaherian, Mohammad and Fang, Linjing and Muse, Victorine and Schaffer, Chris B. and Nishimura, Nozomi and Sabuncu, Mert R.},
	month = jan,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.2.1, I.2.6, I.4.3, I.4.6, I.5.1, I.5.4},
}

@article{srinidhi_recent_2017,
	title = {Recent {Advancements} in {Retinal} {Vessel} {Segmentation}},
	volume = {41},
	issn = {0148-5598, 1573-689X},
	url = {https://link.springer.com/article/10.1007/s10916-017-0719-2},
	doi = {10.1007/s10916-017-0719-2},
	abstract = {Retinal vessel segmentation is a key step towards the accurate visualization, diagnosis, early treatment and surgery planning of ocular diseases. For the last two decades, a tremendous amount of research has been dedicated in developing automated methods for segmentation of blood vessels from retinal fundus images. Despite the fact, segmentation of retinal vessels still remains a challenging task due to the presence of abnormalities, varying size and shape of the vessels, non-uniform illumination and anatomical variability between subjects. In this paper, we carry out a systematic review of the most recent advancements in retinal vessel segmentation methods published in last five years. The objectives of this study are as follows: first, we discuss the most crucial preprocessing steps that are involved in accurate segmentation of vessels. Second, we review most recent state-of-the-art retinal vessel segmentation techniques which are classified into different categories based on their main principle. Third, we quantitatively analyse these methods in terms of its sensitivity, specificity, accuracy, area under the curve and discuss newly introduced performance metrics in current literature. Fourth, we discuss the advantages and limitations of the existing segmentation techniques. Finally, we provide an insight into active problems and possible future directions towards building successful computer-aided diagnostic system.},
	language = {en},
	number = {4},
	urldate = {2018-07-30},
	journal = {Journal of Medical Systems},
	author = {Srinidhi, Chetan L. and Aparna, P. and Rajan, Jeny},
	month = apr,
	year = {2017},
	pages = {70},
}

@inproceedings{kamnitsas_deepmedic_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DeepMedic} for {Brain} {Tumor} {Segmentation}},
	isbn = {978-3-319-55523-2 978-3-319-55524-9},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-55524-9_14},
	doi = {10.1007/978-3-319-55524-9_14},
	abstract = {Accurate automatic algorithms for the segmentation of brain tumours have the potential of improving disease diagnosis, treatment planning, as well as enabling large-scale studies of the pathology. In this work we employ DeepMedic [1], a 3D CNN architecture previously presented for lesion segmentation, which we further improve by adding residual connections. We also present a series of experiments on the BRATS 2015 training database for evaluating the robustness of the network when less training data are available or less filters are used, aiming to shed some light on requirements for employing such a system. Our method was further benchmarked on the BRATS 2016 Challenge, where it achieved very good performance despite the simplicity of the pipeline.},
	language = {en},
	urldate = {2018-07-25},
	booktitle = {Brainlesion: {Glioma}, {Multiple} {Sclerosis}, {Stroke} and {Traumatic} {Brain} {Injuries}},
	publisher = {Springer, Cham},
	author = {Kamnitsas, Konstantinos and Ferrante, Enzo and Parisot, Sarah and Ledig, Christian and Nori, Aditya V. and Criminisi, Antonio and Rueckert, Daniel and Glocker, Ben},
	month = oct,
	year = {2016},
	pages = {138--149},
}

@article{wang_interactive_2018,
	title = {Interactive {Medical} {Image} {Segmentation} using {Deep} {Learning} with {Image}-specific {Fine}-tuning},
	volume = {37},
	issn = {0278-0062, 1558-254X},
	url = {http://arxiv.org/abs/1710.04043},
	doi = {10.1109/TMI.2018.2791721},
	abstract = {Convolutional neural networks (CNNs) have achieved state-of-the-art performance for automatic medical image segmentation. However, they have not demonstrated sufficiently accurate and robust results for clinical use. In addition, they are limited by the lack of image-specific adaptation and the lack of generalizability to previously unseen object classes. To address these problems, we propose a novel deep learning-based framework for interactive segmentation by incorporating CNNs into a bounding box and scribble-based segmentation pipeline. We propose image-specific fine-tuning to make a CNN model adaptive to a specific test image, which can be either unsupervised (without additional user interactions) or supervised (with additional scribbles). We also propose a weighted loss function considering network and interaction-based uncertainty for the fine-tuning. We applied this framework to two applications: 2D segmentation of multiple organs from fetal MR slices, where only two types of these organs were annotated for training; and 3D segmentation of brain tumor core (excluding edema) and whole brain tumor (including edema) from different MR sequences, where only tumor cores in one MR sequence were annotated for training. Experimental results show that 1) our model is more robust to segment previously unseen objects than state-of-the-art CNNs; 2) image-specific fine-tuning with the proposed weighted loss function significantly improves segmentation accuracy; and 3) our method leads to accurate results with fewer user interactions and less user time than traditional interactive segmentation methods.},
	number = {7},
	urldate = {2018-07-25},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Wang, Guotai and Li, Wenqi and Zuluaga, Maria A. and Pratt, Rosalind and Patel, Premal A. and Aertsen, Michael and Doel, Tom and David, Anna L. and Deprest, Jan and Ourselin, Sebastien and Vercauteren, Tom},
	month = jul,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1562--1573},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers{\textemdash}8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2018-07-25},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{dumoulin_guide_2016,
	title = {A guide to convolution arithmetic for deep learning},
	url = {https://arxiv.org/abs/1603.07285},
	language = {en},
	urldate = {2018-07-25},
	author = {Dumoulin, Vincent and Visin, Francesco},
	month = mar,
	year = {2016},
}

@inproceedings{krizhevsky_imagenet_2012,
	address = {USA},
	series = {{NIPS}'12},
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	urldate = {2018-07-25},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {Curran Associates Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
	pages = {1097--1105},
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2018-07-25},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{shen_deep_2017,
	title = {Deep {Learning} in {Medical} {Image} {Analysis}},
	volume = {19},
	issn = {1545-4274},
	doi = {10.1146/annurev-bioeng-071516-044442},
	abstract = {This review covers computer-assisted analysis of images in the field of medical imaging. Recent advances in machine learning, especially with regard to deep learning, are helping to identify, classify, and quantify patterns in medical images. At the core of these advances is the ability to exploit hierarchical feature representations learned solely from data, instead of features designed by hand according to domain-specific knowledge. Deep learning is rapidly becoming the state of the art, leading to enhanced performance in various medical applications. We introduce the fundamentals of deep learning methods and review their successes in image registration, detection of anatomical and cellular structures, tissue segmentation, computer-aided disease diagnosis and prognosis, and so on. We conclude by discussing research issues and suggesting future directions for further improvement.},
	language = {eng},
	journal = {Annual Review of Biomedical Engineering},
	author = {Shen, Dinggang and Wu, Guorong and Suk, Heung-Il},
	month = jun,
	year = {2017},
	pmid = {28301734},
	pmcid = {PMC5479722},
	keywords = {Algorithms, Image Enhancement, deep learning, Computer-Assisted, Image Interpretation, Neural Networks (Computer), Diagnostic Imaging, Automated, medical image analysis, Pattern Recognition, unsupervised feature learning, Unsupervised Machine Learning},
	pages = {221--248},
}

@inproceedings{ajmal_convolutional_2018,
	title = {Convolutional neural network based image segmentation: a review},
	volume = {10649},
	shorttitle = {Convolutional neural network based image segmentation},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10649/106490N/Convolutional-neural-network-based-image-segmentation-a-review/10.1117/12.2304711.short},
	doi = {10.1117/12.2304711},
	abstract = {Object recognition and semantic segmentation have been the two most common problems of traditional scene understanding in the computer vision domain. Major breakthroughs were reported in the last few years because of the increased utilization of deep learning, which offer a convincing alternative by learning the problem specific features on their own. In this paper, a summary of the frequently used framework {\textendash} convolutional neural networks (CNN) is discussed. Accordingly a categorization scheme has been proposed to analyze the deep networks developed for image segmentation. Under this scheme, thirteen methods from the literature have been reviewed which are classified on the basis on how they perform segmentation operation i.e. semantic segmentation, instance segmentation and hybrid approaches. These method were reviewed from different aspects like their category, the novelty in the architecture of the method, and their special features in contrast with the traditional approaches. Latest review and analysis of these segmentation approaches, which provided outstanding results for image segmentation compared to the ordinary system, reveals that deep learning is increasingly becoming an important part of image segmentation and improvement in deep learning algorithms, which could resolve computer vision problems.},
	urldate = {2018-07-25},
	booktitle = {Pattern {Recognition} and {Tracking} {XXIX}},
	publisher = {International Society for Optics and Photonics},
	author = {Ajmal, Hina and Rehman, Saad and Farooq, Umar and Ain, Qurrat U. and Riaz, Farhan and Hassan, Ali},
	month = apr,
	year = {2018},
	pages = {106490N},
}

@inproceedings{zhiqiang_review_2017,
	title = {A review of object detection based on convolutional neural network},
	doi = {10.23919/ChiCC.2017.8029130},
	abstract = {With the development of intelligent device and social media, the data bulk on Internet has grown with high speed. As an important aspect of image processing, object detection has become one of the international popular research fields. In recent years, the powerful ability with feature learning and transfer learning of Convolutional Neural Network (CNN) has received growing interest within the computer vision community, thus making a series of important breakthroughs in object detection. So it is a significant survey that how to apply CNN to object detection for better performance. First the paper introduced the basic concept and architecture of CNN. Secondly the methods that how to solve the existing problems of conventional object detection are surveyed, mainly analyzing the detection algorithm based on region proposal and based on regression. Thirdly it mentioned some means which improve the performance of object detection. Then the paper introduced some public datasets of object detection and the concept of evaluation criterion. Finally, it combed the current research achievements and thoughts of object detection, summarizing the important progress and discussing the future directions.},
	booktitle = {2017 36th {Chinese} {Control} {Conference} ({CCC})},
	author = {Zhiqiang, W. and Jun, L.},
	month = jul,
	year = {2017},
	keywords = {computer vision, Internet, social networking (online), Training, CNN, Feature extraction, neural nets, Computer vision, learning (artificial intelligence), convolutional neural network, object detection, transfer learning, Convolutional Neural Network, datasets, feature learning, image processing, intelligent device, Object detection, Proposals, region proposal, regression, regression analysis, social media},
	pages = {11104--11109},
}

@article{ravi_deep_2017,
	title = {Deep {Learning} for {Health} {Informatics}},
	volume = {21},
	issn = {2168-2194},
	doi = {10.1109/JBHI.2016.2636665},
	abstract = {With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.},
	number = {1},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Rav{\`i}, D. and Wong, C. and Deligianni, F. and Berthelot, M. and Andreu-Perez, J. and Lo, B. and Yang, G. Z.},
	month = jan,
	year = {2017},
	keywords = {Humans, Machine Learning, Training, neural nets, deep learning, Monitoring, learning (artificial intelligence), Machine learning, machine learning, Biomedical imaging, Ambulatory, artificial intelligence, Artificial neural networks, bioinformatics, Bioinformatics, Biological neural networks, Computational Biology, health informatics, Informatics, medical imaging, medical informatics, Medical Informatics, medical information systems, multimodality data, Neurons, parallelization, pervasive sensing, public health, Public Health, translational bioinformatics, wearable devices},
	pages = {4--21},
}

@article{meijs_robust_2017,
	title = {Robust {Segmentation} of the {Full} {Cerebral} {Vasculature} in {4D} {CT} of {Suspected} {Stroke} {Patients}},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-15617-w},
	doi = {10.1038/s41598-017-15617-w},
	abstract = {A robust method is presented for the segmentation of the full cerebral vasculature in 4-dimensional (4D) computed tomography (CT). The method consists of candidate vessel selection, feature extraction, random forest classification and postprocessing. Image features include among others the weighted temporal variance image and parameters, including entropy, of an intensity histogram in a local region at different scales. These histogram parameters revealed to be a strong feature in the detection of vessels regardless of shape and size. The method was trained and tested on a large database of 264 patients with suspicion of acute ischemia who underwent 4D CT in our hospital in the period January 2014 to December 2015. Five subvolumes representing different regions of the cerebral vasculature were annotated in each image in the training set by medical assistants. The evaluation was done on 242 patients. A total of 16 ({\textbackslash}textless8\%) patients showed severe under or over segmentation and were reported as failures. One out of five subvolumes was randomly annotated in 159 patients and was used for quantitative evaluation. Quantitative evaluation showed a Dice coefficient of 0.91 {\textpm} 0.07 and a modified Hausdorff distance of 0.23 {\textpm} 0.22 mm. Therefore, robust vessel segmentation in 4D CT is feasible with good accuracy.},
	language = {en},
	number = {1},
	urldate = {2018-07-25},
	journal = {Scientific Reports},
	author = {Meijs, Midas and Patel, Ajay and Leemput, Sil C. van de and Prokop, Mathias and Dijk, Ewoud J. van and Leeuw, Frank-Erik de and Meijer, Frederick J. A. and Ginneken, Bram van and Manniesing, Rashindra},
	month = nov,
	year = {2017},
	pages = {15622},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {0018-9219},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	keywords = {Feature extraction, Machine learning, Neural networks, backpropagation, multilayer perceptrons, Character recognition, Hidden Markov models, 2D shape variability, back-propagation, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, high-dimensional patterns, language modeling, Multi-layer neural network, multilayer neural networks, multimodule systems, optical character recognition, Optical character recognition software, Optical computing, Pattern recognition, performance measure minimization, Principal component analysis, segmentation recognition},
	pages = {2278--2324},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2018-07-25},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
}

@article{serag_segma_2017,
	title = {{SEGMA}: {An} {Automatic} {SEGMentation} {Approach} for {Human} {Brain} {MRI} {Using} {Sliding} {Window} and {Random} {Forests}},
	volume = {11},
	issn = {1662-5196},
	shorttitle = {{SEGMA}},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2017.00002/full},
	doi = {10.3389/fninf.2017.00002},
	abstract = {Quantitative volumes from brain magnetic resonance imaging (MRI) acquired across the life course may be useful for investigating long term effects of risk and resilience factors for brain development and healthy ageing, and for understanding early life determinants of adult brain structure. Therefore, there is an increasing need for automated segmentation tools that can be applied to images acquired at different life stages. We developed an automatic segmentation method for human brain MRI, where a sliding window approach and a multi-class random forest classifier were applied to high-dimensional feature vectors for accurate segmentation. The method performed well on brain MRI data acquired from 179 individuals, analysed in three age groups: newborns (38-42 weeks gestational age), children and adolescents (4-17 years) and adults (35-71 years). As the method can learn from partially labelled datasets, it can be used to segment large-scale datasets efficiently. It could also be applied to different populations and imaging modalities across the life course.},
	language = {English},
	urldate = {2018-07-25},
	journal = {Frontiers in Neuroinformatics},
	author = {Serag, Ahmed and Wilkinson, Alastair G. and Telford, Emma J. and Pataky, Rozalia and Sparrow, Sarah A. and Anblagan, Devasuda and Macnaught, Gillian and Semple, Scott I. and Boardman, James P.},
	year = {2017},
	keywords = {Brain, Classification, large-scale, MRI, adulthood, childhood and adolescence, Life-course, Neonatal period, Random forests, sliding window, Tissue segmentation},
}

@inproceedings{frangi_multiscale_1998,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multiscale vessel enhancement filtering},
	isbn = {978-3-540-65136-9 978-3-540-49563-5},
	url = {https://link.springer.com/chapter/10.1007/BFb0056195},
	doi = {10.1007/BFb0056195},
	abstract = {The multiscale second order local structure of an image (Hessian) is examined with the purpose of developing a vessel enhancement filter. A vesselness measure is obtained on the basis of all eigenvalues of the Hessian. This measure is tested on two dimensional DSA and three dimensional aortoiliac and cerebral MRA data. Its clinical utility is shown by the simultaneous noise and background suppression and vessel enhancement in maximum intensity projections and volumetric displays.},
	language = {en},
	urldate = {2018-07-23},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} {\textemdash} {MICCAI}{\textquoteright}98},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Frangi, Alejandro F. and Niessen, Wiro J. and Vincken, Koen L. and Viergever, Max A.},
	month = oct,
	year = {1998},
	pages = {130--137},
}

@article{frenay_classification_2014,
	title = {Classification in the {Presence} of {Label} {Noise}: {A} {Survey}},
	volume = {25},
	issn = {2162-237X},
	shorttitle = {Classification in the {Presence} of {Label} {Noise}},
	doi = {10.1109/TNNLS.2013.2292894},
	abstract = {Label noise is an important issue in classification, with many potential negative consequences. For example, the accuracy of predictions may decrease, whereas the complexity of inferred models and the number of necessary training samples may increase. Many works in the literature have been devoted to the study of label noise and the development of techniques to deal with label noise. However, the field lacks a comprehensive survey on the different types of label noise, their consequences and the algorithms that consider label noise. This paper proposes to fill this gap. First, the definitions and sources of label noise are considered and a taxonomy of the types of label noise is proposed. Second, the potential consequences of label noise are discussed. Third, label noise-robust, label noise cleansing, and label noise-tolerant algorithms are reviewed. For each category of approaches, a short discussion is proposed to help the practitioner to choose the most suitable technique in its own particular field of application. Eventually, the design of experiments is also discussed, what may interest the researchers who would like to test their own algorithms. In this paper, label noise consists of mislabeled instances: no additional information is assumed to be available like e.g., confidences on labels.},
	number = {5},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Frenay, B. and Verleysen, M.},
	month = may,
	year = {2014},
	keywords = {Reliability, Taxonomy, Training, classification, learning (artificial intelligence), Labeling, machine learning, survey, Context, pattern classification, Class noise, label noise, label noise classification, label noise cleansing algorithm, label noise-robust algorithm, label noise-tolerant algorithms, mislabeling, Noise, Noise measurement, potential negative consequences, robust methods, survey.},
	pages = {845--869},
}

@article{sato_three-dimensional_1998,
	title = {Three-dimensional multi-scale line filter for segmentation and visualization of curvilinear structures in medical images},
	volume = {2},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841598800091},
	doi = {10.1016/S1361-8415(98)80009-1},
	abstract = {This paper describes a method for the enhancement of curvilinear structures such as vessels and bronchi in three-dimensional (3-D) medical images. A 3-D line enhancement filter is developed with the aim of discriminating line structures from other structures and recovering line structures of various widths. The 3-D line filter is based on a combination of the eigenvalues of the 3-D Hessian matrix. Multi-scale integration is formulated by taking the maximum among single-scale filter responses, and its characteristics are examined to derive criteria for the selection of parameters in the formulation. The resultant multi-scale line-filtered images provide significantly improved segmentation and visualization of curvilinear structures. The usefulness of the method is demonstrated by the segmentation and visualization of brain vessels from magnetic resonance imaging (MRI) and magnetic resonance angiography (MRA), bronchi from a chest CT, and liver vessels (portal veins) from an abdominal CT.},
	number = {2},
	urldate = {2018-07-23},
	journal = {Medical Image Analysis},
	author = {Sato, Yoshinobu and Nakajima, Shin and Shiraga, Nobuyuki and Atsumi, Hideki and Yoshida, Shigeyuki and Koller, Thomas and Gerig, Guido and Kikinis, Ron},
	month = jun,
	year = {1998},
	keywords = {3-D image analysis, line detection, line measure, multi-scale integration, vessel enhancement},
	pages = {143--168},
}

@article{momi_multi-trajectories_2014,
	title = {Multi-trajectories automatic planner for {StereoElectroEncephaloGraphy} ({SEEG})},
	volume = {9},
	issn = {1861-6410, 1861-6429},
	url = {https://link.springer.com/article/10.1007/s11548-014-1004-1},
	doi = {10.1007/s11548-014-1004-1},
	abstract = {Purpose StereoElectroEncephaloGraphy (SEEG) is done to identify the epileptogenic zone of the brain using several multi-lead electrodes whose positions in the brain are pre-operatively defined. Intracranial hemorrhages due to disruption of blood vessels can cause major complications of this procedure ({\textbackslash}textless{\textbackslash}textless{\textbackslash}textless1 \%). In order to increase the intervention safety, we developed and tested planning tools to assist neurosurgeons in choosing the best trajectory configuration.Methods An automated planning method was developed that maximizes the distance of the electrode from the vessels and avoids the sulci as entry points. The angle of the guiding screws is optimized to reduce positioning error. The planner was quantitatively and qualitatively compared with manually computed trajectories on 26 electrodes planned for three patients undergoing SEEG by four neurosurgeons. Quantitative comparison was performed computing for each trajectory using (a) the Euclidean distance from the closest vessel and (b) the incidence angle.Results Quantitative evaluation shows that automatic planned trajectories are safer in terms of distance from the closest vessel with respect to manually planned trajectories. Qualitative evaluation performed by four neurosurgeons showed that the automatically computed trajectories would have been preferred to manually computed ones in 30 \% of the cases and were judged good or acceptable in about 86 \% of the cases. A significant reduction in time required for planning was observed with the automated system (approximately 1/10).Conclusion The automatic SEEG electrode planner satisfied the essential clinical requirements, by providing safe trajectories in an efficient timeframe.},
	language = {en},
	number = {6},
	urldate = {2018-07-23},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Momi, E. De and Caborni, C. and Cardinale, F. and Casaceli, G. and Castana, L. and Cossu, M. and Mai, R. and Gozzo, F. and Francione, S. and Tassi, L. and Russo, G. Lo and Antiga, L. and Ferrigno, G.},
	month = nov,
	year = {2014},
	pages = {1087--1097},
}

@inproceedings{taubin_optimal_1996,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Optimal surface smoothing as filter design},
	isbn = {978-3-540-61122-6 978-3-540-49949-7},
	url = {https://link.springer.com/chapter/10.1007/BFb0015544},
	doi = {10.1007/BFb0015544},
	abstract = {Smooth surfaces are approximated by polyhedral surfaces for a number of computational purposes. An inherent problem of these approximation algorithms is that the resulting polyhedral surfaces appear faceted. Within a recently introduced signal processing approach to solving this problem [7, 8], surface smoothing corresponds to low-pass filtering. In this paper we look at the filter design problem in more detail. We analyze the stability properties of the low-pass filter described in [7, 8], and show how to minimize its running time. We show that most classical techniques used to design finite impulse response (FIR) digital filters can also be used to design significantly faster surface smoothing filters. Finally, we describe an algorithm to estimate the power spectrum of a signal, and use it to evaluate the performance of the different filter design techniques described in the paper.},
	language = {en},
	urldate = {2018-07-19},
	booktitle = {Computer {Vision} {\textemdash} {ECCV} '96},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Taubin, Gabriel and Zhang, Tong and Golub, Gene},
	month = apr,
	year = {1996},
	pages = {283--292},
}

@article{reed_training_2014,
	title = {Training {Deep} {Neural} {Networks} on {Noisy} {Labels} with {Bootstrapping}},
	url = {http://arxiv.org/abs/1412.6596},
	abstract = {Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.},
	urldate = {2018-07-18},
	journal = {arXiv:1412.6596 [cs]},
	author = {Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
	month = dec,
	year = {2014},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{cardinale_stereoelectroencephalography_2013,
	title = {Stereoelectroencephalography: surgical methodology, safety, and stereotactic application accuracy in 500 procedures},
	volume = {72},
	issn = {1524-4040},
	shorttitle = {Stereoelectroencephalography},
	doi = {10.1227/NEU.0b013e31827d1161},
	abstract = {BACKGROUND: Stereoelectroencephalography (SEEG) methodology, originally developed by Talairach and Bancaud, is progressively gaining popularity for the presurgical invasive evaluation of drug-resistant epilepsies. OBJECTIVE: To describe recent SEEG methodological implementations carried out in our center, to evaluate safety, and to analyze in vivo application accuracy in a consecutive series of 500 procedures with a total of 6496 implanted electrodes. METHODS: Four hundred nineteen procedures were performed with the traditional 2-step surgical workflow, which was modified for the subsequent 81 procedures. The new workflow entailed acquisition of brain 3-dimensional angiography and magnetic resonance imaging in frameless and markerless conditions, advanced multimodal planning, and robot-assisted implantation. Quantitative analysis for in vivo entry point and target point localization error was performed on a sub{\textendash}data set of 118 procedures (1567 electrodes). RESULTS: The methodology allowed successful implantation in all cases. Major complication rate was 12 of 500 (2.4\%), including 1 death for indirect morbidity. Median entry point localization error was 1.43 mm (interquartile range, 0.91-2.21 mm) with the traditional workflow and 0.78 mm (interquartile range, 0.49-1.08 mm) with the new one (P {\textbackslash}textless 2.2 {\texttimes} 10). Median target point localization errors were 2.69 mm (interquartile range, 1.89-3.67 mm) and 1.77 mm (interquartile range, 1.25-2.51 mm; P {\textbackslash}textless 2.2 {\texttimes} 10), respectively. CONCLUSION: SEEG is a safe and accurate procedure for the invasive assessment of the epileptogenic zone. Traditional Talairach methodology, implemented by multimodal planning and robot-assisted surgery, allows direct electrical recording from superficial and deep-seated brain structures, providing essential information in the most complex cases of drug-resistant epilepsy.},
	language = {eng},
	number = {3},
	journal = {Neurosurgery},
	author = {Cardinale, Francesco and Cossu, Massimo and Castana, Laura and Casaceli, Giuseppe and Schiariti, Marco Paolo and Miserocchi, Anna and Fuschillo, Dalila and Moscato, Alessio and Caborni, Chiara and Arnulfo, Gabriele and Lo Russo, Giorgio},
	month = mar,
	year = {2013},
	pmid = {23168681},
	keywords = {Algorithms, Brain, Humans, Magnetic Resonance Imaging, Adolescent, Adult, Female, Male, Middle Aged, Young Adult, Epilepsy, Electroencephalography, Child, Drug Resistance, Neurosurgical Procedures, Computer-Assisted, Electrodes, Robotics, Surgery, Multivariate Analysis, Imaging, Cerebral Angiography, Implanted, Preschool, Safety, Stereotaxic Techniques, Three-Dimensional},
	pages = {353--366; discussion 366},
}

@book{nice_epilepsies_2012,
	title = {Epilepsies: diagnosis and management},
	shorttitle = {Epilepsies},
	url = {https://www.nice.org.uk/guidance/cg137},
	language = {eng},
	urldate = {2018-07-04},
	publisher = {National Institute for Health and Care Excellence},
	author = {{NICE}},
	year = {2012},
}

@article{duncan_selecting_2011,
	title = {Selecting patients for epilepsy surgery: {Synthesis} of data},
	volume = {20},
	issn = {1525-5050, 1525-5069},
	shorttitle = {Selecting patients for epilepsy surgery},
	url = {https://www.epilepsybehavior.com/article/S1525-5050(10)00461-0/abstract},
	doi = {10.1016/j.yebeh.2010.06.040},
	language = {English},
	number = {2},
	urldate = {2018-07-04},
	journal = {Epilepsy \& Behavior},
	author = {Duncan, John S.},
	month = feb,
	year = {2011},
	pmid = {20709601},
	keywords = {MRI, EEG, Epilepsy surgery, Neuropsychology, Outcome, Psychiatry},
	pages = {230--232},
}

@article{david_imaging_2011,
	title = {Imaging the seizure onset zone with stereo-electroencephalography},
	volume = {134},
	issn = {0006-8950},
	url = {https://academic.oup.com/brain/article/134/10/2898/323878},
	doi = {10.1093/brain/awr238},
	abstract = {Abstract. Stereo-electroencephalography is used to localize the seizure onset zone and connected neuronal networks in surgical candidates suffering from intrac},
	language = {en},
	number = {10},
	urldate = {2018-07-04},
	journal = {Brain},
	author = {David, Olivier and Blauwblomme, Thomas and Job, Anne-Sophie and Chabard{\`e}s, St{\'e}phan and Hoffmann, Dominique and Minotti, Lorella and Kahane, Philippe},
	month = oct,
	year = {2011},
	pages = {2898--2911},
}

@article{nowell_resection_2017,
	title = {Resection planning in extratemporal epilepsy surgery using {3D} multimodality imaging and intraoperative {MRI}},
	volume = {31},
	issn = {0268-8697},
	url = {https://doi.org/10.1080/02688697.2016.1265086},
	doi = {10.1080/02688697.2016.1265086},
	abstract = {Surgical resection in non-lesional, extratemporal epilepsy, informed by stereoEEG recordings, is challenging. There are no clear borders of resection, and the surgeon is often operating in deep areas of the brain that are difficult to access. We present a technical note where 3D multimodality image integration in EpiNavTM is used to build a planned resection model, based on a previous intracranial EEG evaluation. Intraoperative MRI is then used to ensure a complete resection of the planned model. As stereoEEG becomes more common in the presurgical evaluation of epilepsy, these tools will become increasingly important to facilitate targeted cortical resections.},
	number = {4},
	urldate = {2018-07-04},
	journal = {British Journal of Neurosurgery},
	author = {Nowell, Mark and Sparks, Rachel and Zombori, Gergely and Miserocchi, Anna and Rodionov, Roman and Diehl, Beate and Wehner, Tim and White, Mark and Ourselin, Sebastien and McEvoy, Andrew and Duncan, John},
	month = jul,
	year = {2017},
	pmid = {27931117},
	keywords = {functional neurosurgery, Image-guided surgery, neuronavigation, operation},
	pages = {468--470},
}

@article{iida_stereoelectroencephalography_2017,
	title = {Stereoelectroencephalography: {Indication} and {Efficacy}},
	volume = {57},
	issn = {0470-8105},
	shorttitle = {Stereoelectroencephalography},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5566696/},
	doi = {10.2176/nmc.ra.2017-0008},
	abstract = {Stereoelectroencephalography (SEEG) is a method for invasive study of patients with refractory epilepsy. Localization of the epileptogenic zone in SEEG relied on the hypothesis of anatomo-electro-clinical analysis limited by X-ray, analog electroencephalography (EEG), and seizure semiology in the 1950s. Modern neuroimaging studies and digital video-EEG have developed the hypothesis aiming at more precise localization of the epileptic network. Certain clinical scenarios favor SEEG over subdural EEG (SDEEG). SEEG can cover extensive areas of bilateral hemispheres with highly accurate sampling from sulcal areas and deep brain structures. A hybrid technique of SEEG and subdural strip electrode placement has been reported to overcome the SEEG limitations of poor functional mapping. Technological advances including acquisition of three-dimensional angiography and magnetic resonance image (MRI) in frameless conditions, advanced multimodal planning, and robot-assisted implantation have contributed to the accuracy and safety of electrode implantation in a simplified fashion. A recent meta-analysis of the safety of SEEG concluded the low value of the pooled prevalence for all complications. The complications of SEEG were significantly less than those of SDEEG. The removal of electrodes for SEEG was much simpler than for SDEEG and allowed sufficient time for data analysis, discussion, and consensus for both patients and physicians before the proceeding treatment. Furthermore, SEEG is applicable as a therapeutic alternative for deep-seated lesions, e.g., nodular heterotopia, in nonoperative epilepsies using SEEG-guided radiofrequency thermocoagulation. We review the SEEG method with technological advances for planning and implantation of electrodes. We highlight the indication and efficacy, advantages and disadvantages of SEEG compared with SDEEG.},
	number = {8},
	urldate = {2018-07-04},
	journal = {Neurologia medico-chirurgica},
	author = {Iida, Koji and Otsubo, Hiroshi},
	month = aug,
	year = {2017},
	pmid = {28637943},
	pmcid = {PMC5566696},
	pages = {375--385},
}

@article{bien_characteristics_2009,
	title = {Characteristics and {Surgical} {Outcomes} of {Patients} {With} {Refractory} {Magnetic} {Resonance} {Imaging}{\textendash}{Negative} {Epilepsies}},
	volume = {66},
	issn = {0003-9942},
	url = {https://jamanetwork.com/journals/jamaneurology/fullarticle/798793},
	doi = {10.1001/archneurol.2009.283},
	abstract = {{\textbackslash}textlessh3{\textbackslash}textgreaterObjective{\textbackslash}textless/h3{\textbackslash}textgreater{\textbackslash}textlessp{\textbackslash}textgreaterTo explore several characteristics of patients with pharmacoresistant epilepsy without distinct lesions on magnetic resonance images (MRI$^{\textrm{-}}$\$), who account for a relevant proportion of presurgical patient cohorts.{\textbackslash}textless/p{\textbackslash}textgreater{\textbackslash}textlessh3{\textbackslash}textgreaterDesign{\textbackslash}textless/h3{\textbackslash}textgreater{\textbackslash}textlessp{\textbackslash}textgreaterRetrospective case series.{\textbackslash}textless/p{\textbackslash}textgreater{\textbackslash}textlessh3{\textbackslash}textgreaterSetting{\textbackslash}textless/h3{\textbackslash}textgreater{\textbackslash}textlessp{\textbackslash}textgreaterUniversity epilepsy center.{\textbackslash}textless/p{\textbackslash}textgreater{\textbackslash}textlessh3{\textbackslash}textgreaterPatients{\textbackslash}textless/h3{\textbackslash}textgreater{\textbackslash}textlessp{\textbackslash}textgreaterA cohort of 1200 patients who had comprehensive presurgical assessment from January 1, 2000, through December 31, 2006.{\textbackslash}textless/p{\textbackslash}textgreater{\textbackslash}textlessh3{\textbackslash}textgreaterMain Outcome Measures{\textbackslash}textless/h3{\textbackslash}textgreater{\textbackslash}textlessp{\textbackslash}textgreaterFrequency of MRI$^{\textrm{-}}$$^{\textrm{-}}$$^{\textrm{-}}$\$patients were retrospectively analyzed. Presurgical MRIs were reevaluated for subtle cortical dysplasias by postprocessing and visual reassessment.{\textbackslash}textless/p{\textbackslash}textgreater{\textbackslash}textlessh3{\textbackslash}textgreaterResults{\textbackslash}textless/h3{\textbackslash}textgreater{\textbackslash}textlessp{\textbackslash}textgreaterOne-hundred ninety MRI$^{\textrm{-}}$$^{\textrm{+}}$$^{\textrm{-}}$$^{\textrm{-}}$$^{\textrm{-}}$$^{\textrm{-}}$\$but histopathologically lesional patients became seizure free compared with only 4 of 20 patients without histopathological lesions (\textit{P} = .003). Three-fifths of the histopathologically nonlesional patients had multifocal or extensive epileptogenic areas.{\textbackslash}textless/p{\textbackslash}textgreater{\textbackslash}textlessh3{\textbackslash}textgreaterConclusions{\textbackslash}textless/h3{\textbackslash}textgreater{\textbackslash}textlessp{\textbackslash}textgreaterPatients with epilepsy who are MRI$^{\textrm{-}}$\$can be successfully treated with surgery. Improved sensitivity of MRI will improve the outcomes of presurgically studied patients. Surgical failures in patients without histopathological lesions mostly result from extensive epileptogenic areas.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {en},
	number = {12},
	urldate = {2018-07-04},
	journal = {Archives of Neurology},
	author = {Bien, Christian G. and Szinay, Miriam and Wagner, Jan and Clusmann, Hans and Becker, Albert J. and Urbach, Horst},
	month = dec,
	year = {2009},
	pages = {1491--1499},
}

@article{kovac_invasive_2017,
	title = {Invasive epilepsy surgery evaluation},
	volume = {44},
	issn = {1532-2688},
	doi = {10.1016/j.seizure.2016.10.016},
	abstract = {Intracranial EEG (iEEG) recordings are widely used for the work up of pharmacoresistant epilepsy. Different iEEG recording techniques namely subdural grids, strips, depth electrodes and stereoencephalography (SEEG) are available with distinct limitations and advantages. Epilepsy centres mastering multiple techniques apply them in an individualised patient approach. These tools are used to map the seizure onset zone which is pivotal in approximating the epileptogenic zone, i.e. the zone which is indispensable for the generation of seizures and when resected will render the patient seizure free. Besides, the implanted electrodes can be used to define eloquent cortex through direct cortical stimulation. Different clinical scenarios exist which favour one iEEG recording technique over the other. Proximity of the presumed epileptogenic zone to eloquent cortex, for example, is a clinical scenario which may favour grid electrodes over SEEG. We here review the indication for iEEG for the work-up of patients suffering from pharmacoresistant epilepsy. In addition, we provide a description of the recording techniques focussing on the main techniques used: grid electrodes, depth electrodes and stereoencephalography. We then outline different clinical scenarios and the preferred technical approach for intracranial recordings in these scenarios. Finally, we highlight which advances have been made in the field of iEEG and which advances are in the pipeline waiting to be established for clinical use. This review provides the clinician with an update on the diagnostic use of intracranial EEG for epilepsy surgery and thus aids in understanding patient selection for this technique which may ultimately improve referral patterns.},
	language = {eng},
	journal = {Seizure},
	author = {Kovac, Stjepana and Vakharia, Vejay N. and Scott, Catherine and Diehl, Beate},
	month = jan,
	year = {2017},
	pmid = {27816354},
	keywords = {Humans, Epilepsy, Electroencephalography, Seizure, Stereoelectroencephalography, Neurosurgical Procedures, Stereotaxic Techniques, Cortical stimulation, Neurologic Examination, Pharmacoresistant epilepsy, Subdural EEG},
	pages = {125--136},
}

@article{de_almeida_efficacy_2006,
	title = {Efficacy of and morbidity associated with stereoelectroencephalography using computerized tomography{\textendash} or magnetic resonance imaging{\textendash}guided electrode implantation},
	volume = {104},
	issn = {0022-3085},
	url = {http://thejns.org/doi/abs/10.3171/jns.2006.104.4.483},
	doi = {10.3171/jns.2006.104.4.483},
	number = {4},
	urldate = {2018-07-04},
	journal = {Journal of Neurosurgery},
	author = {de Almeida, Antonio Nogueira and Olivier, Andr{\'e} and Quesney, Felipe and Dubeau, Fran{\c c}ois and Savard, Ghislaine and Andermann, Frederick},
	month = apr,
	year = {2006},
	pages = {483--487},
}

@article{alomar_stereo-electroencephalography_2016,
	series = {Epilepsy},
	title = {The {Stereo}-{Electroencephalography} {Methodology}},
	volume = {27},
	issn = {1042-3680},
	url = {http://www.sciencedirect.com/science/article/pii/S1042368015000881},
	doi = {10.1016/j.nec.2015.08.003},
	number = {1},
	urldate = {2018-07-04},
	journal = {Neurosurgery Clinics of North America},
	author = {Alomar, Soha and Jones, Jaes and Maldonado, Andres and Gonzalez-Martinez, Jorge},
	month = jan,
	year = {2016},
	keywords = {Epilepsy surgery, Morbidity, Seizure outcome, Stereo-electroencephalography, Stereotaxy},
	pages = {83--95},
}

@article{jayakar_diagnostic_2016,
	title = {Diagnostic utility of invasive {EEG} for epilepsy surgery: {Indications}, modalities, and techniques},
	volume = {57},
	copyright = {Wiley Periodicals, Inc. {\textcopyright} 2016 International League Against Epilepsy},
	issn = {1528-1167},
	shorttitle = {Diagnostic utility of invasive {EEG} for epilepsy surgery},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/epi.13515},
	doi = {10.1111/epi.13515},
	abstract = {Many patients with medically refractory epilepsy now undergo successful surgery based on noninvasive diagnostic information, but intracranial electroencephalography (IEEG) continues to be used as increasingly complex cases are considered surgical candidates. The indications for IEEG and the modalities employed vary across epilepsy surgical centers; each modality has its advantages and limitations. IEEG can be performed in the same intraoperative setting, that is, intraoperative electrocorticography, or through an independent implantation procedure with chronic extraoperative recordings; the latter are not only resource intensive but also carry risk. A lack of understanding of IEEG limitations predisposes to data misinterpretation that can lead to denying surgery when indicated or, worse yet, incorrect resection with adverse outcomes. Given the lack of class 1 or 2 evidence on IEEG, a consensus-based expert recommendation on the diagnostic utility of IEEG is presented, with emphasis on the application of various modalities in specific substrates or locations, taking into account their relative efficacy, safety, ease, and incremental cost-benefit. These recommendations aim to curtail outlying indications that risk the over- or underutilization of IEEG, while retaining substantial flexibility in keeping with most standard practices at epilepsy centers and addressing some of the needs of resource-poor regions around the world.},
	language = {en},
	number = {11},
	urldate = {2018-07-04},
	journal = {Epilepsia},
	author = {Jayakar, Prasanna and Gotman, Jean and Harvey, A. Simon and Palmini, Andr{\'e} and Tassi, Laura and Schomer, Donald and Dubeau, Francois and Bartolomei, Fabrice and Yu, Alice and Kr{\v s}ek, Pavel and Velis, Demetrios and Kahane, Philippe},
	year = {2016},
	keywords = {Epilepsy surgery, Indications, Intracranial EEG, Utility},
	pages = {1735--1747},
}

@article{cossu_stereoelectroencephalography_2005,
	title = {Stereoelectroencephalography in the presurgical evaluation of children with drug-resistant focal epilepsy},
	volume = {103},
	issn = {1933-0707},
	url = {http://thejns.org/doi/abs/10.3171/ped.2005.103.4.0333},
	doi = {10.3171/ped.2005.103.4.0333},
	number = {4},
	urldate = {2018-07-04},
	journal = {Journal of Neurosurgery: Pediatrics},
	author = {Cossu, Massimo and Cardinale, Francesco and Colombo, Nadia and Mai, Roberto and Nobili, Lino and Sartori, Ivana and Lo Russo, Giorgio},
	month = oct,
	year = {2005},
	pages = {333--343},
}

@article{mathon_safety_2015,
	title = {Safety profile of intracranial electrode implantation for video-{EEG} recordings in drug-resistant focal epilepsy},
	volume = {262},
	issn = {1432-1459},
	doi = {10.1007/s00415-015-7901-6},
	abstract = {Invasive electroencephalography recordings with depth or subdural electrodes are necessary to identify the ictogenic area in some drug-resistant focal epilepsies. We aimed to analyze the safety profile of intracranial electrode implantation in a tertiary center and the factors associated with its complications. We retrospectively examined complications in 163 intracranial procedures performed in adult patients. Implantation methods included oblique depth stereotactic approach (n = 128) and medial-temporal depth stereotactic approach in combination with subdural strip placement (n = 35). 1201 depth macroelectrodes, 59 bundles of microelectrodes (in 30 patients) and 148 subdural electrodes were implanted. Complications were classified as major (requiring treatment or leading to neurological impairment) or minor. The rate of overall complications was 4.9\% (n = 8), with 3.1\% (n = 5) of major complications, though no permanent morbidity or mortality was recorded. Infection occurred in 1.2\% and hemorrhage in 3.7\% of patients. One hemorrhage occurred for every 225 electrodes implanted (4.4{\textperthousand}). Microelectrodes were not responsible for any complications. Overall and hemorrhagic complications were significantly associated with MRI-negative cases (7.3 and 6.3\% versus 0\%, p = 0.04). We believe that intracranial electrode implantation has a favorable safety profile, without permanent deficit. These risks should be balanced with the benefits of invasive exploration prior to surgery. Furthermore, this study provides preliminary evidence regarding the safety of micro-macroelectrodes.},
	language = {eng},
	number = {12},
	journal = {Journal of Neurology},
	author = {Mathon, Bertrand and Clemenceau, St{\'e}phane and Hasboun, Dominique and Habert, Marie-Odile and Belaid, Hayat and Nguyen-Michel, Vi-Huong and Lambrecq, Virginie and Navarro, Vincent and Dupont, Sophie and Baulac, Michel and Cornu, Philippe and Adam, Claude},
	month = dec,
	year = {2015},
	pmid = {26410749},
	keywords = {Humans, Adolescent, Adult, Female, Male, Middle Aged, Young Adult, Electroencephalography, Neurosurgical Procedures, Electrodes, Drug Resistant Epilepsy, Epilepsy surgery, Implanted, Adverse events, Complications, Depth electrodes, Epilepsies, Invasive EEG, Partial, Postoperative Complications, SEEG (Stereoelectroencephalography)},
	pages = {2699--2712},
}

@article{sparks_automated_2017,
	title = {Automated multiple trajectory planning algorithm for the placement of stereo-electroencephalography ({SEEG}) electrodes in epilepsy treatment},
	volume = {12},
	issn = {1861-6410, 1861-6429},
	url = {https://link.springer.com/article/10.1007/s11548-016-1452-x},
	doi = {10.1007/s11548-016-1452-x},
	abstract = {PurposeAbout one-third of individuals with focal epilepsy continue to have seizures despite optimal medical management. These patients are potentially curable with neurosurgery if the epileptogenic zone (EZ) can be identified and resected. Stereo-electroencephalography (SEEG) to record epileptic activity with intracranial depth electrodes may be required to identify the EZ. Each SEEG electrode trajectory, the path between the entry on the skull and the cerebral target, must be planned carefully to avoid trauma to blood vessels and conflicts between electrodes. In current clinical practice trajectories are determined manually, typically taking 2{\textendash}3 h per patient (15 min per electrode). Manual planning (MP) aims to achieve an implantation plan with good coverage of the putative EZ, an optimal spatial resolution, and 3D distribution of electrodes. Computer-assisted planning tools can reduce planning time by quantifying trajectory suitability.MethodsWe present an automated multiple trajectory planning (MTP) algorithm to compute implantation plans. MTP uses dynamic programming to determine a set of plans. From this set a depth-first search algorithm finds a suitable plan. We compared our MTP algorithm to (a) MP and (b) an automated single trajectory planning (STP) algorithm on 18 patient plans containing 165 electrodes.ResultsMTP changed all 165 trajectories compared to MP. Changes resulted in lower risk (122), increased grey matter sampling (99), shorter length (92), and surgically preferred entry angles (113). MTP changed 42 \% (69/165) trajectories compared to STP. Every plan had between 1 to 8 (median 3.5) trajectories changed to resolve electrode conflicts, resulting in surgically preferred plans.ConclusionMTP is computationally efficient, determining implantation plans containing 7{\textendash}12 electrodes within 1 min, compared to 2{\textendash}3 h for MP.},
	language = {en},
	number = {1},
	urldate = {2018-07-04},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Sparks, Rachel and Zombori, Gergely and Rodionov, Roman and Nowell, Mark and Vos, Sjoerd B. and Zuluaga, Maria A. and Diehl, Beate and Wehner, Tim and Miserocchi, Anna and McEvoy, Andrew W. and Duncan, John S. and Ourselin, Sebastien},
	month = jan,
	year = {2017},
	pages = {123--136},
}

@book{olivier_techniques_2012,
	title = {Techniques in {Epilepsy} {Surgery}: {The} {MNI} {Approach}},
	isbn = {978-1-107-37822-3},
	shorttitle = {Techniques in {Epilepsy} {Surgery}},
	abstract = {Techniques in Epilepsy Surgery presents the operative procedures used in the treatment of intractable epilepsy in a practical, clinically relevant manner. Founded by pioneering neurosurgeon Wilder Penfield, the Montreal Neurological Institute (MNI) is a leading global centre of epilepsy surgery and this volume reflects the Institute's approach, combining traditional techniques with modern neuronavigation-based approaches. There is an emphasis on mastering the important trilogy of topographic, vascular and functional anatomy of the brain. The basic anatomical and physiological mechanisms underlying epilepsy are presented in a practical manner, along with the clinical seizure evaluation that leads to a surgical hypothesis. The consultation skills and investigations necessary for appropriate patient selection are discussed, as well as pitfalls and the avoidance of complications. This is an invaluable resource not only for neurosurgeons, neurosurgical residents and fellows in epilepsy surgery, but also for neurologists, and others who provide medical care for patients with intractable epilepsy.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Olivier, Andr{\'e} and Boling, Warren W. and Tanriverdi, Taner},
	month = feb,
	year = {2012},
	keywords = {Medical / Neurology, Medical / Surgery / General, Medical / Surgery / Neurosurgery},
}

@article{vakharia_accuracy_2017,
	title = {Accuracy of intracranial electrode placement for stereoelectroencephalography: {A} systematic review and meta-analysis},
	volume = {58},
	copyright = {Wiley Periodicals, Inc. {\textcopyright} 2017 International League Against Epilepsy},
	issn = {1528-1167},
	shorttitle = {Accuracy of intracranial electrode placement for stereoelectroencephalography},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/epi.13713},
	doi = {10.1111/epi.13713},
	abstract = {Objective Stereoelectroencephalography (SEEG) is a procedure in which electrodes are inserted into the brain to help define the epileptogenic zone. This is performed prior to definitive epilepsy surgery in patients with drug-resistant focal epilepsy when noninvasive data are inconclusive. The main risk of the procedure is hemorrhage, which occurs in 1{\textendash}2\% of patients. This may result from inaccurate electrode placement or a planned electrode damaging a blood vessel that was not detected on the preoperative vascular imaging. Proposed techniques include the use of a stereotactic frame, frameless image guidance systems, robotic guidance systems, and customized patient-specific fixtures. Methods Using the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) guidelines, a structured search of the PubMed, Embase, and Cochrane databases identified studies that involve the following: (1) SEEG placement as part of the presurgical workup in patients with (2) drug-resistant focal epilepsy for which (3) accuracy data have been provided. Results Three hundred twenty-six publications were retrieved, of which 293 were screened following removal of duplicate and non{\textendash}English-language studies. Following application of the inclusion and exclusion criteria, 15 studies were included in the qualitative and quantitative synthesis of the meta-analysis. Accuracies for SEEG electrode implantations have been combined using a random-effects analysis and stratified by technique. Significance The published literature regarding accuracy of SEEG implantation techniques is limited. There are no prospective controlled clinical trials comparing different SEEG implantation techniques. Significant systematic heterogeneity exists between the identified studies, preventing any meaningful comparison between techniques. The recent introduction of robotic trajectory guidance systems has been suggested to provide a more accurate method of implantation, but supporting evidence is limited to class 3 only. It is important that new techniques are compared to the previous {\textquotedblleft}gold-standard{\textquotedblright} through well-designed and methodologically sound studies before they are introduced into widespread clinical practice.},
	language = {en},
	number = {6},
	urldate = {2018-07-04},
	journal = {Epilepsia},
	author = {Vakharia, Vejay N. and Sparks, Rachel and O'Keeffe, Aidan G. and Rodionov, Roman and Miserocchi, Anna and McEvoy, Andrew and Ourselin, Sebastien and Duncan, John},
	year = {2017},
	keywords = {Stereoelectroencephalography, Robotics, Drug resistance, Epileptogenic zone, Stereotactic frame},
	pages = {921--932},
}

@article{kabat_focal_2012,
	title = {Focal cortical dysplasia {\textendash} review},
	volume = {77},
	issn = {1733-134X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3403799/},
	abstract = {Focal cortical dysplasia is a malformation of cortical development, which is the most common cause of medically refractory epilepsy in the pediatric population and the second/third most common etiology of medically intractable seizures in adults., Both genetic and acquired factors are involved in the pathogenesis of cortical dysplasia. Numerous classifications of the complex structural abnormalities of focal cortical dysplasia have been proposed {\textendash} from Taylor et al. in 1971 to the last modification of Palmini classification made by Blumcke in 2011. In general, three types of cortical dysplasia are recognized., Type I focal cortical dysplasia with mild symptomatic expression and late onset, is more often seen in adults, with changes present in the temporal lobe., Clinical symptoms are more severe in type II of cortical dysplasia usually seen in children. In this type, more extensive changes occur outside the temporal lobe with predilection for the frontal lobes., New type III is one of the above dysplasias with associated another principal lesion as hippocampal sclerosis, tumor, vascular malformation or acquired pathology during early life., Brain MRI imaging shows abnormalities in the majority of type II dysplasias and in only some of type I cortical dysplasias., The most common findings on MRI imaging include: focal cortical thickening or thinning, areas of focal brain atrophy, blurring of the gray-white junction, increased signal on T2- and FLAIR-weighted images in the gray and subcortical white matter often tapering toward the ventricle. On the basis of the MRI findings, it is possible to differentiate between type I and type II cortical dysplasia. A complete resection of the epileptogenic zone is required for seizure-free life. MRI imaging is very helpful to identify those patients who are likely to benefit from surgical treatment in a group of patients with drug-resistant epilepsy., However, in type I cortical dysplasia, MR imaging is often normal, and also in both types the lesion seen on MRI may be smaller than the seizure-generating region seen in the EEG. The abnormalities may also involve vital for life brain parts, where curative surgery will not be an option. Therefore, other diagnostic imaging techniques such as FDG PET, MEG, DTI and intra-cranial EEG are widely used to establish the diagnosis and to decide on management., With advances in both genetics and neuroimaging, we may develop a better understanding of patients with drug-resistant epilepsy, which will help us to provide more successful pharmacological and/or surgical treatment in the future.},
	number = {2},
	urldate = {2018-07-04},
	journal = {Polish Journal of Radiology},
	author = {Kabat, Joanna and Kr{\'o}l, Przemys{\l }aw},
	year = {2012},
	pmid = {22844307},
	pmcid = {PMC3403799},
	pages = {35--43},
}

@article{lu_digital_2012,
	title = {Digital {Subtraction} {CT} {Angiography} for {Detection} of {Intracranial} {Aneurysms}: {Comparison} with {Three}-dimensional {Digital} {Subtraction} {Angiography}},
	shorttitle = {Digital {Subtraction} {CT} {Angiography} for {Detection} of {Intracranial} {Aneurysms}},
	url = {https://pubs.rsna.org/doi/full/10.1148/radiol.11110486},
	doi = {10.1148/radiol.11110486},
	abstract = {Digital subtraction CT angiography should be the preferred noninvasive modality for the evaluation of intracranial aneurysms in patients with acute subarachnoid hemorrhage.},
	language = {en},
	urldate = {2018-07-04},
	journal = {Radiology},
	author = {Lu, Li and Zhang, Long Jiang and Poon, Colin S. and Wu, Sheng Yong and Zhou, Chang Sheng and Luo, Song and Wang, Mei and Lu, Guang Ming},
	month = feb,
	year = {2012},
}

@article{rohrer_comparison_2005,
	title = {Comparison of {Magnetic} {Properties} of {MRI} {Contrast} {Media} {Solutions} at {Different} {Magnetic} {Field} {Strengths}},
	volume = {40},
	issn = {0020-9996},
	url = {https://journals.lww.com/investigativeradiology/fulltext/2005/11000/Comparison_of_Magnetic_Properties_of_MRI_Contrast.5.aspx},
	doi = {10.1097/01.rli.0000184756.66360.d3},
	abstract = {Rationale and Objectives: To characterize and compare commercially available contrast media (CM) for magnetic resonance imaging (MRI) in terms of their relaxivity at magnetic field strengths ranging from 0.47 T to 4.7 T at physiological temperatures in water and in plasma. Relaxivities also were quantified in whole blood at1.5 T. Methods: Relaxivities of MRI-CM were determined by nuclear magnetic resonance (NMR) spectroscopy at 0.47 T and MRI phantom measurements at 1.5 T, 3 T, and 4.7 T, respectively. Both longitudinal (T1) and transverse relaxation times (T2) were measured by appropriate spin-echo sequences. Nuclear magnetic resonance dispersion (NMRD) profiles were also determined for all agents in water and in plasma. Results: Significant dependencies of relaxivities on the field strength and solvents were quantified. Protein binding leads to both increased field strength and solvent dependencies and hence to significantly altered T1 relaxivity values at higher magnetic field strengths. Conclusions: Awareness of the field strength and solvent associated with relaxivity data is crucial for the comparison and evaluation of relaxivity values. Data observed at 0.47 T can thus be misleading and should be replaced by relaxivities measured at 1.5 T and at 3 T in plasma at physiological temperature.},
	language = {en-US},
	number = {11},
	urldate = {2018-07-05},
	journal = {Investigative Radiology},
	author = {Rohrer, Martin and Bauer, Hans and Mintorovitch, Jan and Requardt, Martin and Weinmann, Hanns-Joachim},
	month = nov,
	year = {2005},
	pages = {715},
}

@article{colombo_imaging_2009,
	title = {Imaging of malformations of cortical development},
	volume = {11},
	issn = {1294-9361},
	url = {http://www.jle.com/fr/revues/epd/e-docs/imaging_of_malformations_of_cortical_development_282420/article.phtml?tab=texte},
	doi = {10.1684/epd.2009.0262},
	number = {3},
	urldate = {2018-07-04},
	journal = {Epileptic Disorders},
	author = {Colombo, Nadia and Salamon, Noriko and Raybaud, Charles and {\"O}zkara, {\c C}igdem and Barkovich, A. James},
	month = sep,
	year = {2009},
	pages = {194--205},
}

@article{thom_review_2014,
	title = {Review: {Hippocampal} sclerosis in epilepsy: a neuropathology review},
	volume = {40},
	issn = {0305-1846},
	shorttitle = {Review},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4265206/},
	doi = {10.1111/nan.12150},
	abstract = {Hippocampal sclerosis (HS) is a common pathology encountered in mesial temporal lobe epilepsy (MTLE) as well as other epilepsy syndromes and in both surgical and post-mortem practice. The 2013 International League Against Epilepsy (ILAE) classification segregates HS into typical (type 1) and atypical (type 2 and 3) groups, based on the histological patterns of subfield neuronal loss and gliosis. In addition, granule cell reorganization and alterations of interneuronal populations, neuropeptide fibre networks and mossy fibre sprouting are distinctive features of HS associated with epilepsies; they can be useful diagnostic aids to discriminate from other causes of HS, as well as highlighting potential mechanisms of hippocampal epileptogenesis. The cause of HS remains elusive and may be multifactorial; the contribution of febrile seizures, genetic susceptibility, inflammatory and neurodevelopmental factors are discussed. Post-mortem based research in HS, as an addition to studies on surgical samples, has the added advantage of enabling the study of the wider network changes associated with HS, the long-term effects of epilepsy on the pathology and associated comorbidities. It is likely that HS is heterogeneous in aspects of its cause, epileptogenetic mechanisms, network alterations and response to medical and surgical treatments. Future neuropathological studies will contribute to better recognition and understanding of these clinical and patho-aetiological subtypes of HS.},
	number = {5},
	urldate = {2018-07-04},
	journal = {Neuropathology and Applied Neurobiology},
	author = {Thom, Maria},
	month = aug,
	year = {2014},
	pmid = {24762203},
	pmcid = {PMC4265206},
	pages = {520--543},
}

@book{brown_magnetic_2014,
	title = {Magnetic {Resonance} {Imaging}: {Physical} {Principles} and {Sequence} {Design}: {Second} {Edition}},
	isbn = {978-1-118-63395-3 978-0-471-72085-0},
	shorttitle = {Magnetic {Resonance} {Imaging}},
	url = {https://cwru.pure.elsevier.com/en/publications/magnetic-resonance-imaging-physical-principles-and-sequence-desig-2},
	language = {English (US)},
	urldate = {2018-07-05},
	publisher = {Wiley Blackwell},
	author = {Brown, Robert W. and Cheng, Yu Chung N. and Haacke, E. Mark and Thompson, Michael R. and Venkatesan, Ramesh},
	month = jun,
	year = {2014},
	doi = {10.1002/9781118633953},
}

@article{fedorov_3d_2012,
	title = {{3D} {Slicer} as an {Image} {Computing} {Platform} for the {Quantitative} {Imaging} {Network}},
	volume = {30},
	issn = {0730-725X},
	doi = {10.1016/j.mri.2012.05.001},
	abstract = {Quantitative analysis has tremendous but mostly unrealized potential in healthcare to support objective and accurate interpretation of the clinical imaging. In 2008, the National Cancer Institute began building the Quantitative Imaging Network (QIN) initiative with the goal of advancing quantitative imaging in the context of personalized therapy and evaluation of treatment response. Computerized analysis is an important component contributing to reproducibility and efficiency of the quantitative imaging techniques. The success of quantitative imaging is contingent on robust analysis methods and software tools to bring these methods from bench to bedside., 3D Slicer is a free open source software application for medical image computing. As a clinical research tool, 3D Slicer is similar to a radiology workstation that supports versatile visualizations but also provides advanced functionality such as automated segmentation and registration for a variety of application domains. Unlike a typical radiology workstation, 3D Slicer is free and is not tied to specific hardware. As a programming platform, 3D Slicer facilitates translation and evaluation of the new quantitative methods by allowing the biomedical researcher to focus on the implementation of the algorithm, and providing abstractions for the common tasks of data communication, visualization and user interface development. Compared to other tools that provide aspects of this functionality, 3D Slicer is fully open source and can be readily extended and redistributed. In addition, 3D Slicer is designed to facilitate the development of new functionality in the form of 3D Slicer extensions., In this paper, we present an overview of 3D Slicer as a platform for prototyping, development and evaluation of image analysis tools for clinical research applications. To illustrate the utility of the platform in the scope of QIN, we discuss several use cases of 3D Slicer by the existing QIN teams, and we elaborate on the future directions that can further facilitate development and validation of imaging biomarkers using 3D Slicer.},
	number = {9},
	urldate = {2018-07-05},
	journal = {Magnetic resonance imaging},
	author = {Fedorov, Andriy and Beichel, Reinhard and Kalpathy-Cramer, Jayashree and Finet, Julien and Fillion-Robin, Jean-Christophe and Pujol, Sonia and Bauer, Christian and Jennings, Dominique and Fennessy, Fiona and Sonka, Milan and Buatti, John and Aylward, Stephen and Miller, James V. and Pieper, Steve and Kikinis, Ron},
	month = nov,
	year = {2012},
	pmid = {22770690},
	pmcid = {PMC3466397},
	pages = {1323--1341},
}

@article{modat_global_2014,
	title = {Global image registration using a symmetric block-matching approach},
	volume = {1},
	issn = {2329-4302},
	doi = {10.1117/1.JMI.1.2.024003},
	abstract = {Most medical image registration algorithms suffer from a directionality bias that has been shown to largely impact subsequent analyses. Several approaches have been proposed in the literature to address this bias in the context of nonlinear registration, but little work has been done for global registration. We propose a symmetric approach based on a block-matching technique and least-trimmed square regression. The proposed method is suitable for multimodal registration and is robust to outliers in the input images. The symmetric framework is compared with the original asymmetric block-matching technique and is shown to outperform it in terms of accuracy and robustness. The methodology presented in this article has been made available to the community as part of the NiftyReg open-source package.},
	number = {2},
	urldate = {2018-07-05},
	journal = {Journal of Medical Imaging},
	author = {Modat, Marc and Cash, David M. and Daga, Pankaj and Winston, Gavin P. and Duncan, John S. and Ourselin, S{\'e}bastien},
	month = jul,
	year = {2014},
	pmid = {26158035},
	pmcid = {PMC4478989},
}

@misc{noauthor_overview_nodate,
	title = {Overview {\textbar} {Epilepsies}: diagnosis and management {\textbar} {Guidance} {\textbar} {NICE}},
	shorttitle = {Overview {\textbar} {Epilepsies}},
	url = {https://www.nice.org.uk/guidance/cg137},
	language = {eng},
	urldate = {2020-01-11},
	file = {Snapshot:/home/fernando/Zotero/storage/FH5AUF4A/cg137.html:text/html},
}

@incollection{neligan_chapter_2012,
	series = {Epilepsy},
	title = {Chapter 6 - {The} epidemiology of the epilepsies},
	volume = {107},
	url = {http://www.sciencedirect.com/science/article/pii/B9780444528988000069},
	abstract = {This chapter provides an overview of some aspects of the epidemiology of epilepsy, in particular the incidence and prevalence, prognosis, and mortality. There are a number of methodological difficulties when attempting to apply standard epidemiological processes to a symptom complex such as epilepsy, and these are touched upon. Incidence and prevalence studies in developed and resource-poor settings are reviewed and the changing profile of age-specific incidence of epilepsy is discussed. The prognosis in relation to probabilities of seizure recurrence after single and recurrent seizures, the risk of relapse after antiepileptic drug withdrawal and of the untreated condition are covered. Outcome of epilepsy surgery is reviewed. The data on the largely neglected area of prognosis in treatment-resistant epilepsy are summarized. Evidence for the risk of premature death in people following a first seizure and in people with epilepsy in population-based incident cohorts is reviewed with a discussion of causation and the relative frequency of epilepsy-related and nonepilepsy-related deaths},
	language = {en},
	urldate = {2020-01-11},
	booktitle = {Handbook of {Clinical} {Neurology}},
	publisher = {Elsevier},
	author = {Neligan, Aidan and Hauser, Willard A. and Sander, Josemir W.},
	editor = {Stefan, Hermann and Theodore, William H.},
	month = jan,
	year = {2012},
	doi = {10.1016/B978-0-444-52898-8.00006-9},
	pages = {113--133},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/J5UJUJ3Y/B9780444528988000069.html:text/html},
}

@article{fisher_ilae_2014,
	title = {{ILAE} official report: a practical clinical definition of epilepsy},
	volume = {55},
	issn = {1528-1167},
	shorttitle = {{ILAE} official report},
	doi = {10.1111/epi.12550},
	abstract = {Epilepsy was defined conceptually in 2005 as a disorder of the brain characterized by an enduring predisposition to generate epileptic seizures. This definition is usually practically applied as having two unprovoked seizures {\textgreater}24~h apart. The International League Against Epilepsy (ILAE) accepted recommendations of a task force altering the practical definition for special circumstances that do not meet the two unprovoked seizures criteria. The task force proposed that epilepsy be considered to be a disease of the brain defined by any of the following conditions: (1) At least two unprovoked (or reflex) seizures occurring {\textgreater}24~h apart; (2) one unprovoked (or reflex) seizure and a probability of further seizures similar to the general recurrence risk (at least 60\%) after two unprovoked seizures, occurring over the next 10~years; (3) diagnosis of an epilepsy syndrome. Epilepsy is considered to be resolved for individuals who either had an age-dependent epilepsy syndrome but are now past the applicable age or who have remained seizure-free for the last 10~years and off antiseizure medicines for at least the last 5~years. "Resolved" is not necessarily identical to the conventional view of "remission or "cure." Different practical definitions may be formed and used for various specific purposes. This revised definition of epilepsy brings the term in concordance with common use. A PowerPoint slide summarizing this article is available for download in the Supporting Information section here.},
	language = {eng},
	number = {4},
	journal = {Epilepsia},
	author = {Fisher, Robert S. and Acevedo, Carlos and Arzimanoglou, Alexis and Bogacz, Alicia and Cross, J. Helen and Elger, Christian E. and Engel, Jerome and Forsgren, Lars and French, Jacqueline A. and Glynn, Mike and Hesdorffer, Dale C. and Lee, B. I. and Mathern, Gary W. and Mosh{\'e}, Solomon L. and Perucca, Emilio and Scheffer, Ingrid E. and Tomson, Torbj{\"o}rn and Watanabe, Masako and Wiebe, Samuel},
	month = apr,
	year = {2014},
	pmid = {24730690},
	keywords = {Brain, Humans, Adult, Aged, Aged, 80 and over, Female, Male, Young Adult, Epilepsy, Seizures, Seizure, Recurrence, Child, Societies, Medical, Advisory Committees, Definition, Research Report, Unprovoked},
	pages = {475--482},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@article{greff_sacred_2017,
	title = {The {Sacred} {Infrastructure} for {Computational} {Research}},
	doi = {10.25080/shinma-7f4c6e7-008},
	urldate = {2021-01-08},
	journal = {Proceedings of the 16th Python in Science Conference},
	author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and Schmidhuber, J{\"u}rgen},
	year = {2017},
	pages = {49--56},
	file = {Snapshot:/home/fernando/Zotero/storage/SPBSFZW9/klaus_greff.html:text/html},
}

@article{abadi_tensorflow_2016,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	shorttitle = {{TensorFlow}},
	abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	urldate = {2021-01-08},
	journal = {arXiv:1603.04467 [cs]},
	author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04467},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/H3NEACM3/1603.html:text/html},
}

@article{ermis_fully_2020,
	title = {Fully automated brain resection cavity delineation for radiation target volume definition in glioblastoma patients using deep learning},
	volume = {15},
	issn = {1748-717X},
	doi = {10.1186/s13014-020-01553-z},
	language = {eng},
	number = {1},
	journal = {Radiation Oncology},
	author = {Ermi{\c s}, Ekin and Jungo, Alain and Poel, Robert and Blatti-Moreno, Marcela and Meier, Raphael and Knecht, Urspeter and Aebersold, Daniel M. and Fix, Michael K. and Manser, Peter and Reyes, Mauricio and Herrmann, Evelyn},
	month = may,
	year = {2020},
	pmid = {32375839},
	pmcid = {PMC7204033},
	keywords = {Deep learning, MRI, Glioblastoma, Automatic segmentation, Target definition},
	pages = {100},
}

@article{galovic_association_2019,
	title = {Association of {Piriform} {Cortex} {Resection} {With} {Surgical} {Outcomes} in {Patients} {With} {Temporal} {Lobe} {Epilepsy}},
	volume = {76},
	issn = {2168-6157},
	doi = {10.1001/jamaneurol.2019.0204},
	language = {eng},
	number = {6},
	journal = {JAMA neurology},
	author = {Galovic, Marian and Baudracco, Irene and Wright-Goff, Evan and Pillajo, Galo and Nachev, Parashkev and Wandschneider, Britta and Woermann, Friedrich and Thompson, Pamela and Baxendale, Sallie and McEvoy, Andrew W. and Nowell, Mark and Mancini, Matteo and Vos, Sjoerd B. and Winston, Gavin P. and Sparks, Rachel and Prados, Ferran and Miserocchi, Anna and de Tisi, Jane and Van Graan, Louis Andr{\'e} and Rodionov, Roman and Wu, Chengyuan and Alizadeh, Mahdi and Kozlowski, Lauren and Sharan, Ashwini D. and Kini, Lohith G. and Davis, Kathryn A. and Litt, Brian and Ourselin, Sebastien and Mosh{\'e}, Solomon L. and Sander, Josemir W. A. and L{\"o}scher, Wolfgang and Duncan, John S. and Koepp, Matthias J.},
	month = jun,
	year = {2019},
	pmid = {30855662},
	pmcid = {PMC6490233},
	keywords = {Humans, Magnetic Resonance Imaging, Reproducibility of Results, Adult, Female, Male, Middle Aged, Case-Control Studies, Cohort Studies, Prospective Studies, Epilepsy, Temporal Lobe, Treatment Outcome, Neurosurgical Procedures, Organ Size, Drug Resistant Epilepsy, Gray Matter, Piriform Cortex, Proof of Concept Study},
	pages = {690--700},
}

@inproceedings{matzkin_self-supervised_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Self-supervised {Skull} {Reconstruction} in {Brain} {CT} {Images} with {Decompressive} {Craniectomy}},
	isbn = {978-3-030-59713-9},
	doi = {10.1007/978-3-030-59713-9_38},
	abstract = {Decompressive craniectomy (DC) is a common surgical procedure consisting of the removal of a portion of the skull that is performed after incidents such as stroke, traumatic brain injury (TBI) or other events that could result in acute subdural hemorrhage and/or increasing intracranial pressure. In these cases, CT scans are obtained to diagnose and assess injuries, or guide a certain therapy and intervention. We propose a deep learning based method to reconstruct the skull defect removed during DC performed after TBI from post-operative CT images. This reconstruction is useful in multiple scenarios, e.g. to support the creation of cranioplasty plates, accurate measurements of bone flap volume and total intracranial volume, important for studies that aim to relate later atrophy to patient outcome. We propose and compare alternative self-supervised methods where an encoder-decoder convolutional neural network (CNN) estimates the missing bone flap on post-operative CTs. The self-supervised learning strategy only requires images with complete skulls and avoids the need for annotated DC images. For evaluation, we employ real and simulated images with DC, comparing the results with other state-of-the-art approaches. The experiments show that the proposed model outperforms current manual methods, enabling reconstruction even in highly challenging cases where big skull defects have been removed during surgery.},
	language = {en},
	booktitle = {{MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Matzkin, Franco and Newcombe, Virginia and Stevenson, Susan and Khetani, Aneesh and Newman, Tom and Digby, Richard and Stevens, Andrew and Glocker, Ben and Ferrante, Enzo},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {Self-supervised learning, Decompressive craniectomy, Skull reconstruction},
	pages = {390--399},
}

@article{ashburner_unified_2005,
	title = {Unified segmentation},
	volume = {26},
	issn = {1053-8119},
	doi = {10.1016/j.neuroimage.2005.02.018},
	abstract = {A probabilistic framework is presented that enables image registration, tissue classification, and bias correction to be combined within the same generative model. A derivation of a log-likelihood objective function for the unified model is provided. The model is based on a mixture of Gaussians and is extended to incorporate a smooth intensity variation and nonlinear registration with tissue probability maps. A strategy for optimising the model parameters is described, along with the requisite partial derivatives of the objective function.},
	language = {eng},
	number = {3},
	journal = {NeuroImage},
	author = {Ashburner, John and Friston, Karl J.},
	month = jul,
	year = {2005},
	pmid = {15955494},
	keywords = {Algorithms, Image Processing, Computer-Assisted, Magnetic Resonance Imaging, Likelihood Functions, Models, Neurological, Brain Mapping, Data Interpretation, Statistical, Fuzzy Logic, Models, Statistical, Nonlinear Dynamics, Normal Distribution, Probability Theory},
	pages = {839--851},
}

@book{schroeder_visualization_2006-1,
	title = {The {Visualization} {Toolkit}},
	publisher = {Kitware},
	author = {Schroeder, Will and Martin, Ken and Lorensen, Bill},
	year = {2006},
}

@article{jack_alzheimers_2008,
	title = {The {Alzheimer}'s {Disease} {Neuroimaging} {Initiative} ({ADNI}): {MRI} methods},
	volume = {27},
	issn = {1053-1807},
	shorttitle = {The {Alzheimer}'s {Disease} {Neuroimaging} {Initiative} ({ADNI})},
	doi = {10.1002/jmri.21049},
	abstract = {The Alzheimer's Disease Neuroimaging Initiative (ADNI) is a longitudinal multisite observational study of healthy elders, mild cognitive impairment (MCI), and Alzheimer's disease. Magnetic resonance imaging (MRI), (18F)-fluorodeoxyglucose positron emission tomography (FDG PET), urine serum, and cerebrospinal fluid (CSF) biomarkers, as well as clinical/psychometric assessments are acquired at multiple time points. All data will be cross-linked and made available to the general scientific community. The purpose of this report is to describe the MRI methods employed in ADNI. The ADNI MRI core established specifications that guided protocol development. A major effort was devoted to evaluating 3D T(1)-weighted sequences for morphometric analyses. Several options for this sequence were optimized for the relevant manufacturer platforms and then compared in a reduced-scale clinical trial. The protocol selected for the ADNI study includes: back-to-back 3D magnetization prepared rapid gradient echo (MP-RAGE) scans; B(1)-calibration scans when applicable; and an axial proton density-T(2) dual contrast (i.e., echo) fast spin echo/turbo spin echo (FSE/TSE) for pathology detection. ADNI MRI methods seek to maximize scientific utility while minimizing the burden placed on participants. The approach taken in ADNI to standardization across sites and platforms of the MRI protocol, postacquisition corrections, and phantom-based monitoring of all scanners could be used as a model for other multisite trials.},
	language = {eng},
	number = {4},
	journal = {Journal of magnetic resonance imaging: JMRI},
	author = {Jack, Clifford R. and Bernstein, Matt A. and Fox, Nick C. and Thompson, Paul and Alexander, Gene and Harvey, Danielle and Borowski, Bret and Britson, Paula J. and L Whitwell, Jennifer and Ward, Chadwick and Dale, Anders M. and Felmlee, Joel P. and Gunter, Jeffrey L. and Hill, Derek L. G. and Killiany, Ron and Schuff, Norbert and Fox-Bosetti, Sabrina and Lin, Chen and Studholme, Colin and DeCarli, Charles S. and Krueger, Gunnar and Ward, Heidi A. and Metzger, Gregory J. and Scott, Katherine T. and Mallozzi, Richard and Blezek, Daniel and Levy, Joshua and Debbins, Josef P. and Fleisher, Adam S. and Albert, Marilyn and Green, Robert and Bartzokis, George and Glover, Gary and Mugler, John and Weiner, Michael W.},
	month = apr,
	year = {2008},
	pmid = {18302232},
	pmcid = {PMC2544629},
	keywords = {Brain, Humans, Magnetic Resonance Imaging, Aged, Alzheimer Disease},
	pages = {685--691},
}

@article{lamontagne_oasis-3_2019,
	title = {{OASIS}-3: {Longitudinal} {Neuroimaging}, {Clinical}, and {Cognitive} {Dataset} for {Normal} {Aging} and {Alzheimer} {Disease}},
	copyright = {{\textcopyright} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	issn = {1901-4902},
	shorttitle = {{OASIS}-3},
	doi = {10.1101/2019.12.13.19014902},
	abstract = {{\textless}h3{\textgreater}ABSTRACT{\textless}/h3{\textgreater} {\textless}p{\textgreater}OASIS-3 is a compilation of MRI and PET imaging and related clinical data for 1098 participants who were collected across several ongoing studies in the Washington University Knight Alzheimer Disease Research Center over the course of 15 years. Participants include 605 cognitively normal adults and 493 individuals at various stages of cognitive decline ranging in age from 42 to 95 years. The OASIS-3 dataset contains over 2000 MR sessions, including multiple structural and functional sequences. PET metabolic and amyloid imaging includes over 1500 raw imaging scans and the accompanying post-processed files from the PET Unified Pipeline (PUP) are also available in OASIS-3. OASIS-3 also contains post-processed imaging data such as volumetric segmentations and PET analyses. Imaging data is accompanied by dementia and APOE status and longitudinal clinical and cognitive outcomes. OASIS-3 is available as an open access data set to the scientific community to answer questions related to healthy aging and dementia.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2021-01-04},
	journal = {medRxiv},
	author = {LaMontagne, Pamela J. and Benzinger, Tammie LS and Morris, John C. and Keefe, Sarah and Hornbeck, Russ and Xiong, Chengjie and Grant, Elizabeth and Hassenstab, Jason and Moulder, Krista and Vlassenko, Andrei G. and Raichle, Marcus E. and Cruchaga, Carlos and Marcus, Daniel},
	month = dec,
	year = {2019},
	note = {Publisher: Cold Spring Harbor Laboratory Press},
	pages = {2019.12.13.19014902},
	file = {Snapshot:/home/fernando/Zotero/storage/T4MDZXCT/2019.12.13.html:text/html},
}

@article{kendall_what_2017,
	title = {What {Uncertainties} {Do} {We} {Need} in {Bayesian} {Deep} {Learning} for {Computer} {Vision}?},
	abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
	urldate = {2021-01-03},
	journal = {arXiv:1703.04977 [cs]},
	author = {Kendall, Alex and Gal, Yarin},
	month = oct,
	year = {2017},
	note = {arXiv: 1703.04977},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/WKVBG3DU/1703.html:text/html},
}

@article{wang_aleatoric_2019-1,
	title = {Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks},
	volume = {338},
	issn = {0925-2312},
	doi = {10.1016/j.neucom.2019.01.103},
	abstract = {Despite the state-of-the-art performance for medical image segmentation, deep convolutional neural networks (CNNs) have rarely provided uncertainty estimations regarding their segmentation outputs, e.g., model (epistemic) and image-based (aleatoric) uncertainties. In this work, we analyze these different types of uncertainties for CNN-based 2D and 3D medical image segmentation tasks at both pixel level and structure level. We additionally propose a test-time augmentation-based aleatoric uncertainty to analyze the effect of different transformations of the input image on the segmentation output. Test-time augmentation has been previously used to improve segmentation accuracy, yet not been formulated in a consistent mathematical framework. Hence, we also propose a theoretical formulation of test-time augmentation, where a distribution of the prediction is estimated by Monte Carlo simulation with prior distributions of parameters in an image acquisition model that involves image transformations and noise. We compare and combine our proposed aleatoric uncertainty with model uncertainty. Experiments with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic Resonance Images (MRI) showed that 1) the test-time augmentation-based aleatoric uncertainty provides a better uncertainty estimation than calculating the test-time dropout-based model uncertainty alone and helps to reduce overconfident incorrect predictions, and 2) our test-time augmentation outperforms a single-prediction baseline and dropout-based multiple predictions.},
	language = {en},
	urldate = {2021-01-03},
	journal = {Neurocomputing},
	author = {Wang, Guotai and Li, Wenqi and Aertsen, Michael and Deprest, Jan and Ourselin, S{\'e}bastien and Vercauteren, Tom},
	month = apr,
	year = {2019},
	keywords = {Data augmentation, Convolutional neural networks, Medical image segmentation, Uncertainty estimation},
	pages = {34--45},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/9GPHMP2K/S0925231219301961.html:text/html},
}

@article{radosavovic_data_2017,
	title = {Data {Distillation}: {Towards} {Omni}-{Supervised} {Learning}},
	shorttitle = {Data {Distillation}},
	abstract = {We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.},
	urldate = {2021-01-03},
	journal = {arXiv:1712.04440 [cs]},
	author = {Radosavovic, Ilija and Doll{\'a}r, Piotr and Girshick, Ross and Gkioxari, Georgia and He, Kaiming},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.04440},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/DJZC5ZAR/1712.html:text/html},
}

@article{ghafoorian_transfer_2017,
	title = {Transfer {Learning} for {Domain} {Adaptation} in {MRI}: {Application} in {Brain} {Lesion} {Segmentation}},
	volume = {10435},
	shorttitle = {Transfer {Learning} for {Domain} {Adaptation} in {MRI}},
	url = {http://arxiv.org/abs/1702.07841},
	doi = {10.1007/978-3-319-66179-7_59},
	abstract = {Magnetic Resonance Imaging (MRI) is widely used in routine clinical diagnosis and treatment. However, variations in MRI acquisition protocols result in different appearances of normal and diseased tissue in the images. Convolutional neural networks (CNNs), which have shown to be successful in many medical image analysis tasks, are typically sensitive to the variations in imaging protocols. Therefore, in many cases, networks trained on data acquired with one MRI protocol, do not perform satisfactorily on data acquired with different protocols. This limits the use of models trained with large annotated legacy datasets on a new dataset with a different domain which is often a recurring situation in clinical settings. In this study, we aim to answer the following central questions regarding domain adaptation in medical image analysis: Given a fitted legacy model, 1) How much data from the new domain is required for a decent adaptation of the original network?; and, 2) What portion of the pre-trained model parameters should be retrained given a certain number of the new domain training samples? To address these questions, we conducted extensive experiments in white matter hyperintensity segmentation task. We trained a CNN on legacy MR images of brain and evaluated the performance of the domain-adapted network on the same task with images from a different domain. We then compared the performance of the model to the surrogate scenarios where either the same trained network is used or a new network is trained from scratch on the new dataset.The domain-adapted network tuned only by two training examples achieved a Dice score of 0.63 substantially outperforming a similar network trained on the same set of examples from scratch.},
	urldate = {2021-01-02},
	journal = {arXiv:1702.07841 [cs]},
	author = {Ghafoorian, Mohsen and Mehrtash, Alireza and Kapur, Tina and Karssemeijer, Nico and Marchiori, Elena and Pesteie, Mehran and Guttmann, Charles R. G. and de Leeuw, Frank-Erik and Tempany, Clare M. and van Ginneken, Bram and Fedorov, Andriy and Abolmaesumi, Purang and Platel, Bram and Wells III, William M.},
	year = {2017},
	note = {arXiv: 1702.07841},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {516--524},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/ZNXPUQ9N/1702.html:text/html},
}

@article{jungo_analyzing_2020,
	title = {Analyzing the {Quality} and {Challenges} of {Uncertainty} {Estimations} for {Brain} {Tumor} {Segmentation}},
	volume = {14},
	issn = {1662-453X},
	doi = {10.3389/fnins.2020.00282},
	abstract = {Automatic segmentation of brain tumors has the potential to enable volumetric measures and high-throughput analysis in the clinical setting. Reaching this potential seems almost achieved, considering the steady increase in segmentation accuracy. However, despite segmentation accuracy, the current methods still do not meet the robustness levels required for patient-centered clinical use. In this regard, uncertainty estimates are a promising direction to improve the robustness of automated segmentation systems. Different uncertainty estimation methods have been proposed, but little is known about their usefulness and limitations for brain tumor segmentation. In this study, we present an analysis of the most commonly used uncertainty estimation methods in regards to benefits and challenges for brain tumor segmentation. We evaluated their quality in terms of calibration, segmentation error localization, and segmentation failure detection. Our results show that the uncertainty methods are typically well-calibrated when evaluated at the dataset level. Evaluated at the subject level, we found notable miscalibrations and limited segmentation error localization (e.g., for correcting segmentations), which hinder the direct use of the voxel-wise uncertainties. Nevertheless, voxel-wise uncertainty showed value to detect failed segmentations when uncertainty estimates are aggregated at the subject level. Therefore, we suggest a careful usage of voxel-wise uncertainty measures and highlight the importance of developing solutions that address the subject-level requirements on calibration and segmentation error localization.},
	language = {English},
	urldate = {2021-01-02},
	journal = {Frontiers in Neuroscience},
	author = {Jungo, Alain and Balsiger, Fabian and Reyes, Mauricio},
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {segmentation, deep learning, brain tumor, quality, Uncertainty Estimation},
}

@article{devries_leveraging_2018,
	title = {Leveraging {Uncertainty} {Estimates} for {Predicting} {Segmentation} {Quality}},
	abstract = {The use of deep learning for medical imaging has seen tremendous growth in the research community. One reason for the slow uptake of these systems in the clinical setting is that they are complex, opaque and tend to fail silently. Outside of the medical imaging domain, the machine learning community has recently proposed several techniques for quantifying model uncertainty (i.e.{\textasciitilde}a model knowing when it has failed). This is important in practical settings, as we can refer such cases to manual inspection or correction by humans. In this paper, we aim to bring these recent results on estimating uncertainty to bear on two important outputs in deep learning-based segmentation. The first is producing spatial uncertainty maps, from which a clinician can observe where and why a system thinks it is failing. The second is quantifying an image-level prediction of failure, which is useful for isolating specific cases and removing them from automated pipelines. We also show that reasoning about spatial uncertainty, the first output, is a useful intermediate representation for generating segmentation quality predictions, the second output. We propose a two-stage architecture for producing these measures of uncertainty, which can accommodate any deep learning-based medical segmentation pipeline.},
	urldate = {2021-01-02},
	journal = {arXiv:1807.00502 [cs]},
	author = {DeVries, Terrance and Taylor, Graham W.},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.00502},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/EFPVY3TT/1807.html:text/html},
}

@article{jungo_uncertainty-driven_2018,
	title = {Uncertainty-driven {Sanity} {Check}: {Application} to {Postoperative} {Brain} {Tumor} {Cavity} {Segmentation}},
	shorttitle = {Uncertainty-driven {Sanity} {Check}},
	abstract = {Uncertainty estimates of modern neuronal networks provide additional information next to the computed predictions and are thus expected to improve the understanding of the underlying model. Reliable uncertainties are particularly interesting for safety-critical computer-assisted applications in medicine, e.g., neurosurgical interventions and radiotherapy planning. We propose an uncertainty-driven sanity check for the identification of segmentation results that need particular expert review. Our method uses a fully-convolutional neural network and computes uncertainty estimates by the principle of Monte Carlo dropout. We evaluate the performance of the proposed method on a clinical dataset with 30 postoperative brain tumor images. The method can segment the highly inhomogeneous resection cavities accurately (Dice coefficients 0.792 \${\textbackslash}pm\$ 0.154). Furthermore, the proposed sanity check is able to detect the worst segmentation and three out of the four outliers. The results highlight the potential of using the additional information from the model's parameter uncertainty to validate the segmentation performance of a deep learning model.},
	urldate = {2021-01-02},
	journal = {arXiv:1806.03106 [cs]},
	author = {Jungo, Alain and Meier, Raphael and Ermis, Ekin and Herrmann, Evelyn and Reyes, Mauricio},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.03106},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/5635WUEJ/1806.html:text/html},
}

@inproceedings{eaton-rosen_towards_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards {Safe} {Deep} {Learning}: {Accurately} {Quantifying} {Biomarker} {Uncertainty} in {Neural} {Network} {Predictions}},
	isbn = {978-3-030-00928-1},
	shorttitle = {Towards {Safe} {Deep} {Learning}},
	doi = {10.1007/978-3-030-00928-1_78},
	abstract = {Automated medical image segmentation, specifically using deep learning, has shown outstanding performance in semantic segmentation tasks. However, these methods rarely quantify their uncertainty, which may lead to errors in downstream analysis. In this work we propose to use Bayesian neural networks to quantify uncertainty within the domain of semantic segmentation. We also propose a method to convert voxel-wise segmentation uncertainty into volumetric uncertainty, and calibrate the accuracy and reliability of confidence intervals of derived measurements. When applied to a tumour volume estimation application, we demonstrate that by using such modelling of uncertainty, deep learning systems can be made to report volume estimates with well-calibrated error-bars, making them safer for clinical use. We also show that the uncertainty estimates extrapolate to unseen data, and that the confidence intervals are robust in the presence of artificial noise. This could be used to provide a form of quality control and quality assurance, and may permit further adoption of deep learning tools in the clinic.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} {\textendash} {MICCAI} 2018},
	publisher = {Springer International Publishing},
	author = {Eaton-Rosen, Zach and Bragman, Felix and Bisdas, Sotirios and Ourselin, S{\'e}bastien and Cardoso, M. Jorge},
	editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-L{\'o}pez, Carlos and Fichtinger, Gabor},
	year = {2018},
	pages = {691--699},
}

@article{jungo_assessing_2019,
	title = {Assessing {Reliability} and {Challenges} of {Uncertainty} {Estimations} for {Medical} {Image} {Segmentation}},
	abstract = {Despite the recent improvements in overall accuracy, deep learning systems still exhibit low levels of robustness. Detecting possible failures is critical for a successful clinical integration of these systems, where each data point corresponds to an individual patient. Uncertainty measures are a promising direction to improve failure detection since they provide a measure of a system's confidence. Although many uncertainty estimation methods have been proposed for deep learning, little is known on their benefits and current challenges for medical image segmentation. Therefore, we report results of evaluating common voxel-wise uncertainty measures with respect to their reliability, and limitations on two medical image segmentation datasets. Results show that current uncertainty methods perform similarly and although they are well-calibrated at the dataset level, they tend to be miscalibrated at subject-level. Therefore, the reliability of uncertainty estimates is compromised, highlighting the importance of developing subject-wise uncertainty estimations. Additionally, among the benchmarked methods, we found auxiliary networks to be a valid alternative to common uncertainty methods since they can be applied to any previously trained segmentation model.},
	urldate = {2021-01-02},
	journal = {arXiv:1907.03338 [cs, eess]},
	author = {Jungo, Alain and Reyes, Mauricio},
	month = oct,
	year = {2019},
	note = {arXiv: 1907.03338},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/ZV3T67YC/1907.html:text/html},
}

@article{graham_mild-net_2019,
	title = {{MILD}-{Net}: {Minimal} {Information} {Loss} {Dilated} {Network} for {Gland} {Instance} {Segmentation} in {Colon} {Histology} {Images}},
	volume = {52},
	issn = {13618415},
	shorttitle = {{MILD}-{Net}},
	doi = {10.1016/j.media.2018.12.001},
	abstract = {The analysis of glandular morphology within colon histopathology images is an important step in determining the grade of colon cancer. Despite the importance of this task, manual segmentation is laborious, time-consuming and can suffer from subjectivity among pathologists. The rise of computational pathology has led to the development of automated methods for gland segmentation that aim to overcome the challenges of manual segmentation. However, this task is non-trivial due to the large variability in glandular appearance and the difficulty in differentiating between certain glandular and non-glandular histological structures. Furthermore, a measure of uncertainty is essential for diagnostic decision making. To address these challenges, we propose a fully convolutional neural network that counters the loss of information caused by max-pooling by re-introducing the original image at multiple points within the network. We also use atrous spatial pyramid pooling with varying dilation rates for preserving the resolution and multi-level aggregation. To incorporate uncertainty, we introduce random transformations during test time for an enhanced segmentation result that simultaneously generates an uncertainty map, highlighting areas of ambiguity. We show that this map can be used to define a metric for disregarding predictions with high uncertainty. The proposed network achieves state-of-the-art performance on the GlaS challenge dataset and on a second independent colorectal adenocarcinoma dataset. In addition, we perform gland instance segmentation on whole-slide images from two further datasets to highlight the generalisability of our method. As an extension, we introduce MILD-Net+ for simultaneous gland and lumen segmentation, to increase the diagnostic power of the network.},
	urldate = {2021-01-02},
	journal = {Medical Image Analysis},
	author = {Graham, Simon and Chen, Hao and Gamper, Jevgenij and Dou, Qi and Heng, Pheng-Ann and Snead, David and Tsang, Yee Wah and Rajpoot, Nasir},
	month = feb,
	year = {2019},
	note = {arXiv: 1806.01963},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {199--211},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/4FERPM4D/1806.html:text/html},
}

@article{nair_exploring_2018,
	title = {Exploring {Uncertainty} {Measures} in {Deep} {Networks} for {Multiple} {Sclerosis} {Lesion} {Detection} and {Segmentation}},
	abstract = {Deep learning (DL) networks have recently been shown to outperform other segmentation methods on various public, medical-image challenge datasets [3,11,16], especially for large pathologies. However, in the context of diseases such as Multiple Sclerosis (MS), monitoring all the focal lesions visible on MRI sequences, even very small ones, is essential for disease staging, prognosis, and evaluating treatment efficacy. Moreover, producing deterministic outputs hinders DL adoption into clinical routines. Uncertainty estimates for the predictions would permit subsequent revision by clinicians. We present the first exploration of multiple uncertainty estimates based on Monte Carlo (MC) dropout [4] in the context of deep networks for lesion detection and segmentation in medical images. Specifically, we develop a 3D MS lesion segmentation CNN, augmented to provide four different voxel-based uncertainty measures based on MC dropout. We train the network on a proprietary, large-scale, multi-site, multi-scanner, clinical MS dataset, and compute lesion-wise uncertainties by accumulating evidence from voxel-wise uncertainties within detected lesions. We analyze the performance of voxel-based segmentation and lesion-level detection by choosing operating points based on the uncertainty. Empirical evidence suggests that uncertainty measures consistently allow us to choose superior operating points compared only using the network's sigmoid output as a probability.},
	urldate = {2021-01-02},
	journal = {arXiv:1808.01200 [cs]},
	author = {Nair, Tanya and Precup, Doina and Arnold, Douglas L. and Arbel, Tal},
	month = oct,
	year = {2018},
	note = {arXiv: 1808.01200},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/NRU9FNRX/1808.html:text/html},
}

@inproceedings{perez-garcia_simulation_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Simulation of {Brain} {Resection} for {Cavity} {Segmentation} {Using} {Self}-supervised and {Semi}-supervised {Learning}},
	isbn = {978-3-030-59716-0},
	doi = {10.1007/978-3-030-59716-0_12},
	abstract = {Resective surgery may be curative for drug-resistant focal epilepsy, but only 40\% to 70\% of patients achieve seizure freedom after surgery. Retrospective quantitative analysis could elucidate patterns in resected structures and patient outcomes to improve resective surgery. However, the resection cavity must first be segmented on the postoperative MR image. Convolutional neural networks (CNNs) are the state-of-the-art image segmentation technique, but require large amounts of annotated data for training. Annotation of medical images is a time-consuming process requiring highly-trained raters, and often suffering from high inter-rater variability. Self-supervised learning can be used to generate training instances from unlabeled data. We developed an algorithm to simulate resections on preoperative MR images. We curated a new dataset, EPISURG, comprising 431 postoperative and 269 preoperative MR images from 431 patients who underwent resective surgery. In addition to EPISURG, we used three public datasets comprising 1813 preoperative MR images for training. We trained a 3D CNN on artificially resected images created on the fly during training, using images from 1) EPISURG, 2) public datasets and 3) both. To evaluate trained models, we calculate Dice score (DSC) between model segmentations and 200 manual annotations performed by three human raters. The model trained on data with manual annotations obtained a median (interquartile range) DSC of 65.3 (30.6). The DSC of our best-performing model, trained with no manual annotations, is 81.7 (14.2). For comparison, inter-rater agreement between human annotators was 84.0 (9.9). We demonstrate a training method for CNNs using simulated resection cavities that can accurately segment real resection cavities, without manual annotations.},
	language = {en},
	booktitle = {{MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {P{\'e}rez-Garc{\'i}a, Fernando and Rodionov, Roman and Alim-Marvasti, Ali and Sparks, Rachel and Duncan, John S. and Ourselin, S{\'e}bastien},
	year = {2020},
	keywords = {Segmentation, Self-supervised learning, Neurosurgery},
	pages = {115--125},
}

@article{singh_3d_2020,
	title = {{3D} {Deep} {Learning} on {Medical} {Images}: {A} {Review}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {{3D} {Deep} {Learning} on {Medical} {Images}},
	doi = {10.3390/s20185097},
	abstract = {The rapid advancements in machine learning, graphics processing technologies and the availability of medical imaging data have led to a rapid increase in the use of deep learning models in the medical domain. This was exacerbated by the rapid advancements in convolutional neural network (CNN) based architectures, which were adopted by the medical imaging community to assist clinicians in disease diagnosis. Since the grand success of AlexNet in 2012, CNNs have been increasingly used in medical image analysis to improve the efficiency of human clinicians. In recent years, three-dimensional (3D) CNNs have been employed for the analysis of medical images. In this paper, we trace the history of how the 3D CNN was developed from its machine learning roots, we provide a brief mathematical description of 3D CNN and provide the preprocessing steps required for medical images before feeding them to 3D CNNs. We review the significant research in the field of 3D medical imaging analysis using 3D CNNs (and its variants) in different medical areas such as classification, segmentation, detection and localization. We conclude by discussing the challenges associated with the use of 3D CNNs in the medical imaging domain (and the use of deep learning models in general) and possible future trends in the field.},
	language = {en},
	number = {18},
	urldate = {2021-01-16},
	journal = {Sensors},
	author = {Singh, Satya P. and Wang, Lipo and Gupta, Sukrit and Goli, Haveesh and Padmanabhan, Parasuraman and Guly{\'a}s, Bal{\'a}zs},
	month = jan,
	year = {2020},
	note = {Number: 18
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {segmentation, classification, 3D convolutional neural networks, 3D medical images, detection, localization},
	pages = {5097},
	file = {Snapshot:/home/fernando/Zotero/storage/FSEJC4W4/htm.html:text/html},
}

@article{nikolenko_synthetic_2019,
	title = {Synthetic {Data} for {Deep} {Learning}},
	abstract = {Synthetic data is an increasingly popular tool for training deep learning models, especially in computer vision but also in other areas. In this work, we attempt to provide a comprehensive survey of the various directions in the development and application of synthetic data. First, we discuss synthetic datasets for basic computer vision problems, both low-level (e.g., optical flow estimation) and high-level (e.g., semantic segmentation), synthetic environments and datasets for outdoor and urban scenes (autonomous driving), indoor scenes (indoor navigation), aerial navigation, simulation environments for robotics, applications of synthetic data outside computer vision (in neural programming, bioinformatics, NLP, and more); we also survey the work on improving synthetic data development and alternative ways to produce it such as GANs. Second, we discuss in detail the synthetic-to-real domain adaptation problem that inevitably arises in applications of synthetic data, including synthetic-to-real refinement with GAN-based models and domain adaptation at the feature/model level without explicit data transformations. Third, we turn to privacy-related applications of synthetic data and review the work on generating synthetic datasets with differential privacy guarantees. We conclude by highlighting the most promising directions for further work in synthetic data studies.},
	urldate = {2021-01-16},
	journal = {arXiv:1909.11512 [cs]},
	author = {Nikolenko, Sergey I.},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.11512},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/ETM7AZWW/1909.html:text/html},
}

@inproceedings{zhang_deep_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep {Adversarial} {Networks} for {Biomedical} {Image} {Segmentation} {Utilizing} {Unannotated} {Images}},
	isbn = {978-3-319-66179-7},
	doi = {10.1007/978-3-319-66179-7_47},
	abstract = {Semantic segmentation is a fundamental problem in biomedical image analysis. In biomedical practice, it is often the case that only limited annotated data are available for model training. Unannotated images, on the other hand, are easier to acquire. How to utilize unannotated images for training effective segmentation models is an important issue. In this paper, we propose a new deep adversarial network (DAN) model for biomedical image segmentation, aiming to attain consistently good segmentation results on both annotated and unannotated images. Our model consists of two networks: (1) a segmentation network (SN) to conduct segmentation; (2) an evaluation network (EN) to assess segmentation quality. During training, EN is encouraged to distinguish between segmentation results of unannotated images and annotated ones (by giving them different scores), while SN is encouraged to produce segmentation results of unannotated images such that EN cannot distinguish these from the annotated ones. Through an iterative adversarial training process, because EN is constantly {\textquotedblleft}criticizing{\textquotedblright} the segmentation results of unannotated images, SN can be trained to produce more and more accurate segmentation for unannotated and unseen samples. Experiments show that our proposed DAN model is effective in utilizing unannotated image data to obtain considerably better segmentation.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} - {MICCAI} 2017},
	publisher = {Springer International Publishing},
	author = {Zhang, Yizhe and Yang, Lin and Chen, Jianxu and Fredericksen, Maridel and Hughes, David P. and Chen, Danny Z.},
	editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D. Louis and Duchesne, Simon},
	year = {2017},
	pages = {408--416},
}

@inproceedings{chitphakdithai_non-rigid_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Non-rigid {Registration} with {Missing} {Correspondences} in {Preoperative} and {Postresection} {Brain} {Images}},
	isbn = {978-3-642-15705-9},
	doi = {10.1007/978-3-642-15705-9_45},
	abstract = {Registration of preoperative and postresection images is often needed to evaluate the effectiveness of treatment. While several non-rigid registration methods exist, most would be unable to accurately align these types of datasets due to the absence of tissue in one image. Here we present a joint registration and segmentation algorithm which handles the missing correspondence problem. An intensity-based prior is used to aid in the segmentation of the resection region from voxels with valid correspondences in the two images. The problem is posed in a maximum a posteriori (MAP) framework and optimized using the expectation-maximization (EM) algorithm. Results on both synthetic and real data show our method improved image alignment compared to a traditional non-rigid registration algorithm as well as a method using a robust error kernel in the registration similarity metric.},
	language = {en},
	booktitle = {{MICCAI} 2010},
	publisher = {Springer},
	author = {Chitphakdithai, Nicha and Duncan, James S.},
	editor = {Jiang, Tianzi and Navab, Nassir and Pluim, Josien P. W. and Viergever, Max A.},
	year = {2010},
	keywords = {Registration Problem, Correspondence Region, Registration Parameter, Resection Region, Segmentation Framework},
	pages = {367--374},
}

@article{dorent_learning_2021,
	title = {Learning joint segmentation of tissues and brain lesions from task-specific hetero-modal domain-shifted datasets},
	volume = {67},
	issn = {1361-8415},
	doi = {10.1016/j.media.2020.101862},
	abstract = {Brain tissue segmentation from multimodal MRI is a key building block of many neuroimaging analysis pipelines. Established tissue segmentation approaches have, however, not been developed to cope with large anatomical changes resulting from pathology, such as white matter lesions or tumours, and often fail in these cases. In the meantime, with the advent of deep neural networks (DNNs), segmentation of brain lesions has matured significantly. However, few existing approaches allow for the joint segmentation of normal tissue and brain lesions. Developing a DNN for such a joint task is currently hampered by the fact that annotated datasets typically address only one specific task and rely on task-specific imaging protocols including a task-specific set of imaging modalities. In this work, we propose a novel approach to build a joint tissue and lesion segmentation model from aggregated task-specific hetero-modal domain-shifted and partially-annotated datasets. Starting from a variational formulation of the joint problem, we show how the expected risk can be decomposed and optimised empirically. We exploit an upper bound of the risk to deal with heterogeneous imaging modalities across datasets. To deal with potential domain shift, we integrated and tested three conventional techniques based on data augmentation, adversarial learning and pseudo-healthy generation. For each individual task, our joint approach reaches comparable performance to task-specific and fully-supervised models. The proposed framework is assessed on two different types of brain lesions: White matter lesions and gliomas. In the latter case, lacking a joint ground-truth for quantitative assessment purposes, we propose and use a novel clinically-relevant qualitative assessment methodology.},
	language = {en},
	urldate = {2021-01-16},
	journal = {Medical Image Analysis},
	author = {Dorent, Reuben and Booth, Thomas and Li, Wenqi and Sudre, Carole H. and Kafiabadi, Sina and Cardoso, Jorge and Ourselin, Sebastien and Vercauteren, Tom},
	month = jan,
	year = {2021},
	keywords = {Domain adaptation, Joint learning, Multi-Modal, Multi-Task learning},
	pages = {101862},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/YDXG6T8W/S1361841520302267.html:text/html},
}

@inproceedings{jegou_one_2017,
	title = {The {One} {Hundred} {Layers} {Tiramisu}: {Fully} {Convolutional} {DenseNets} for {Semantic} {Segmentation}},
	shorttitle = {The {One} {Hundred} {Layers} {Tiramisu}},
	doi = {10.1109/CVPRW.2017.156},
	abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {J{\'e}gou, S. and Drozdzal, M. and Vazquez, D. and Romero, A. and Bengio, Y.},
	month = jul,
	year = {2017},
	note = {ISSN: 2160-7516},
	keywords = {image resolution, Computer architecture, convolutional neural networks, image classification, Spatial resolution, feature extraction, learning (artificial intelligence), image segmentation, Image segmentation, Semantics, convolution, Benchmark testing, convolutional DenseNets, semantic image segmentation, Standards, Tiramisu layers, upsampling path training},
	pages = {1175--1183},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/RP2Q5MYB/8014890.html:text/html},
}

@article{simpson_large_2019,
	title = {A large annotated medical image dataset for the development and evaluation of segmentation algorithms},
	abstract = {Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts. We sought to create a large collection of annotated medical image datasets of various clinically relevant anatomies available under open source license to facilitate the development of semantic segmentation algorithms. Such a resource would allow: 1) objective assessment of general-purpose segmentation methods through comprehensive benchmarking and 2) open and free access to medical image data for any researcher interested in the problem domain. Through a multi-institutional effort, we generated a large, curated dataset representative of several highly variable segmentation tasks that was used in a crowd-sourced challenge - the Medical Segmentation Decathlon held during the 2018 Medical Image Computing and Computer Aided Interventions Conference in Granada, Spain. Here, we describe these ten labeled image datasets so that these data may be effectively reused by the research community.},
	urldate = {2021-01-16},
	journal = {arXiv:1902.09063 [cs, eess]},
	author = {Simpson, Amber L. and Antonelli, Michela and Bakas, Spyridon and Bilello, Michel and Farahani, Keyvan and van Ginneken, Bram and Kopp-Schneider, Annette and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and Bilic, Patrick and Christ, Patrick F. and Do, Richard K. G. and Gollub, Marc and Golia-Pernicka, Jennifer and Heckers, Stephan H. and Jarnagin, William R. and McHugo, Maureen K. and Napel, Sandy and Vorontsov, Eugene and Maier-Hein, Lena and Cardoso, M. Jorge},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.09063},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/LCWLA7F7/1902.html:text/html},
}

@inproceedings{venturini_uncertainty_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Uncertainty {Estimates} as {Data} {Selection} {Criteria} to {Boost} {Omni}-{Supervised} {Learning}},
	isbn = {978-3-030-59710-8},
	doi = {10.1007/978-3-030-59710-8_67},
	abstract = {For many medical applications, large quantities of imaging data are routinely obtained but it can be difficult and time-consuming to obtain high-quality labels for that data. We propose a novel uncertainty-based method to improve the performance of segmentation networks when limited manual labels are available in a large dataset. We estimate segmentation uncertainty on unlabeled data using test-time augmentation and test-time dropout. We then use uncertainty metrics to select unlabeled samples for further training in a semi-supervised learning framework. Compared to random data selection, our method gives a significant boost in Dice coefficient for semi-supervised volume segmentation on the EADC-ADNI/HARP MRI dataset and the large-scale INTERGROWTH-21st ultrasound dataset. Our results show a greater performance boost on the ultrasound dataset, suggesting that our method is most useful with data of lower or more variable quality.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} {\textendash} {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Venturini, Lorenzo and Papageorghiou, Aris T. and Noble, J. Alison and Namburete, Ana I. L.},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {Uncertainty, Boosting, Omni-supervised learning},
	pages = {689--698},
}

@article{abdar_review_2021,
	title = {A {Review} of {Uncertainty} {Quantification} in {Deep} {Learning}: {Techniques}, {Applications} and {Challenges}},
	shorttitle = {A {Review} of {Uncertainty} {Quantification} in {Deep} {Learning}},
	abstract = {Uncertainty quantification (UQ) plays a pivotal role in reduction of uncertainties during both optimization and decision making processes. It can be applied to solve a variety of real-world applications in science and engineering. Bayesian approximation and ensemble learning techniques are two most widely-used UQ methods in the literature. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning. Moreover, we also investigate the application of these methods in reinforcement learning (RL). Then, we outline a few important applications of UQ methods. Finally, we briefly highlight the fundamental research challenges faced by UQ methods and discuss the future research directions in this field.},
	urldate = {2021-01-16},
	journal = {arXiv:2011.06225 [cs]},
	author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
	month = jan,
	year = {2021},
	note = {arXiv: 2011.06225},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/QPUITDJM/2011.html:text/html},
}

@article{yu_uncertainty-aware_2019,
	title = {Uncertainty-aware {Self}-ensembling {Model} for {Semi}-supervised {3D} {Left} {Atrium} {Segmentation}},
	abstract = {Training deep convolutional neural networks usually requires a large amount of labeled data. However, it is expensive and time-consuming to annotate data for medical image segmentation tasks. In this paper, we present a novel uncertainty-aware semi-supervised framework for left atrium segmentation from 3D MR images. Our framework can effectively leverage the unlabeled data by encouraging consistent predictions of the same input under different perturbations. Concretely, the framework consists of a student model and a teacher model, and the student model learns from the teacher model by minimizing a segmentation loss and a consistency loss with respect to the targets of the teacher model. We design a novel uncertainty-aware scheme to enable the student model to gradually learn from the meaningful and reliable targets by exploiting the uncertainty information. Experiments show that our method achieves high performance gains by incorporating the unlabeled data. Our method outperforms the state-of-the-art semi-supervised methods, demonstrating the potential of our framework for the challenging semi-supervised problems.},
	urldate = {2021-01-16},
	journal = {arXiv:1907.07034 [cs]},
	author = {Yu, Lequan and Wang, Shujun and Li, Xiaomeng and Fu, Chi-Wing and Heng, Pheng-Ann},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.07034},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/24CZ9RU9/1907.html:text/html},
}

@article{xia_uncertainty-aware_2020,
	title = {Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation},
	volume = {65},
	issn = {13618415},
	doi = {10.1016/j.media.2020.101766},
	abstract = {Although having achieved great success in medical image segmentation, deep learning-based approaches usually require large amounts of well-annotated data, which can be extremely expensive in the field of medical image analysis. Unlabeled data, on the other hand, is much easier to acquire. Semi-supervised learning and unsupervised domain adaptation both take the advantage of unlabeled data, and they are closely related to each other. In this paper, we propose uncertainty-aware multi-view co-training (UMCT), a unified framework that addresses these two tasks for volumetric medical image segmentation. Our framework is capable of efficiently utilizing unlabeled data for better performance. We firstly rotate and permute the 3D volumes into multiple views and train a 3D deep network on each view. We then apply co-training by enforcing multi-view consistency on unlabeled data, where an uncertainty estimation of each view is utilized to achieve accurate labeling. Experiments on the NIH pancreas segmentation dataset and a multi-organ segmentation dataset show state-of-the-art performance of the proposed framework on semi-supervised medical image segmentation. Under unsupervised domain adaptation settings, we validate the effectiveness of this work by adapting our multi-organ segmentation model to two pathological organs from the Medical Segmentation Decathlon Datasets. Additionally, we show that our UMCT-DA model can even effectively handle the challenging situation where labeled source data is inaccessible, demonstrating strong potentials for real-world applications.},
	urldate = {2021-01-16},
	journal = {Medical Image Analysis},
	author = {Xia, Yingda and Yang, Dong and Yu, Zhiding and Liu, Fengze and Cai, Jinzheng and Yu, Lequan and Zhu, Zhuotun and Xu, Daguang and Yuille, Alan and Roth, Holger},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.16806},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {101766},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/CF8DPVLA/2006.html:text/html},
}

@article{noauthor_pediatric_2019,
	title = {Pediatric epilepsy resection {MRI} dataset},
	doi = {10.1184/R1/9856205},
	abstract = {Participants: six pediatric patients
with resections to the visual cortex and two patients with resections outside the visual cortex, and
15 age-matched typically-developing controls. Images were acquired on a Siemens Verio 3T scanner with a 32-channel head coil at Carnegie Mellon University. For all participants: a skull-stripped T1-weighted anatomical image and one set of diffusion spectrum images are included here. See Maallo et al, "Effects of unilateral cortical resection of the visual cortex on bilateral human white matter," NeuroImage (in press), doi: 10.1016/j.neuroimage.2019.116345},
	language = {en},
	urldate = {2021-01-17},
	month = nov,
	year = {2019},
	note = {Publisher: Carnegie Mellon University
type: dataset},
	file = {Snapshot:/home/fernando/Zotero/storage/DRYRSWV3/9856205.html:text/html},
}

@book{zwillinger_crc_1999,
	title = {{CRC} {Standard} {Probability} and {Statistics} {Tables} and {Formulae}},
	isbn = {978-1-4200-5026-4},
	abstract = {Whether you are a statistician, engineer, or businessperson, you need statistics. You want to be able to easily reference tables, find formulas, and know how to use them so you can extract information from data without getting bogged down by advanced statistical methods. Your goal is to determine the appropriate statistical procedures and interpret the results. Standard Probability and Statistics: Tables and Formulae provides the tools you need to do just that. Logically organized and reaching far beyond a mere catalog, a textual description accompanies each entry- most include an example. The topics addressed are directly applicable to modern business and engineering as well as to statistics, including regression analysis, ANOVA, decision theory, signal processing, and control theory. The result is an accessible, example-oriented handbook that supplies the basic principles, the most commonly used values, and the information to make them work for you. It is easy to fill a statistics reference with hundreds of pages of tables - sometimes for just one test. This handbook is much more. With topics ranging from classical statistics to modern applications, Standard Probability and Statistics fills the need for an up-to-date, authoritative statistics reference.},
	language = {en},
	publisher = {CRC Press},
	author = {Zwillinger, Daniel and Kokoska, Stephen},
	month = dec,
	year = {1999},
	note = {Google-Books-ID: tB3RVEZ0UIMC},
	keywords = {Mathematics / Probability \& Statistics / Bayesian Analysis, Mathematics / Probability \& Statistics / General},
}

@article{fonov_unbiased_2011,
	title = {Unbiased average age-appropriate atlases for pediatric studies},
	volume = {54},
	issn = {1053-8119},
	doi = {10.1016/j.neuroimage.2010.07.033},
	abstract = {Spatial normalization, registration, and segmentation techniques for Magnetic Resonance Imaging (MRI) often use a target or template volume to facilitate processing, take advantage of prior information, and define a common coordinate system for analysis. In the neuroimaging literature, the MNI305 Talairach-like coordinate system is often used as a standard template. However, when studying pediatric populations, variation from the adult brain makes the MNI305 suboptimal for processing brain images of children. Morphological changes occurring during development render the use of age-appropriate templates desirable to reduce potential errors and minimize bias during processing of pediatric data. This paper presents the methods used to create unbiased, age-appropriate MRI atlas templates for pediatric studies that represent the average anatomy for the age range of 4.5{\textendash}18.5years, while maintaining a high level of anatomical detail and contrast. The creation of anatomical T1-weighted, T2-weighted, and proton density-weighted templates for specific developmentally important age-ranges, used data derived from the largest epidemiological, representative (healthy and normal) sample of the U.S. population, where each subject was carefully screened for medical and psychiatric factors and characterized using established neuropsychological and behavioral assessments. Use of these age-specific templates was evaluated by computing average tissue maps for gray matter, white matter, and cerebrospinal fluid for each specific age range, and by conducting an exemplar voxel-wise deformation-based morphometry study using 66 young (4.5{\textendash}6.9years) participants to demonstrate the benefits of using the age-appropriate templates. The public availability of these atlases/templates will facilitate analysis of pediatric MRI data and enable comparison of results between studies in a common standardized space specific to pediatric research.},
	language = {en},
	number = {1},
	urldate = {2021-01-20},
	journal = {NeuroImage},
	author = {Fonov, Vladimir and Evans, Alan C. and Botteron, Kelly and Almli, C. Robert and McKinstry, Robert C. and Collins, D. Louis},
	month = jan,
	year = {2011},
	keywords = {Atlas template, Pediatric image analysis, Registration},
	pages = {313--327},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/STQE9QZ3/S1053811910010062.html:text/html},
}

@inproceedings{li_compactness_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Compactness}, {Efficiency}, and {Representation} of {3D} {Convolutional} {Networks}: {Brain} {Parcellation} as a {Pretext} {Task}},
	isbn = {978-3-319-59050-9},
	shorttitle = {On the {Compactness}, {Efficiency}, and {Representation} of {3D} {Convolutional} {Networks}},
	doi = {10.1007/978-3-319-59050-9_28},
	abstract = {Deep convolutional neural networks are powerful tools for learning visual representations from images. However, designing efficient deep architectures to analyse volumetric medical images remains challenging. This work investigates efficient and flexible elements of modern convolutional networks such as dilated convolution and residual connection. With these essential building blocks, we propose a high-resolution, compact convolutional network for volumetric image segmentation. To illustrate its efficiency of learning 3D representation from large-scale image data, the proposed network is validated with the challenging task of parcellating 155 neuroanatomical structures from brain MR images. Our experiments show that the proposed network architecture compares favourably with state-of-the-art volumetric segmentation networks while being an order of magnitude more compact. We consider the brain parcellation task as a pretext task for volumetric image segmentation; our trained network potentially provides a good starting point for transfer learning. Additionally, we show the feasibility of voxel-level uncertainty estimation using a sampling approximation through dropout.},
	language = {en},
	booktitle = {Information {Processing} in {Medical} {Imaging}},
	publisher = {Springer International Publishing},
	author = {Li, Wenqi and Wang, Guotai and Fidon, Lucas and Ourselin, Sebastien and Cardoso, M. Jorge and Vercauteren, Tom},
	editor = {Niethammer, Marc and Styner, Martin and Aylward, Stephen and Zhu, Hongtu and Oguz, Ipek and Yap, Pew-Thian and Shen, Dinggang},
	year = {2017},
	keywords = {Convolutional Neural Network, Dice Coefficient Similarity, Receptive Field, Trained Network, Transfer Learning},
	pages = {348--360},
}

@article{gal_dropout_2016,
	title = {Dropout as a {Bayesian} {Approximation}: {Representing} {Model} {Uncertainty} in {Deep} {Learning}},
	shorttitle = {Dropout as a {Bayesian} {Approximation}},
	url = {http://arxiv.org/abs/1506.02142},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
	urldate = {2021-01-21},
	journal = {arXiv:1506.02142 [cs, stat]},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = oct,
	year = {2016},
	note = {arXiv: 1506.02142},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/97U2A2UM/1506.html:text/html},
}

@inproceedings{hacohen_power_2019,
	title = {On {The} {Power} of {Curriculum} {Learning} in {Training} {Deep} {Networks}},
	abstract = {Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum lear...},
	language = {en},
	urldate = {2021-01-25},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hacohen, Guy and Weinshall, Daphna},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2535--2544},
	file = {Snapshot:/home/fernando/Zotero/storage/5NQM2LKN/hacohen19a.html:text/html},
}

@article{granados_generative_2021,
	title = {A generative model of hyperelastic strain energy density functions for multiple tissue brain deformation},
	volume = {16},
	issn = {1861-6429},
	doi = {10.1007/s11548-020-02284-y},
	abstract = {Estimation of brain deformation is crucial during neurosurgery. Whilst mechanical characterisation captures stress{\textendash}strain relationships of tissue, biomechanical models are limited by experimental conditions. This results in variability reported in the literature. The aim of this work was to demonstrate a generative model of strain energy density functions can estimate the elastic properties of tissue using observed brain deformation.},
	language = {en},
	number = {1},
	urldate = {2021-01-25},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Granados, Alejandro and P{\'e}rez-Garc{\'i}a, Fernando and Schweiger, Martin and Vakharia, Vejay and Vos, Sjoerd B. and Miserocchi, Anna and McEvoy, Andrew W. and Duncan, John S. and Sparks, Rachel and Ourselin, S{\'e}bastien},
	month = jan,
	year = {2021},
	pages = {141--150},
}

@article{winston_optic_2012,
	title = {Optic radiation tractography and vision in anterior temporal lobe resection},
	volume = {71},
	issn = {1531-8249},
	doi = {10.1002/ana.22619},
	language = {eng},
	number = {3},
	journal = {Annals of Neurology},
	author = {Winston, Gavin P. and Daga, Pankaj and Stretton, Jason and Modat, Marc and Symms, Mark R. and McEvoy, Andrew W. and Ourselin, Sebastien and Duncan, John S.},
	month = mar,
	year = {2012},
	pmid = {22451201},
	pmcid = {PMC3698700},
	keywords = {Humans, Adolescent, Adult, Female, Male, Middle Aged, Young Adult, Vision, Ocular, Epilepsy, Temporal Lobe, Temporal Lobe, Postoperative Complications, Diffusion Tensor Imaging, Preoperative Care, Vision Disorders, Visual Fields, Visual Pathways},
	pages = {334--341},
}

@inproceedings{zhu_effective_2014,
	title = {An {Effective} {Interactive} {Medical} {Image} {Segmentation} {Method} using {Fast} {GrowCut}},
	volume = {17},
	abstract = {Segmentation of anatomical structures in medical imagery is a key step in a variety of clinical applications. Designing a generic, automated method that works for various structures and imaging modalities is a daunting task. In this paper, we present an effective interactive segmentation method that reformulates the GrowCut algorithm as a clustering problem and computes a fast, approximate solution. The method is further improved by using an efficient updating scheme requiring only local computations when new user input becomes available, making it applicable to high resolution images. The algorithm may easily be included as a user-oriented software module in any number of available medical imaging/image processing platforms such as 3D Slicer. The efficiency and effectiveness of the algorithm are demonstrated through tests on several challenging data sets where it is also compared to standard GrowCut.},
	booktitle = {Int {Conf} {Med} {Image} {Comput} {Comput} {Assist} {Interv}. {Workshop} on {Interactive} {Methods}},
	author = {Zhu, Liangija and Kolesov, Ivan and Gao, Yi and Kikinis, Ron and Tannenbaum, Allen},
	year = {2014},
}

@article{vilella_association_2021,
	title = {Association of {Peri}-ictal {Brainstem} {Posturing} {With} {Seizure} {Severity} and {Breathing} {Compromise} in {Patients} {With} {Generalized} {Convulsive} {Seizures}},
	volume = {96},
	issn = {1526-632X},
	doi = {10.1212/WNL.0000000000011274},
	abstract = {OBJECTIVE: To analyze the association between peri-ictal brainstem posturing semiologies with postictal generalized electroencephalographic suppression (PGES) and breathing dysfunction in generalized convulsive seizures (GCS).
METHODS: In this prospective, multicenter analysis of GCS, ictal brainstem semiology was classified as (1) decerebration (bilateral symmetric tonic arm extension), (2) decortication (bilateral symmetric tonic arm flexion only), (3) hemi-decerebration (unilateral tonic arm extension with contralateral flexion) and (4) absence of ictal tonic phase. Postictal posturing was also assessed. Respiration was monitored with thoracoabdominal belts, video, and pulse oximetry.
RESULTS: Two hundred ninety-five seizures (180 patients) were analyzed. Ictal decerebration was observed in 122 of 295 (41.4\%), decortication in 47 of 295 (15.9\%), and hemi-decerebration in 28 of 295 (9.5\%) seizures. Tonic phase was absent in 98 of 295 (33.2\%) seizures. Postictal posturing occurred in 18 of 295 (6.1\%) seizures. PGES risk increased with ictal decerebration (odds ratio [OR] 14.79, 95\% confidence interval [CI] 6.18-35.39, p {\textless} 0.001), decortication (OR 11.26, 95\% CI 2.96-42.93, p {\textless} 0.001), or hemi-decerebration (OR 48.56, 95\% CI 6.07-388.78, p {\textless} 0.001). Ictal decerebration was associated with longer PGES (p = 0.011). Postictal posturing was associated with postconvulsive central apnea (PCCA) (p = 0.004), longer hypoxemia (p {\textless} 0.001), and Spo2 recovery (p = 0.035).
CONCLUSIONS: Ictal brainstem semiology is associated with increased PGES risk. Ictal decerebration is associated with longer PGES. Postictal posturing is associated with a 6-fold increased risk of PCCA, longer hypoxemia, and Spo2 recovery. Peri-ictal brainstem posturing may be a surrogate biomarker for GCS severity identifiable without in-hospital monitoring.
CLASSIFICATION OF EVIDENCE: This study provides Class III evidence that peri-ictal brainstem posturing is associated with the GCS with more prolonged PGES and more severe breathing dysfunction.},
	language = {eng},
	number = {3},
	journal = {Neurology},
	author = {Vilella, Laura and Lacuey, Nuria and Hampson, Johnson P. and Zhu, Liang and Omidi, Shirin and Ochoa-Urrea, Manuela and Tao, Shiqiang and Rani, M. R. Sandhya and Sainju, Rup K. and Friedman, Daniel and Nei, Maromi and Strohl, Kingman and Scott, Catherine and Allen, Luke and Gehlbach, Brian K. and Hupp, Norma J. and Hampson, Jaison S. and Shafiabadi, Nassim and Zhao, Xiuhe and Reick-Mitrisin, Victoria and Schuele, Stephan and Ogren, Jennifer and Harper, Ronald M. and Diehl, Beate and Bateman, Lisa M. and Devinsky, Orrin and Richerson, George B. and Ryvlin, Philippe and Zhang, Guo-Qiang and Lhatoo, Samden D.},
	month = jan,
	year = {2021},
	pmid = {33268557},
	keywords = {Humans, Adolescent, Adult, Aged, Female, Male, Middle Aged, Young Adult, Electroencephalography, Epilepsy, Generalized, Seizures, Brain Stem, Posture, Respiration, Severity of Illness Index},
	pages = {e352--e365},
}

@article{cardoso_geodesic_2015,
	title = {Geodesic {Information} {Flows}: {Spatially}-{Variant} {Graphs} and {Their} {Application} to {Segmentation} and {Fusion}},
	volume = {34},
	issn = {1558-254X},
	shorttitle = {Geodesic {Information} {Flows}},
	doi = {10.1109/TMI.2015.2418298},
	abstract = {Clinical annotations, such as voxel-wise binary or probabilistic tissue segmentations, structural parcellations, pathological regions-of-interest and anatomical landmarks are key to many clinical studies. However, due to the time consuming nature of manually generating these annotations, they tend to be scarce and limited to small subsets of data. This work explores a novel framework to propagate voxel-wise annotations between morphologically dissimilar images by diffusing and mapping the available examples through intermediate steps. A spatially-variant graph structure connecting morphologically similar subjects is introduced over a database of images, enabling the gradual diffusion of information to all the subjects, even in the presence of large-scale morphological variability. We illustrate the utility of the proposed framework on two example applications: brain parcellation using categorical labels and tissue segmentation using probabilistic features. The application of the proposed method to categorical label fusion showed highly statistically significant improvements when compared to state-of-the-art methodologies. Significant improvements were also observed when applying the proposed framework to probabilistic tissue segmentation of both synthetic and real data, mainly in the presence of large morphological variability.},
	language = {eng},
	number = {9},
	journal = {IEEE Trans Med Imaging},
	author = {Cardoso, M. Jorge and Modat, Marc and Wolz, Robin and Melbourne, Andrew and Cash, David and Rueckert, Daniel and Ourselin, Sebastien},
	month = sep,
	year = {2015},
	pmid = {25879909},
	keywords = {Algorithms, Brain, Humans, Image Processing, Computer-Assisted, Magnetic Resonance Imaging, Computer Simulation, Adult, Aged, Aged, 80 and over, Female, Male, Middle Aged, Neuroimaging, Young Adult},
	pages = {1976--1988},
}

@article{fonov_unbiased_2009,
	title = {Unbiased nonlinear average age-appropriate brain templates from birth to adulthood},
	volume = {47},
	doi = {10.1016/S1053-8119(09)70884-5},
	journal = {Neuroimage},
	author = {Fonov, Vladimir and Evans, Alan and Mckinstry, Robert and Almli, C.R. and Collins, Louis},
	month = jul,
	year = {2009},
}

@article{herrmann_fully_2018,
	title = {Fully {Automated} {Segmentation} of the {Brain} {Resection} {Cavity} for {Radiation} {Target} {Volume} {Definition} in {Glioblastoma} {Patients}},
	volume = {102},
	doi = {10.1016/j.ijrobp.2018.07.087},
	journal = {International Journal of Radiation Oncology*Biology*Physics},
	author = {Herrmann, Evelyn and Ermis, Ekin and Meier, Raphael and Blatti-Moreno, M. and Knecht, Urspeter and Aebersold, Daniel and Manser, Peter and Mauricio, R.},
	month = nov,
	year = {2018},
	pages = {S194},
}

@inproceedings{hutchinson_accuracy_2020,
	title = {Accuracy and {Performance} {Comparison} of {Video} {Action} {Recognition} {Approaches}},
	doi = {10.1109/HPEC43674.2020.9286249},
	abstract = {Over the past few years, there has been significant interest in video action recognition systems and models. However, direct comparison of accuracy and computational performance results remain clouded by differing training environments, hardware specifications, hyperparameters, pipelines, and inference methods. This article provides a direct comparison between fourteen {\textquotedblleft}off-the-shelf{\textquotedblright} and state-of-the-art models by ensuring consistency in these training characteristics in order to provide readers with a meaningful comparison across different types of video action recognition algorithms. Accuracy of the models is evaluated using standard Top-1 and Top-5 accuracy metrics in addition to a proposed new accuracy metric. Additionally, we compare computational performance of distributed training from two to sixty-four GPUs on a state-of-the-art HPC system.},
	booktitle = {2020 {IEEE} {High} {Performance} {Extreme} {Computing} {Conference} ({HPEC})},
	author = {Hutchinson, M. and Samsi, S. and Arcand, W. and Bestor, D. and Bergeron, B. and Byun, C. and Houle, M. and Hubbell, M. and Jones, M. and Kepner, J. and Kirby, A. and Michaleas, P. and Milechin, L. and Mullen, J. and Prout, A. and Rosa, A. and Reuther, A. and Yee, C. and Gadepally, V.},
	month = sep,
	year = {2020},
	note = {ISSN: 2643-1971},
	keywords = {Training, Computational modeling, Feature extraction, deep learning, Three-dimensional displays, Solid modeling, action recognition, Two dimensional displays, Measurement, accuracy metrics, computational performance, neural network},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/MUBSHHT8/9286249.html:text/html},
}

@inproceedings{xie_aggregated_2017,
	title = {Aggregated {Residual} {Transformations} for {Deep} {Neural} {Networks}},
	doi = {10.1109/CVPR.2017.634},
	abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Xie, S. and Girshick, R. and Doll{\'a}r, P. and Tu, Z. and He, K.},
	month = jul,
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {Computer architecture, image classification, Neural networks, Network topology, topology, Neurons, aggregated residual transformations, building block, cardinality, classification accuracy, COCO detection set, Complexity theory, deep neural networks, highly modularized network architecture, hyper-parameters, ILSVRC 2016 classification task, ImageNet-1K dataset, multibranch architecture, neural net architecture, simple network architecture, Topology},
	pages = {5987--5995},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/MSE4XQ8K/8100117.html:text/html},
}

@article{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	urldate = {2021-02-16},
	journal = {arXiv:1711.05101 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv: 1711.05101},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/7Z9VA7UQ/1711.html:text/html},
}

@article{zagoruyko_wide_2017,
	title = {Wide {Residual} {Networks}},
	url = {http://arxiv.org/abs/1605.07146},
	abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
	urldate = {2021-02-18},
	journal = {arXiv:1605.07146 [cs]},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	month = jun,
	year = {2017},
	note = {arXiv: 1605.07146},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/PGP3S38U/1605.html:text/html},
}

@article{ghadiyaram_large-scale_2019,
	title = {Large-scale weakly-supervised pre-training for video action recognition},
	url = {http://arxiv.org/abs/1905.00561},
	abstract = {Current fully-supervised video datasets consist of only a few hundred thousand videos and fewer than a thousand domain-specific labels. This hinders the progress towards advanced video architectures. This paper presents an in-depth study of using large volumes of web videos for pre-training video models for the task of action recognition. Our primary empirical finding is that pre-training at a very large scale (over 65 million videos), despite on noisy social-media videos and hashtags, substantially improves the state-of-the-art on three challenging public action recognition datasets. Further, we examine three questions in the construction of weakly-supervised video action datasets. First, given that actions involve interactions with objects, how should one construct a verb-object pre-training label space to benefit transfer learning the most? Second, frame-based models perform quite well on action recognition; is pre-training for good image features sufficient or is pre-training for spatio-temporal features valuable for optimal transfer learning? Finally, actions are generally less well-localized in long videos vs. short videos; since action labels are provided at a video level, how should one choose video clips for best performance, given some fixed budget of number or minutes of videos?},
	urldate = {2021-02-25},
	journal = {arXiv:1905.00561 [cs]},
	author = {Ghadiyaram, Deepti and Feiszli, Matt and Tran, Du and Yan, Xueting and Wang, Heng and Mahajan, Dhruv},
	month = may,
	year = {2019},
	note = {arXiv: 1905.00561},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/8RC3DJYZ/1905.html:text/html},
}

@article{nashef_unifying_2012,
	title = {Unifying the definitions of sudden unexpected death in epilepsy},
	volume = {53},
	issn = {1528-1167},
	doi = {10.1111/j.1528-1167.2011.03358.x},
	abstract = {Sudden unexpected death in epilepsy (SUDEP) is a category of death in people with epilepsy occurring in the absence of a known structural cause of death and is most likely heterogeneous with regard to mechanisms and circumstances. SUDEP is particularly difficult to investigate in research studies for several reasons, including its relatively low incidence, its unpredictable occurrence often in unwitnessed settings, and its low rate of complete autopsy examinations. Over the past two decades, two complementary definitions have been used in most SUDEP studies, but often with variations. We propose here a unified SUDEP definition and classification to resolve current ambiguities and to retrieve cases that would not have been further studied if the previous definitions were used. The proposed Unified SUDEP Definition and Classification contains, in addition to concepts inherent in the previous definitions, nine main recommendations. (1) The word "unexpected," and not the word "unexplained," should be uniformly used in the term SUDEP. (2) The SUDEP category should be applied when appropriate, whether or not a terminal seizure is known to have occurred. (3) The "Possible SUDEP" category should be used only for cases with competing causes of death, with cases left unclassified when data are insufficient to reasonably permit their classification. (4) Cases that would otherwise fulfill the definition of SUDEP should be designated as "SUDEP Plus" when evidence indicates that a preexisting condition, known before or after autopsy, could have contributed to the death, which otherwise is classified as SUDEP (e.g., coronary insufficiency with no evidence of myocardial infarction or long-QT syndrome with no documented primary ventricular arrhythmia leading to death). (5) To be considered SUDEP, the death should have occurred within 1 h from the onset of a known terminal event. (6) For status epilepticus as an exclusion criterion for SUDEP, the duration of seizure activity should be 30 min or more. (7) A specific category of SUDEP due to asphyxia should not be designated, the distinction being largely impractical on circumstantial or autopsy evidence, with more than one mechanism likely to be contributory in many cases. (8) Death occurring in water but without circumstantial or autopsy evidence of submersion should be classified as "Possible SUDEP." If any evidence of submersion is present, the death should not be classified as SUDEP. (9) A category of "Near-SUDEP" should be agreed to include cases in which cardiorespiratory arrest was reversed by resuscitation efforts with subsequent survival for more than 1 h. Scenarios that demonstrate the basis for each SUDEP category are described. If disagreement exists about which category fits a particular case, we suggest the use of consensus decision by a panel of informed reviewers to adjudicate the classification of the case.},
	language = {eng},
	number = {2},
	journal = {Epilepsia},
	author = {Nashef, Lina and So, Elson L. and Ryvlin, Philippe and Tomson, Torbj{\"o}rn},
	month = feb,
	year = {2012},
	pmid = {22191982},
	keywords = {Humans, Epilepsy, Death, Sudden},
	pages = {227--233},
}

@article{ryvlin_incidence_2013,
	title = {Incidence and mechanisms of cardiorespiratory arrests in epilepsy monitoring units ({MORTEMUS}): a retrospective study},
	volume = {12},
	issn = {1474-4465},
	shorttitle = {Incidence and mechanisms of cardiorespiratory arrests in epilepsy monitoring units ({MORTEMUS})},
	doi = {10.1016/S1474-4422(13)70214-X},
	abstract = {BACKGROUND: Sudden unexpected death in epilepsy (SUDEP) is the leading cause of death in people with chronic refractory epilepsy. Very rarely, SUDEP occurs in epilepsy monitoring units, providing highly informative data for its still elusive pathophysiology. The MORTEMUS study expanded these data through comprehensive evaluation of cardiorespiratory arrests encountered in epilepsy monitoring units worldwide.
METHODS: Between Jan 1, 2008, and Dec 29, 2009, we did a systematic retrospective survey of epilepsy monitoring units located in Europe, Israel, Australia, and New Zealand, to retrieve data for all cardiorespiratory arrests recorded in these units and estimate their incidence. Epilepsy monitoring units from other regions were invited to report similar cases to further explore the mechanisms. An expert panel reviewed data, including video electroencephalogram (VEEG) and electrocardiogram material at the time of cardiorespiratory arrests whenever available.
FINDINGS: 147 (92\%) of 160 units responded to the survey. 29 cardiorespiratory arrests, including 16 SUDEP (14 at night), nine near SUDEP, and four deaths from other causes, were reported. Cardiorespiratory data, available for ten cases of SUDEP, showed a consistent and previously unrecognised pattern whereby rapid breathing (18-50 breaths per min) developed after secondary generalised tonic-clonic seizure, followed within 3 min by transient or terminal cardiorespiratory dysfunction. Where transient, this dysfunction later recurred with terminal apnoea occurring within 11 min of the end of the seizure, followed by cardiac arrest. SUDEP incidence in adult epilepsy monitoring units was 5{\textperiodcentered}1 (95\% CI 2{\textperiodcentered}6-9{\textperiodcentered}2) per 1000 patient-years, with a risk of 1{\textperiodcentered}2 (0{\textperiodcentered}6-2{\textperiodcentered}1) per 10,000 VEEG monitorings, probably aggravated by suboptimum supervision and possibly by antiepileptic drug withdrawal.
INTERPRETATION: SUDEP in epilepsy monitoring units primarily follows an early postictal, centrally mediated, severe alteration of respiratory and cardiac function induced by generalised tonic-clonic seizure, leading to immediate death or a short period of partly restored cardiorespiratory function followed by terminal apnoea then cardiac arrest. Improved supervision is warranted in epilepsy monitoring units, in particular during night time.
FUNDING: Commission of European Affairs of the International League Against Epilepsy.},
	language = {eng},
	number = {10},
	journal = {The Lancet. Neurology},
	author = {Ryvlin, Philippe and Nashef, Lina and Lhatoo, Samden D. and Bateman, Lisa M. and Bird, Jonathan and Bleasel, Andrew and Boon, Paul and Crespel, Arielle and Dworetzky, Barbara A. and H{\o}genhaven, Hans and Lerche, Holger and Maillard, Louis and Malter, Michael P. and Marchal, Cecile and Murthy, Jagarlapudi M. K. and Nitsche, Michael and Pataraia, Ekaterina and Rabben, Terje and Rheims, Sylvain and Sadzot, Bernard and Schulze-Bonhage, Andreas and Seyal, Masud and So, Elson L. and Spitz, Mark and Szucs, Anna and Tan, Meng and Tao, James X. and Tomson, Torbj{\"o}rn},
	month = oct,
	year = {2013},
	pmid = {24012372},
	keywords = {Humans, Adult, Female, Male, Middle Aged, Young Adult, Epilepsy, Incidence, Seizures, Retrospective Studies, Child, Australia, Death, Sudden, Cardiac, Europe, Heart Arrest, Hospital Units, Israel, New Zealand},
	pages = {966--977},
}

@inproceedings{cicek_3d_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{3D} {U}-{Net}: {Learning} {Dense} {Volumetric} {Segmentation} from {Sparse} {Annotation}},
	isbn = {978-3-319-46723-8},
	shorttitle = {{3D} {U}-{Net}},
	doi = {10.1007/978-3-319-46723-8_49},
	abstract = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} {\textendash} {MICCAI} 2016},
	publisher = {Springer International Publishing},
	author = {Cicek, Ozgun and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
	editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
	year = {2016},
	keywords = {Convolutional neural networks, 3D, Biomedical volumetric image segmentation, Fully-automated, Semi-automated, Sparse annotation, Xenopus kidney},
	pages = {424--432},
}

@inproceedings{abadi_tensorflow_2016-1,
	address = {USA},
	series = {{OSDI}'16},
	title = {{TensorFlow}: a system for large-scale machine learning},
	isbn = {978-1-931971-33-1},
	shorttitle = {{TensorFlow}},
	abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 12th {USENIX} conference on {Operating} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Abadi, Mart{\'i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = nov,
	year = {2016},
	pages = {265--283},
}

@article{buslaev_albumentations_2020,
	title = {Albumentations: {Fast} and {Flexible} {Image} {Augmentations}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Albumentations},
	url = {https://www.mdpi.com/2078-2489/11/2/125},
	doi = {10.3390/info11020125},
	abstract = {Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve corresponding output labels. In computer vision, image augmentations have become a common implicit regularization technique to combat overfitting in deep learning models and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations of flipping, rotating, scaling, and cropping. Moreover, image processing speed varies in existing image augmentation libraries. We present Albumentations, a fast and flexible open source library for image augmentation with many various image transform operations available that is also an easy-to-use wrapper around other augmentation libraries. We discuss the design principles that drove the implementation of Albumentations and give an overview of the key features and distinct capabilities. Finally, we provide examples of image augmentations for different computer vision tasks and demonstrate that Albumentations is faster than other commonly used image augmentation tools on most image transform operations.},
	language = {en},
	number = {2},
	urldate = {2021-04-27},
	journal = {Information},
	author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {data augmentation, computer vision, deep learning},
	pages = {125},
	file = {Snapshot:/home/fernando/Zotero/storage/KNRVGK42/125.html:text/html},
}

@inproceedings{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://proceedings.mlr.press/v37/ioffe15.html},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer{\textquoteright}s inputs changes during training, as the parameters of the previous layers change. This slows down the t...},
	language = {en},
	urldate = {2021-04-27},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {448--456},
	file = {Snapshot:/home/fernando/Zotero/storage/768VNZEX/ioffe15.html:text/html},
}

@inproceedings{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring sp...},
	language = {en},
	urldate = {2021-04-27},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
	file = {Snapshot:/home/fernando/Zotero/storage/ZSNJLPVJ/chen20j.html:text/html},
}

@inproceedings{micikevicius_mixed_2018,
	title = {Mixed {Precision} {Training}},
	url = {https://openreview.net/forum?id=r1gs9JgRZ},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}, {ICLR} 2018, {Vancouver}, {BC}, {Canada}, {April} 30 - {May} 3, 2018, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory F. and Elsen, Erich and Garc{\'i}a, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
	year = {2018},
}

@inproceedings{brugger_partially_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Partially} {Reversible} {U}-{Net} for {Memory}-{Efficient} {Volumetric} {Image} {Segmentation}},
	isbn = {978-3-030-32248-9},
	doi = {10.1007/978-3-030-32248-9_48},
	abstract = {One of the key drawbacks of 3D convolutional neural networks for segmentation is their memory footprint, which necessitates compromises in the network architecture in order to fit into a given memory budget. Motivated by the RevNet for image classification, we propose a partially reversible U-Net architecture that reduces memory consumption substantially. The reversible architecture allows us to exactly recover each layer{\textquoteright}s outputs from the subsequent layer{\textquoteright}s ones, eliminating the need to store activations for backpropagation. This alleviates the biggest memory bottleneck and enables very deep (theoretically infinitely deep) 3D architectures. On the BraTS challenge dataset, we demonstrate substantial memory savings. We further show that the freed memory can be used for processing the whole field-of-view (FOV) instead of patches. Increasing network depth led to higher segmentation accuracy while growing the memory footprint only by a very small fraction, thanks to the partially reversible architecture.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} {\textendash} {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Br{\"u}gger, Robin and Baumgartner, Christian F. and Konukoglu, Ender},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	keywords = {CNN, Dice loss, Reversible neural network, U-Net},
	pages = {429--437},
}

@article{cheplygina_cats_2019,
	series = {Futures of {BME}: {Digital} {Health} and {BME} ? {Biomedical} {Imaging}: {Cardiovascular} {Imaging}},
	title = {Cats or {CAT} scans: {Transfer} learning from natural or medical image source data sets?},
	volume = {9},
	issn = {2468-4511},
	shorttitle = {Cats or {CAT} scans},
	url = {https://www.sciencedirect.com/science/article/pii/S2468451118300527},
	doi = {10.1016/j.cobme.2018.12.005},
	abstract = {Transfer learning is a widely used strategy in medical image analysis. Instead of only training a network with a limited amount of data from the target task of interest, we can first train the network with other, potentially larger source data sets, creating a more robust model. The source data sets do not have to be related to the target task. For a classification task in lung computed tomography (CT) images, we could use both head CT images~and images of cats as the source. While head CT images appear more similar to lung CT images, the number and diversity of cat images might lead to a better model overall. In this survey, we review a number of articles that have studied similar comparisons. Although the answer to which strategy is best seems to be {\textquoteleft}it depends{\textquoteright}, we discuss a number of research directions we need to take as a community to gain more understanding of this topic.},
	language = {en},
	urldate = {2021-04-27},
	journal = {Current Opinion in Biomedical Engineering},
	author = {Cheplygina, Veronika},
	month = mar,
	year = {2019},
	keywords = {Deep learning, Medical imaging, Transfer learning},
	pages = {21--27},
	file = {ScienceDirect Snapshot:/home/fernando/Zotero/storage/EDUBKTP4/S2468451118300527.html:text/html},
}

@inproceedings{raghu_transfusion_2019,
	title = {Transfusion: {Understanding} {Transfer} {Learning} for {Medical} {Imaging}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch{\'e}-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
}

@article{van_der_walt_numpy_2011,
	title = {The {NumPy} {Array}: {A} {Structure} for {Efficient} {Numerical} {Computation}},
	volume = {13},
	issn = {1558-366X},
	shorttitle = {The {NumPy} {Array}},
	doi = {10.1109/MCSE.2011.37},
	abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
	number = {2},
	journal = {Computing in Science Engineering},
	author = {van der Walt, Stefan and Colbert, S. Chris and Varoquaux, Gael},
	month = mar,
	year = {2011},
	note = {Conference Name: Computing in Science Engineering},
	keywords = {Computational efficiency, Arrays, Finite element methods, Numerical analysis, numerical computations, NumPy, Performance evaluation, programming libraries, Python, Resource management, scientific programming, Vector quantization},
	pages = {22--30},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/4JLCKEFB/5725236.html:text/html},
}

@inproceedings{shaw_heteroscedastic_2020,
	title = {A {Heteroscedastic} {Uncertainty} {Model} for {Decoupling} {Sources} of {MRI} {Image} {Quality}},
	url = {http://proceedings.mlr.press/v121/shaw20a.html},
	abstract = {Quality control (QC) of medical images is essential to ensure that downstream analyses such as segmentation can be performed successfully. Currently, QC is predominantly performed visually at signi...},
	language = {en},
	urldate = {2021-04-27},
	booktitle = {Medical {Imaging} with {Deep} {Learning}},
	publisher = {PMLR},
	author = {Shaw, Richard and Sudre, Carole H. and Ourselin, S{\'e}bastien and Cardoso, M. Jorge},
	month = sep,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {733--742},
	file = {Snapshot:/home/fernando/Zotero/storage/V58SXVLV/shaw20a.html:text/html},
}

@article{cardoso_geodesic_2015-1,
	title = {Geodesic {Information} {Flows}: {Spatially}-{Variant} {Graphs} and {Their} {Application} to {Segmentation} and {Fusion}},
	volume = {34},
	issn = {1558-254X},
	shorttitle = {Geodesic {Information} {Flows}},
	doi = {10.1109/TMI.2015.2418298},
	abstract = {Clinical annotations, such as voxel-wise binary or probabilistic tissue segmentations, structural parcellations, pathological regions-of-interest and anatomical landmarks are key to many clinical studies. However, due to the time consuming nature of manually generating these annotations, they tend to be scarce and limited to small subsets of data. This work explores a novel framework to propagate voxel-wise annotations between morphologically dissimilar images by diffusing and mapping the available examples through intermediate steps. A spatially-variant graph structure connecting morphologically similar subjects is introduced over a database of images, enabling the gradual diffusion of information to all the subjects, even in the presence of large-scale morphological variability. We illustrate the utility of the proposed framework on two example applications: brain parcellation using categorical labels and tissue segmentation using probabilistic features. The application of the proposed method to categorical label fusion showed highly statistically significant improvements when compared to state-of-the-art methodologies. Significant improvements were also observed when applying the proposed framework to probabilistic tissue segmentation of both synthetic and real data, mainly in the presence of large morphological variability.},
	language = {eng},
	number = {9},
	journal = {IEEE transactions on medical imaging},
	author = {Cardoso, M. Jorge and Modat, Marc and Wolz, Robin and Melbourne, Andrew and Cash, David and Rueckert, Daniel and Ourselin, Sebastien},
	month = sep,
	year = {2015},
	pmid = {25879909},
	keywords = {Algorithms, Brain, Humans, Image Processing, Computer-Assisted, Magnetic Resonance Imaging, Computer Simulation, Adult, Aged, Aged, 80 and over, Female, Male, Middle Aged, Neuroimaging, Young Adult},
	pages = {1976--1988},
}

@inproceedings{berger_adaptive_2018,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {An {Adaptive} {Sampling} {Scheme} to {Efficiently} {Train} {Fully} {Convolutional} {Networks} for {Semantic} {Segmentation}},
	isbn = {978-3-319-95921-4},
	doi = {10.1007/978-3-319-95921-4_26},
	abstract = {Deep convolutional neural networks (CNNs) have shown excellent performance in object recognition tasks and dense classification problems such as semantic segmentation. However, training deep neural networks on large and sparse datasets is still challenging and can require large amounts of computation and memory. In this work, we address the task of performing semantic segmentation on large data sets, such as three-dimensional medical images. We propose an adaptive sampling scheme that uses a-posterior error maps, generated throughout training, to focus sampling on difficult regions, resulting in improved learning. Our contribution is threefold: (1) We give a detailed description of the proposed sampling algorithm to speed up and improve learning performance on large images. (2) We propose a deep dual path CNN that captures information at fine and coarse scales, resulting in a network with a large field of view and high resolution outputs. (3) We show that our method is able to attain new state-of-the-art results on the VISCERAL Anatomy benchmark.},
	language = {en},
	booktitle = {Medical {Image} {Understanding} and {Analysis}},
	publisher = {Springer International Publishing},
	author = {Berger, Lorenz and Eoin, Hyde and Cardoso, M. Jorge and Ourselin, S{\'e}bastien},
	editor = {Nixon, Mark and Mahmoodi, Sasan and Zwiggelaar, Reyer},
	year = {2018},
	keywords = {Adaptive Sampling Strategy, Convolutional Neural Network (CNN), Density Classification Problem, Dice Score, Proposed Sampling Algorithm},
	pages = {277--286},
}

@article{iglesias_joint_2020,
	title = {Joint super-resolution and synthesis of 1 mm isotropic {MP}-{RAGE} volumes from clinical {MRI} exams with scans of different orientation, resolution and contrast},
	journal = {arXiv preprint arXiv:2012.13340},
	author = {Iglesias, Juan Eugenio and Billot, Benjamin and Balbastre, Yael and Tabari, Azadeh and Conklin, John and Alexander, Daniel and Golland, Polina and Edlow, Brian and Fischl, Bruce},
	year = {2020},
	file = {Joint super-resolution and synthesis of 1 mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast | Juan Eugenio Iglesias:/home/fernando/Zotero/storage/JY7GKMMU/joint-super-resolution-and-synthesis-1-mm-isotropic-mp-rage-volumes-clinical.html:text/html},
}

@article{klaser_multi-channel_2021,
	title = {A {Multi}-{Channel} {Uncertainty}-{Aware} {Multi}-{Resolution} {Network} for {MR} to {CT} {Synthesis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2076-3417/11/4/1667},
	doi = {10.3390/app11041667},
	abstract = {Synthesising computed tomography (CT) images from magnetic resonance images (MRI) plays an important role in the field of medical image analysis, both for quantification and diagnostic purposes. Convolutional neural networks (CNNs) have achieved state-of-the-art results in image-to-image translation for brain applications. However, synthesising whole-body images remains largely uncharted territory, involving many challenges, including large image size and limited field of view, complex spatial context, and anatomical differences between images acquired at different times. We propose the use of an uncertainty-aware multi-channel multi-resolution 3D cascade network specifically aiming for whole-body MR to CT synthesis. The Mean Absolute Error on the synthetic CT generated with the MultiResunc network (73.90 HU) is compared to multiple baseline CNNs like 3D U-Net (92.89 HU), HighRes3DNet (89.05 HU) and deep boosted regression (77.58 HU) and shows superior synthesis performance. We ultimately exploit the extrapolation properties of the MultiRes networks on sub-regions of the body.},
	language = {en},
	number = {4},
	urldate = {2021-05-04},
	journal = {Applied Sciences},
	author = {Klaser, Kerstin and Borges, Pedro and Shaw, Richard and Ranzini, Marta and Modat, Marc and Atkinson, David and Thielemans, Kris and Hutton, Brian and Goh, Vicky and Cook, Gary and Cardoso, Jorge and Ourselin, Sebastien},
	month = jan,
	year = {2021},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {MR to CT synthesis, multi-resolution CNN, uncertainty},
	pages = {1667},
	file = {Snapshot:/home/fernando/Zotero/storage/92G4J9BK/1667.html:text/html},
}

@misc{mccormick_itk_2021,
	title = {{ITK} 5.2 {Release} {Candidate} 3 available for testing},
	url = {https://blog.kitware.com/itk-5-2-release-candidate-3-available-for-testing/},
	abstract = {We are happy to announce the Insight Toolkit (ITK) 5.2 Release Candidate 3 is available for testing! ITK is an open-source, cross-platform toolkit for N-dimensional ... Read More},
	language = {en-US},
	urldate = {2021-05-04},
	journal = {Kitware Blog},
	author = {McCormick, Matt and Zuki{\'c}, D{\v z}enan and on, Stephen Aylward},
	month = mar,
	year = {2021},
	file = {Snapshot:/home/fernando/Zotero/storage/43BRHCXG/itk-5-2-release-candidate-3-available-for-testing.html:text/html},
}

@inproceedings{lee_image-and-spatial_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Image-and-{Spatial} {Transformer} {Networks} for {Structure}-{Guided} {Image} {Registration}},
	isbn = {978-3-030-32245-8},
	doi = {10.1007/978-3-030-32245-8_38},
	abstract = {Image registration with deep neural networks has become an active field of research and exciting avenue for a long standing problem in medical imaging. The goal is to learn a complex function that maps the appearance of input image pairs to parameters of a spatial transformation in order to align corresponding anatomical structures. We argue and show that the current direct, non-iterative approaches are sub-optimal, in particular if we seek accurate alignment of Structures-of-Interest (SoI). Information about SoI is often available at training time, for example, in form of segmentations or landmarks. We introduce a novel, generic framework, Image-and-Spatial Transformer Networks (ISTNs), to leverage SoI information allowing us to learn new image representations that are optimised for the downstream registration task. Thanks to these representations we can employ a test-specific, iterative refinement over the transformation parameters which yields highly accurate registration even with very limited training data. Performance is demonstrated on pairwise 3D brain registration and illustrative synthetic data.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} {\textendash} {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Lee, Matthew C. H. and Oktay, Ozan and Schuh, Andreas and Schaap, Michiel and Glocker, Ben},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	pages = {337--345},
}

@misc{noauthor_monai_nodate,
	title = {{MONAI} {Archives}},
	url = {https://blog.kitware.com/tag/monai/},
	language = {en-US},
	urldate = {2021-05-04},
	journal = {Kitware Blog},
	file = {Snapshot:/home/fernando/Zotero/storage/EEI6NQQS/monai.html:text/html},
}

@article{kofler_are_2021,
	title = {Are we using appropriate segmentation metrics? {Identifying} correlates of human expert perception for {CNN} training beyond rolling the {DICE} coefficient},
	shorttitle = {Are we using appropriate segmentation metrics?},
	url = {http://arxiv.org/abs/2103.06205},
	abstract = {In this study, we explore quantitative correlates of qualitative human expert perception. We discover that current quality metrics and loss functions, considered for biomedical image segmentation tasks, correlate moderately with segmentation quality assessment by experts, especially for small yet clinically relevant structures, such as the enhancing tumor in brain glioma. We propose a method employing classical statistics and experimental psychology to create complementary compound loss functions for modern deep learning methods, towards achieving a better fit with human quality assessment. When training a CNN for delineating adult brain tumor in MR images, all four proposed loss candidates outperform the established baselines on the clinically important and hardest to segment enhancing tumor label, while maintaining performance for other label channels.},
	urldate = {2021-04-28},
	journal = {arXiv:2103.06205 [cs, eess]},
	author = {Kofler, Florian and Ezhov, Ivan and Isensee, Fabian and Balsiger, Fabian and Berger, Christoph and Koerner, Maximilian and Paetzold, Johannes and Li, Hongwei and Shit, Suprosanna and McKinley, Richard and Bakas, Spyridon and Zimmer, Claus and Ankerst, Donna and Kirschke, Jan and Wiestler, Benedikt and Menze, Bjoern H.},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.06205},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/S9KQGU6R/2103.html:text/html},
}

@misc{nic_ma_project-monaimonai_2021,
	title = {Project-{MONAI}/{MONAI}: 0.5.0},
	shorttitle = {Project-{MONAI}/{MONAI}},
	url = {https://zenodo.org/record/4679866#.YImZHZNKgWo},
	abstract = {Added Overview document for feature highlights in v0.5.0 Invertible spatial transforms InvertibleTransform base APIs Batch inverse and decollating APIs Inverse of Compose Batch inverse event handling Test-time augmentation as an application Initial support of learning-based image registration: Bending energy, LNCC, and global mutual information loss Fully convolutional architectures Dense displacement field, dense velocity field computation Warping with high-order interpolation with C++/CUDA implementations Deepgrow modules for interactive segmentation: Workflows with simulations of clicks Distance-based transforms for guidance signals Digital pathology support: Efficient whole slide imaging IO and sampling with Nvidia cuCIM and SmartCache FROC measurements for lesion Probabilistic post-processing for lesion detection TorchVision classification model adaptor for fully convolutional analysis 12 new transforms, grid patch dataset, ThreadDataLoader, EfficientNets B0-B7 4 iteration events for the engine for finer control of workflows New C++/CUDA extensions: Conditional random field Fast bilateral filtering using the permutohedral lattice Metrics summary reporting and saving APIs DiceCELoss, DiceFocalLoss, a multi-scale wrapper for segmentation loss computation Data loading utilities: decollate\_batch PadListDataCollate with inverse support Support of slicing syntax for Dataset Initial Torchscript support for the loss modules Learning rate finder Allow for missing keys in the dictionary-based transforms Support of checkpoint loading for transfer learning Various summary and plotting utilities for Jupyter notebooks Contributor Covenant Code of Conduct Major CI/CD enhancements covering the tutorial repository Fully compatible with PyTorch 1.8 Initial nightly CI/CD pipelines using Nvidia Blossom Infrastructure Changed Enhanced list\_data\_collate error handling Unified iteration metric APIs densenet* extensions are renamed to DenseNet* se\_res* network extensions are renamed to SERes* Transform base APIs are rearranged into compose, inverse, and transform \_do\_transform flag for the random augmentations is unified via RandomizableTransform Decoupled post-processing steps, e.g. softmax, to\_onehot\_y, from the metrics computations Moved the distributed samplers to monai.data.samplers from monai.data.utils Engine's data loaders now accept generic iterables as input Workflows now accept additional custom events and state properties Various type hints according to Numpy 1.20 Refactored testing utility runtests.sh to have --unittest and --net integration tests options Base Docker image upgraded to nvcr.io/nvidia/pytorch:21.02-py3 from nvcr.io/nvidia/pytorch:20.10-py3 Docker images are now built with self-hosted environments Primary contact email updated to monai.contact@gmail.com Now using GitHub Discussions as the primary communication forum Removed Compatibility tests for PyTorch 1.5.x Format specific loaders, e.g. LoadNifti, NiftiDataset Assert statements from non-test files from module import * statements, addressed flake8 F403 Fixed Uses American English spelling for code, as per PyTorch Code coverage now takes multiprocessing runs into account SmartCache with initial shuffling ConvertToMultiChannelBasedOnBratsClasses now supports channel-first inputs Checkpoint handler to save with non-root permissions Fixed an issue for exiting the distributed unit tests Unified DynUNet to have single tensor output w/o deep supervision SegmentationSaver now supports user-specified data types and a squeeze\_end\_dims flag Fixed *Saver event handlers output filenames with a data\_root\_dir option Load image functions now ensure little-endian Fixed the test runner to support regex-based test case matching Usability issues in the event handlers},
	urldate = {2021-04-28},
	publisher = {Zenodo},
	author = {Nic Ma and Wenqi Li and Richard Brown and Yiheng Wang and Benjamin Gorman and Behrooz and Hans Johnson and Isaac Yang and Eric Kerfoot and Yiwen Li and Mohammad Adil and Yuan-Ting Hsieh and charliebudd and Arpit Aggarwal and Cameron Trentz and adam aji and Ben Murray and Gagan Daroach and Petru-Daniel Tudosiu and myron and Mark Graham and Balamurali and Christian Baker and Jan Sellner and Lucas Fidon and Alex Powers and Guy Leroy and Alxaline and Daniel Schulz},
	month = apr,
	year = {2021},
	doi = {10.5281/zenodo.4679866},
	file = {Zenodo Snapshot:/home/fernando/Zotero/storage/HXZR9B7N/4679866.html:text/html},
}

@article{ranzini_monaifbs_2021,
	title = {{MONAIfbs}: {MONAI}-based fetal brain {MRI} deep learning segmentation},
	shorttitle = {{MONAIfbs}},
	url = {http://arxiv.org/abs/2103.13314},
	abstract = {In fetal Magnetic Resonance Imaging, Super Resolution Reconstruction (SRR) algorithms are becoming popular tools to obtain high-resolution 3D volume reconstructions from low-resolution stacks of 2D slices, acquired at different orientations. To be effective, these algorithms often require accurate segmentation of the region of interest, such as the fetal brain in suspected pathological cases. In the case of Spina Bifida, Ebner, Wang et al. (NeuroImage, 2020) combined their SRR algorithm with a 2-step segmentation pipeline (2D localisation followed by a 2D segmentation network). However, if the localisation step fails, the second network is not able to recover a correct brain mask, thus requiring manual corrections for an effective SRR. In this work, we aim at improving the fetal brain segmentation for SRR in Spina Bifida. We hypothesise that a well-trained single-step UNet can achieve accurate performance, avoiding the need of a 2-step approach. We propose a new tool for fetal brain segmentation called MONAIfbs, which takes advantage of the Medical Open Network for Artificial Intelligence (MONAI) framework. Our network is based on the dynamic UNet (dynUNet), an adaptation of the nnU-Net framework. When compared to the original 2-step approach proposed in Ebner-Wang, and the same Ebner-Wang approach retrained with the expanded dataset available for this work, the dynUNet showed to achieve higher performance using a single step only. It also showed to reduce the number of outliers, as only 28 stacks obtained Dice score less than 0.9, compared to 68 for Ebner-Wang and 53 Ebner-Wang expanded. The proposed dynUNet model thus provides an improvement of the state-of-the-art fetal brain segmentation techniques, reducing the need for manual correction in automated SRR pipelines. Our code and our trained model are made publicly available at https://github.com/gift-surg/MONAIfbs.},
	urldate = {2021-04-28},
	journal = {arXiv:2103.13314 [cs, eess]},
	author = {Ranzini, Marta B. M. and Fidon, Lucas and Ourselin, S{\'e}bastien and Modat, Marc and Vercauteren, Tom},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.13314},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:/home/fernando/Zotero/storage/7ZDPRNSV/2103.html:text/html},
}

@article{isensee_nnu-net_2021,
	title = {{nnU}-{Net}: a self-configuring method for deep learning-based biomedical image segmentation},
	volume = {18},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {{nnU}-{Net}},
	url = {https://www.nature.com/articles/s41592-020-01008-z},
	doi = {10.1038/s41592-020-01008-z},
	abstract = {Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.},
	language = {en},
	number = {2},
	urldate = {2021-04-28},
	journal = {Nature Methods},
	author = {Isensee, Fabian and Jaeger, Paul F. and Kohl, Simon A. A. and Petersen, Jens and Maier-Hein, Klaus H.},
	month = feb,
	year = {2021},
	note = {Number: 2
Publisher: Nature Publishing Group},
	pages = {203--211},
	file = {Snapshot:/home/fernando/Zotero/storage/Q5DF2N3K/s41592-020-01008-z.html:text/html},
}

@inproceedings{valindria_multi-modal_2018,
	title = {Multi-modal {Learning} from {Unpaired} {Images}: {Application} to {Multi}-organ {Segmentation} in {CT} and {MRI}},
	shorttitle = {Multi-modal {Learning} from {Unpaired} {Images}},
	doi = {10.1109/WACV.2018.00066},
	abstract = {Convolutional neural networks have been widely used in medical image segmentation. The amount of training data strongly determines the overall performance. Most approaches are applied for a single imaging modality, e.g., brain MRI. In practice, it is often difficult to acquire sufficient training data of a certain imaging modality. The same anatomical structures, however, may be visible in different modalities such as major organs on abdominal CT and MRI. In this work, we investigate the effectiveness of learning from multiple modalities to improve the segmentation accuracy on each individual modality. We study the feasibility of using a dual-stream encoder-decoder architecture to learn modality-independent, and thus, generalisable and robust features. All of our MRI and CT data are unpaired, which means they are obtained from different subjects and not registered to each other. Experiments show that multi-modal learning can improve overall accuracy over modality-specific training. Results demonstrate that information across modalities can in particular improve performance on varying structures such as the spleen.},
	booktitle = {2018 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Valindria, Vanya V. and Pawlowski, Nick and Rajchl, Martin and Lavdas, Ioannis and Aboagye, Eric O. and Rockall, Andrea G. and Rueckert, Daniel and Glocker, Ben},
	month = mar,
	year = {2018},
	keywords = {Training, Magnetic resonance imaging, Image segmentation, Biomedical imaging, Computed tomography, Decoding},
	pages = {547--556},
	file = {IEEE Xplore Abstract Record:/home/fernando/Zotero/storage/USD3G6QK/8354170.html:text/html},
}

@article{cuadrado-godia_cerebral_2018,
	title = {Cerebral {Small} {Vessel} {Disease}: {A} {Review} {Focusing} on {Pathophysiology}, {Biomarkers}, and {Machine} {Learning} {Strategies}},
	volume = {20},
	issn = {2287-6391},
	shorttitle = {Cerebral {Small} {Vessel} {Disease}},
	doi = {10.5853/jos.2017.02922},
	abstract = {Cerebral small vessel disease (cSVD) has a crucial role in lacunar stroke and brain hemorrhages and is a leading cause of cognitive decline and functional loss in elderly patients. Based on underlying pathophysiology, cSVD can be subdivided into amyloidal and non-amyloidal subtypes. Genetic factors of cSVD play a pivotal role in terms of unraveling molecular mechanism. An important pathophysiological mechanism of cSVD is blood-brain barrier leakage and endothelium dysfunction which gives a clue in identification of the disease through circulating biological markers. Detection of cSVD is routinely carried out by key neuroimaging markers including white matter hyperintensities, lacunes, small subcortical infarcts, perivascular spaces, cerebral microbleeds, and brain atrophy. Application of neural networking, machine learning and deep learning in image processing have increased significantly for correct severity of cSVD. A linkage between cSVD and other neurological disorder, such as Alzheimer's and Parkinson's disease and non-cerebral disease, has also been investigated recently. This review draws a broad picture of cSVD, aiming to inculcate new insights into its pathogenesis and biomarkers. It also focuses on the role of deep machine strategies and other dimensions of cSVD by linking it with several cerebral and non-cerebral diseases as well as recent advances in the field to achieve sensitive detection, effective prevention and disease management.},
	language = {eng},
	number = {3},
	journal = {Journal of Stroke},
	author = {Cuadrado-Godia, Elisa and Dwivedi, Pratistha and Sharma, Sanjiv and Ois Santiago, Angel and Roquer Gonzalez, Jaume and Balcells, Mercedes and Laird, John and Turk, Monika and Suri, Harman S. and Nicolaides, Andrew and Saba, Luca and Khanna, Narendra N. and Suri, Jasjit S.},
	month = sep,
	year = {2018},
	pmid = {30309226},
	pmcid = {PMC6186915},
	keywords = {Neuroimaging, Machine learning, Biomarkers, Blood-brain barrier, Small vessel disease},
	pages = {302--320},
}

@article{mohamed_temporoparietooccipital_2011,
	title = {Temporoparietooccipital disconnection in children with intractable epilepsy: {Clinical} article},
	volume = {7},
	issn = {1933-0715, 1933-0707},
	shorttitle = {Temporoparietooccipital disconnection in children with intractable epilepsy},
	doi = {10.3171/2011.4.PEDS10454},
	language = {en\_US},
	number = {6},
	urldate = {2021-05-18},
	journal = {Journal of Neurosurgery: Pediatrics},
	author = {Mohamed, Ahmad R. and Freeman, Jeremy L. and Maixner, Wirginia and Bailey, Catherine A. and Wrennall, Jacquie A. and Harvey, A. Simon},
	month = jun,
	year = {2011},
	note = {Publisher: American Association of Neurological Surgeons
Section: Journal of Neurosurgery: Pediatrics},
	pages = {660--670},
	file = {Snapshot:/home/fernando/Zotero/storage/6Z38MEK9/article-p660.html:text/html},
}

@inproceedings{perez-garcia_transfer_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Transfer {Learning} of {Deep} {Spatiotemporal} {Networks} to {Model} {Arbitrarily} {Long} {Videos} of {Seizures}},
	isbn = {978-3-030-87240-3},
	doi = {10.1007/978-3-030-87240-3_32},
	abstract = {Detailed analysis of seizure semiology, the symptoms and signs which occur during a seizure, is critical for management of epilepsy patients. Inter-rater reliability using qualitative visual analysis is often poor for semiological features. Therefore, automatic and quantitative analysis of video-recorded seizures is needed for objective assessment. We present GESTURES, a novel architecture combining convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to learn deep representations of arbitrarily long videos of epileptic seizures. We use a spatiotemporal CNN (STCNN) pre-trained on large human action recognition (HAR) datasets to extract features from short snippets (??\{{\textbackslash}approx \}0.5 s) sampled from seizure videos. We then train an RNN to learn seizure-level representations from the sequence of features. We curated a dataset of seizure videos from 68 patients and evaluated GESTURES on its ability to classify seizures into focal onset seizures (FOSs) (??=106N=106N = 106) vs. focal to bilateral tonic-clonic seizures (TCSs) (??=77N=77N = 77), obtaining an accuracy of 98.9\% using bidirectional long short-term memory (BLSTM) units. We demonstrate that an STCNN trained on a HAR dataset can be used in combination with an RNN to accurately represent arbitrarily long videos of seizures. GESTURES can provide accurate seizure classification by modeling sequences of semiologies. The code, models and features dataset are available at https://github.com/fepegar/gestures-miccai-2021.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} {\textendash} {MICCAI} 2021},
	publisher = {Springer International Publishing},
	author = {P{\'e}rez-Garc{\'i}a, Fernando and Scott, Catherine and Sparks, Rachel and Diehl, Beate and Ourselin, S{\'e}bastien},
	year = {2021},
	keywords = {Transfer learning, Epilepsy video-telemetry, Temporal segment networks},
	pages = {334--344},
	file = {Springer Full Text PDF:/home/fernando/Zotero/storage/BGN8NURA/P{\'e}rez-Garc{\'i}a et al. - 2021 - Transfer Learning of Deep Spatiotemporal Networks .pdf:application/pdf},
}

@article{perez-garcia_torchio_2021,
	title = {{TorchIO}: {A} {Python} library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning},
	volume = {208},
	issn = {0169-2607},
	shorttitle = {{TorchIO}},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260721003102},
	doi = {10.1016/j.cmpb.2021.106236},
	abstract = {Background and objective
Processing of medical images such as MRI or CT presents different challenges compared to RGB images typically used in computer vision. These include a lack of labels for large datasets, high computational costs, and the need of metadata to describe the physical properties of voxels. Data augmentation is used to artificially increase the size of the training datasets. Training with image subvolumes or patches decreases the need for computational power. Spatial metadata needs to be carefully taken into account in order to ensure a correct alignment and orientation of volumes.
Methods
We present TorchIO, an open-source Python library to enable efficient loading, preprocessing, augmentation and patch-based sampling of medical images for deep learning. TorchIO follows the style of PyTorch and integrates standard medical image processing libraries to efficiently process images during training of neural networks. TorchIO transforms can be easily composed, reproduced, traced and extended. Most transforms can be inverted, making the library suitable for test-time augmentation and estimation of aleatoric uncertainty in the context of segmentation. We provide multiple generic preprocessing and augmentation operations as well as simulation of MRI-specific artifacts.
Results
Source code, comprehensive tutorials and extensive documentation for TorchIO can be found at http://torchio.rtfd.io/. The package can be installed from the Python Package Index (PyPI) running pip install torchio. It includes a command-line interface which allows users to apply transforms to image files without using Python. Additionally, we provide a graphical user interface within a TorchIO extension in 3D Slicer to visualize the effects of transforms.
Conclusion
TorchIO was developed to help researchers standardize medical image processing pipelines and allow them to focus on the deep learning experiments. It encourages good open-science practices, as it supports experiment reproducibility and is version-controlled so that the software can be cited precisely. Due to its modularity, the library is compatible with other frameworks for deep learning with medical images.},
	language = {en},
	urldate = {2021-10-21},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {P{\'e}rez-Garc{\'i}a, Fernando and Sparks, Rachel and Ourselin, S{\'e}bastien},
	month = sep,
	year = {2021},
	keywords = {Data augmentation, Deep learning, Medical image computing, Preprocessing},
	pages = {106236},
	file = {Full Text:/home/fernando/Zotero/storage/FZFMHZ8H/P{\'e}rez-Garc{\'i}a et al. - 2021 - TorchIO A Python library for efficient loading, p.pdf:application/pdf;ScienceDirect Snapshot:/home/fernando/Zotero/storage/CZRWFSRJ/S0169260721003102.html:text/html},
}

@article{vakharia_multicenter_2019,
	title = {Multicenter validation of automated trajectories for selective laser amygdalohippocampectomy},
	volume = {60},
	copyright = {{\textcopyright} 2019 The Authors. Epilepsia published by Wiley Periodicals, Inc. on behalf of International League Against Epilepsy},
	issn = {1528-1167},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/epi.16307},
	doi = {10.1111/epi.16307},
	abstract = {Objective Laser interstitial thermal therapy (LITT) is a novel minimally invasive alternative to open mesial temporal resection in drug-resistant mesial temporal lobe epilepsy (MTLE). The safety and efficacy of the procedure are dependent on the preplanned trajectory and the extent of the planned ablation achieved. Ablation of the mesial hippocampal head has been suggested to be an independent predictor of seizure freedom, whereas sparing of collateral structures is thought to result in improved neuropsychological outcomes. We aim to validate an automated trajectory planning platform against manually planned trajectories to objectively standardize the process. Methods Using the EpiNav platform, we compare automated trajectory planning parameters derived from expert opinion and machine learning to undertake a multicenter validation against manually planned and implemented trajectories in 95 patients with MTLE. We estimate ablation volumes of regions of interest and quantify the size of the avascular corridor through the use of a risk score as a marker of safety. We also undertake blinded external expert feasibility and preference ratings. Results Automated trajectory planning employs complex algorithms to maximize ablation of the mesial hippocampal head and amygdala, while sparing the parahippocampal gyrus. Automated trajectories resulted in significantly lower calculated risk scores and greater amygdala ablation percentage, whereas overall hippocampal ablation percentage did not differ significantly. In addition, estimated damage to collateral structures was reduced. Blinded external expert raters were significantly more likely to prefer automated to manually planned trajectories. Significance Retrospective studies of automated trajectory planning show much promise in improving safety parameters and ablation volumes during LITT for MTLE. Multicenter validation provides evidence that the algorithm is robust, and blinded external expert ratings indicate that the trajectories are clinically feasible. Prospective validation studies are now required to determine if automated trajectories translate into improved seizure freedom rates and reduced neuropsychological deficits.},
	language = {en},
	number = {9},
	urldate = {2020-10-06},
	journal = {Epilepsia},
	author = {Vakharia, Vejay N. and Sparks, Rachel E. and Li, Kuo and O'Keeffe, Aidan G. and P{\'e}rez-Garc{\'i}a, Fernando and Fran{\c c}a, Lucas G. S. and Ko, Andrew L. and Wu, Chengyuan and Aronson, Joshua P. and Youngerman, Brett E. and Sharan, Ashwini and McKhann, Guy and Ourselin, Sebastien and Duncan, John S.},
	year = {2019},
	keywords = {computer-assisted planning, laser interstitial thermal therapy, mesial temporal lobe epilepsy, selective laser amygdalohippocampectomy},
	pages = {1949--1959},
	file = {Full Text PDF:/home/fernando/Zotero/storage/YQJ4G54V/Vakharia et al. - 2019 - Multicenter validation of automated trajectories f.pdf:application/pdf;Snapshot:/home/fernando/Zotero/storage/8YA5Z9TB/epi.html:text/html},
}

@article{perez-garcia_self-supervised_2021,
	title = {A self-supervised learning strategy for postoperative brain cavity segmentation simulating resections},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-021-02420-2},
	doi = {10.1007/s11548-021-02420-2},
	abstract = {Accurate segmentation of brain resection cavities (RCs) aids in postoperative analysis and determining follow-up treatment. Convolutional neural networks (CNNs) are the state-of-the-art image segmentation technique, but require large annotated datasets for training. Annotation of 3D medical images is time-consuming, requires highly trained raters and may suffer from high inter-rater variability. Self-supervised learning strategies can leverage unlabeled data for training.},
	language = {en},
	urldate = {2021-06-14},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {P{\'e}rez-Garc{\'i}a, Fernando and Dorent, Reuben and Rizzi, Michele and Cardinale, Francesco and Frazzini, Valerio and Navarro, Vincent and Essert, Caroline and Ollivier, Ir{\`e}ne and Vercauteren, Tom and Sparks, Rachel and Duncan, John S. and Ourselin, S{\'e}bastien},
	month = jun,
	year = {2021},
	file = {Springer Full Text PDF:/home/fernando/Zotero/storage/F8JQ29YJ/P{\'e}rez-Garc{\'i}a et al. - 2021 - A self-supervised learning strategy for postoperat.pdf:application/pdf},
}

@article{alim-marvasti_probabilistic_2021,
	title = {Probabilistic {Landscape} of {Seizure} {Semiology} {Localising} {Values}},
	journal = {Under submission (Brain)},
	author = {Alim-Marvasti, Ali and Romagnoli, Gloria and Dahele, Karan and Modarres, Hadi and P{\'e}rez-Garc{\'i}a, Fernando and Sparks, Rachel and Ourselin, S{\'e}bastien and Clarkson, Matthew J. and Chowdhury, Fahmida and Diehl, Beate and Duncan, John S.},
	year = {2021},
}

@article{alim-marvasti_mapping_2021,
	title = {Mapping {Epileptic} {Symptomatology} to {Cortical} {Epileptogenicity} in {MNI} {Space}: {Seizure} {Semiology}-to-{Brain} {Visualisation} {Tool}},
	journal = {Under submission (NeuroImage: Clinical)},
	author = {Alim-Marvasti, Ali and P{\'e}rez-Garc{\'i}a, Fernando and Romagnoli, Gloria and Taylor, Peter N. and Vakharia, Vejay and Sparks, Rachel and Ourselin, S{\'e}bastien and Clarkson, Matthew J. and Duncan, John S.},
	year = {2021},
}

@article{alim-marvasti_machine_2021,
	title = {Machine {Learning} for {Localizing} {Epileptogenic}-{Zone} in the {Temporal} {Lobe}: {Quantifying} the {Value} of {Multimodal} {Clinical}-{Semiology} and {Imaging} {Concordance}},
	volume = {3},
	issn = {2673-253X},
	shorttitle = {Machine {Learning} for {Localizing} {Epileptogenic}-{Zone} in the {Temporal} {Lobe}},
	url = {https://www.frontiersin.org/article/10.3389/fdgth.2021.559103},
	doi = {10.3389/fdgth.2021.559103},
	urldate = {2021-10-27},
	journal = {Frontiers in Digital Health},
	author = {Alim-Marvasti, Ali and P{\'e}rez-Garc{\'i}a, Fernando and Dahele, Karan and Romagnoli, Gloria and Diehl, Beate and Sparks, Rachel and Ourselin, Sebastien and Clarkson, Matthew J. and Duncan, John S.},
	year = {2021},
	pages = {8},
	file = {Full Text PDF:/home/fernando/Zotero/storage/HVLI9BSP/Alim-Marvasti et al. - 2021 - Machine Learning for Localizing Epileptogenic-Zone.pdf:application/pdf},
}

@inproceedings{perez-garcia_transfer_2021-1,
	title = {Transfer {Learning} of {Deep} {Spatiotemporal} {Networks} to {Model} {Arbitrarily} {Long} {Videos} of {Seizures}},
	language = {en},
	booktitle = {American {Epilepsy} {Society} ({AES}) {Annual} {Meeting}},
	author = {P{\'e}rez-Garc{\'i}a, Fernando and Scott, Catherine and Sparks, Rachel and Diehl, Beate and Ourselin, S{\'e}bastien},
	year = {2021},
	keywords = {Transfer learning, Epilepsy video-telemetry, Temporal segment networks},
}

@inproceedings{perez-garcia_towards_2020,
	title = {Towards {Objective} {Targeting} of {Intracranial} {Electroencephalography} {Using} {Data}-{Driven} {Semiology}-{Brain} {Visualisation}},
	booktitle = {International {League} {Against} {Epilepsy} ({ILAE}) {British} {Branch} {Virtual} {Annual} {Scientific} {Conference}},
	author = {P{\'e}rez-Garc{\'i}a, Fernando and Alim-Marvasti, Ali and Romagnoli, Gloria and Clarkson, Matthew J. and Sparks, Rachel and Duncan, John S. and Ourselin, S{\'e}bastien},
	year = {2020},
}

@inproceedings{perez-garcia_towards_2020-1,
	title = {Towards {Objective} {Targeting} of {Intracranial} {Electroencephalography} {Using} {Data}-{Driven} {Semiology}-{Brain} {Visualisation}},
	booktitle = {American {Epilepsy} {Society} ({AES}) {Annual} {Meeting}},
	author = {P{\'e}rez-Garc{\'i}a, Fernando and Alim-Marvasti, Ali and Romagnoli, Gloria and Clarkson, Matthew J. and Sparks, Rachel and Duncan, John S. and Ourselin, S{\'e}bastien},
	year = {2020},
}

@inproceedings{milletari_v-net_2016,
	title = {V-{Net}: {Fully} {Convolutional} {Neural} {Networks} for {Volumetric} {Medical} {Image} {Segmentation}},
	isbn = {978-1-5090-5407-7},
	shorttitle = {V-{Net}},
	url = {https://www.computer.org/csdl/proceedings-article/3dv/2016/5407a565/12OmNqFJhzz},
	doi = {10.1109/3DV.2016.79},
	abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
	language = {English},
	urldate = {2021-10-28},
	booktitle = {2016 {Fourth} {International} {Conference} on {3D} {Vision} ({3DV})},
	publisher = {IEEE Computer Society},
	author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	month = oct,
	year = {2016},
	pages = {565--571},
	file = {Snapshot:/home/fernando/Zotero/storage/LQMCSHLE/12OmNqFJhzz.html:text/html;Submitted Version:/home/fernando/Zotero/storage/TLHHET6B/Milletari et al. - 2016 - V-Net Fully Convolutional Neural Networks for Vol.pdf:application/pdf},
}

@misc{perez-garcia_fepegartorchio_2020,
	title = {fepegar/torchio: {TorchIO}: a {Python} library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning},
	shorttitle = {fepegar/torchio},
	url = {https://zenodo.org/record/4296288},
	abstract = {Tools for loading, augmenting and writing 3D medical images with PyTorch},
	urldate = {2021-10-28},
	publisher = {Zenodo},
	author = {P{\'e}rez-Garc{\'i}a, Fernando},
	month = nov,
	year = {2020},
	doi = {10.5281/zenodo.4296288},
	keywords = {deep-learning, machine-learning, convolutional-neural-networks, medical-image-computing, pytorch},
	file = {Zenodo Snapshot:/home/fernando/Zotero/storage/FAS7PNMR/4296288.html:text/html},
}

@misc{perez-garcia_episurg_2020,
	title = {{EPISURG}: a dataset of postoperative magnetic resonance images ({MRI}) for quantitative analysis of resection neurosurgery for refractory epilepsy},
	url = {https://rdr.ucl.ac.uk/articles/dataset/EPISURG_a_dataset_of_postoperative_magnetic_resonance_images_MRI_for_quantitative_analysis_of_resection_neurosurgery_for_refractory_epilepsy/9996158/1},
	publisher = {University College London},
	author = {P{\'e}rez-Garc{\'i}a, Fernando and Rodionov, Roman and Alim-Marvasti, Ali and Sparks, Rachel and Duncan, John and Ourselin, Sebastien},
	month = dec,
	year = {2020},
	doi = {10.5522/04/9996158.v1},
}

@misc{perez-garcia_data_2021,
	title = {Data to support the paper "{Transfer} {Learning} of {Deep} {Spatiotemporal} {Networks} to {Model} {Arbitrarily} {Long} {Videos} of {Seizures}"},
	url = {https://rdr.ucl.ac.uk/articles/dataset/Data_to_support_the_paper_Transfer_Learning_of_Deep_Spatiotemporal_Networks_to_Model_Arbitrarily_Long_Videos_of_Seizures_/14781771/1},
	publisher = {University College London},
	author = {P{\'e}rez-Garc{\'i}a, Fernando and Scott, Catherine and Sparks, Rachel and Diehl, Beate and Ourselin, Sebastien},
	month = jul,
	year = {2021},
	doi = {10.5522/04/14781771.v1},
}

@article{zhang_self-supervised_2021,
	title = {Self-supervised {Tumor} {Segmentation} through {Layer} {Decomposition}},
	url = {http://arxiv.org/abs/2109.03230},
	abstract = {In this paper, we propose a self-supervised approach for tumor segmentation. Specifically, we advocate a zero-shot setting, where models from self-supervised learning should be directly applicable for the downstream task, without using any manual annotations whatsoever. We make the following contributions. First, with careful examination on existing self-supervised learning approaches, we reveal the surprising result that, given suitable data augmentation, models trained from scratch in fact achieve comparable performance to those pre-trained with self-supervised learning. Second, inspired by the fact that tumors tend to be characterized independently to the contexts, we propose a scalable pipeline for generating synthetic tumor data, and train a self-supervised model that minimises the generalisation gap with the downstream task. Third, we conduct extensive ablation studies on different downstream datasets, BraTS2018 for brain tumor segmentation and LiTS2017 for liver tumor segmentation. While evaluating the model transferability for tumor segmentation under a low-annotation regime, including an extreme case of zero-shot segmentation, the proposed approach demonstrates state-of-the-art performance, substantially outperforming all existing self-supervised approaches, and opening up the usage of self-supervised learning in practical scenarios.},
	urldate = {2021-11-15},
	journal = {arXiv:2109.03230 [cs]},
	author = {Zhang, Xiaoman and Xie, Weidi and Huang, Chaoqin and Zhang, Ya and Wang, Yanfeng},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.03230},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/fernando/Zotero/storage/EYHMUFIM/Zhang et al. - 2021 - Self-supervised Tumor Segmentation through Layer D.pdf:application/pdf;arXiv.org Snapshot:/home/fernando/Zotero/storage/WLXWGJU2/2109.html:text/html},
}

@article{nashef_sudden_1997,
	title = {Sudden unexpected death in epilepsy: terminology and definitions},
	volume = {38},
	issn = {1528-1167},
	shorttitle = {Sudden unexpected death in epilepsy},
	doi = {10.1111/j.1528-1157.1997.tb06130.x},
	abstract = {SUMMARY: Inconsistent and inaccurate death certification, lack of agreed definitions, different terminology, and different understanding of the same terminology hamper research into mortality in epilepsy and result in national statistics that are difficult to interpret. A consensus in death certification and in classification of epilepsy-related deaths, including sudden unexpected death in epilepsy (SUDEP), is needed. Guidelines for classifying cases as SUDEP are proposed. Their aim is to allow uniformity and comparability between studies. These guidelines take into account the limits of the information usually available in this setting even when these deaths are investigated. The guidelines are complemented by those dealing with identified deaths in epidemiologic studies where data are lacking.},
	language = {eng},
	number = {11 Suppl},
	journal = {Epilepsia},
	author = {Nashef, L.},
	month = nov,
	year = {1997},
	pmid = {19909329},
	keywords = {Humans, Epilepsy, Death, Sudden, Terminology as Topic},
	pages = {S6--8},
	file = {Full Text:/home/fernando/Zotero/storage/3PE8JXJP/Nashef - 1997 - Sudden unexpected death in epilepsy terminology a.pdf:application/pdf},
}

@misc{epilepsy_society_epilepsy_nodate,
	title = {Epilepsy {Society} {\textbar} {Transforming} lives through advocacy, research and care},
	url = {https://epilepsysociety.org.uk/},
	urldate = {2021-11-15},
	author = {Epilepsy Society},
	file = {Epilepsy Society | Transforming lives through advocacy, research and care:/home/fernando/Zotero/storage/Z6PHWI5U/epilepsysociety.org.uk.html:text/html},
}

@article{devinsky_sudden_2016,
	title = {Sudden unexpected death in epilepsy: epidemiology, mechanisms, and prevention},
	volume = {15},
	issn = {1474-4465},
	shorttitle = {Sudden unexpected death in epilepsy},
	doi = {10.1016/S1474-4422(16)30158-2},
	abstract = {Sudden unexpected death in epilepsy (SUDEP) can affect individuals of any age, but is most common in younger adults (aged 20-45 years). Generalised tonic-clonic seizures are the greatest risk factor for SUDEP; most often, SUDEP occurs after this type of seizure in bed during sleep hours and the person is found in a prone position. SUDEP excludes other forms of seizure-related sudden death that might be mechanistically related (eg, death after single febrile, unprovoked seizures, or status epilepticus). Typically, postictal apnoea and bradycardia progress to asystole and death. A crucial element of SUDEP is brainstem dysfunction, for which postictal generalised EEG suppression might be a biomarker. Dysfunction in serotonin and adenosine signalling systems, as well as genetic disorders affecting cardiac conduction and neuronal excitability, might also contribute. Because generalised tonic-clonic seizures precede most cases of SUDEP, patients must be better educated about prevention. The value of nocturnal monitoring to detect seizures and postictal stimulation is unproven but warrants further study.},
	language = {eng},
	number = {10},
	journal = {The Lancet. Neurology},
	author = {Devinsky, Orrin and Hesdorffer, Dale C. and Thurman, David J. and Lhatoo, Samden and Richerson, George},
	month = sep,
	year = {2016},
	pmid = {27571159},
	keywords = {Humans, Epilepsy, Death, Sudden},
	pages = {1075--1088},
}

@article{jha_sudden_2021,
	title = {Sudden {Unexpected} {Death} in {Epilepsy}: {A} {Personalized} {Prediction} {Tool}},
	volume = {96},
	copyright = {Copyright {\textcopyright} 2021 The Author(s). Published by Wolters Kluwer Health, Inc. on behalf of the American Academy of Neurology.. This is an open access article distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND), which permits downloading and sharing the work provided it is properly cited. The work cannot be changed in any way or used commercially without permission from the journal.},
	issn = {0028-3878, 1526-632X},
	shorttitle = {Sudden {Unexpected} {Death} in {Epilepsy}},
	url = {https://n.neurology.org/content/96/21/e2627},
	doi = {10.1212/WNL.0000000000011849},
	abstract = {Objective To develop and validate a tool for individualized prediction of sudden unexpected death in epilepsy (SUDEP) risk, we reanalyzed data from 1 cohort and 3 case{\textendash}control studies undertaken from 1980 through 2005.
Methods We entered 1,273 epilepsy cases (287 SUDEP, 986 controls) and 22 clinical predictor variables into a Bayesian logistic regression model.
Results Cross-validated individualized model predictions were superior to baseline models developed from only average population risk or from generalized tonic-clonic seizure frequency (pairwise difference in leave-one-subject-out expected log posterior density = 35.9, SEM {\textpm} 12.5, and 22.9, SEM {\textpm} 11.0, respectively). The mean cross-validated (95\% bootstrap confidence interval) area under the receiver operating curve was 0.71 (0.68{\textendash}0.74) for our model vs 0.38 (0.33{\textendash}0.42) and 0.63 (0.59{\textendash}0.67) for the baseline average and generalized tonic-clonic seizure frequency models, respectively. Model performance was weaker when applied to nonrepresented populations. Prognostic factors included generalized tonic-clonic and focal-onset seizure frequency, alcohol excess, younger age at epilepsy onset, and family history of epilepsy. Antiseizure medication adherence was associated with lower risk.
Conclusions Even when generalized to unseen data, model predictions are more accurate than population-based estimates of SUDEP. Our tool can enable risk-based stratification for biomarker discovery and interventional trials. With further validation in unrepresented populations, it may be suitable for routine individualized clinical decision-making. Clinicians should consider assessment of multiple risk factors, and not focus only on the frequency of convulsions.},
	language = {en},
	number = {21},
	urldate = {2021-11-15},
	journal = {Neurology},
	author = {Jha, Ashwani and Oh, Cheongeun and Hesdorffer, Dale and Diehl, Beate and Devore, Sasha and Brodie, Martin J. and Tomson, Torbj{\"o}rn and Sander, Josemir W. and Walczak, Thaddeus S. and Devinsky, Orrin},
	month = may,
	year = {2021},
	pmid = {33910939},
	note = {Publisher: Wolters Kluwer Health, Inc. on behalf of the American Academy of Neurology
Section: Article},
	pages = {e2627--e2638},
	file = {Snapshot:/home/fernando/Zotero/storage/J3F7YFMQ/e2627.html:text/html;Full Text PDF:/home/fernando/Zotero/storage/RGZYA4QL/Jha et al. - 2021 - Sudden Unexpected Death in Epilepsy A Personalize.pdf:application/pdf},
}

@article{so_what_2008,
	title = {What is known about the mechanisms underlying {SUDEP}?},
	volume = {49 Suppl 9},
	issn = {1528-1167},
	doi = {10.1111/j.1528-1167.2008.01932.x},
	abstract = {This article highlights studies in three major domains of potential mechanisms of sudden unexplained death in epilepsy (SUDEP): cardiac, respiratory, and autonomic. Ictal cardiac arrest is a clinically rare but well-recognized potential mechanism of SUDEP. Studies have failed to identify preexisting cardiac electrophysiologic or structural abnormalities that distinguish SUDEP persons. Some degree of pulmonary congestion is a common autopsy finding, but severe pulmonary edema occurs very rarely with seizures. In contrast, periictal apnea and hypoxia occur commonly with generalized tonic-clonic seizures and, to a lesser degree, with complex partial seizures. There are several animal models of postictal respiratory arrest. Postictal respiratory arrest in audiogenic seizure mice can be induced by serotonin receptor inhibition or prevented by selective serotonin reuptake inhibitor (SSRI) drugs. Reduced heart rate variability occurs in patients with refractory epilepsy and can be induced in animal seizure models, but its precise role in predisposing persons to sudden death requires further investigation.},
	language = {eng},
	journal = {Epilepsia},
	author = {So, Elson L.},
	month = dec,
	year = {2008},
	pmid = {19087123},
	keywords = {Humans, Epilepsy, Death, Sudden, Animals, Autonomic Nervous System, Heart Diseases, Lung Diseases},
	pages = {93--98},
}

@inproceedings{achilles_patient_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Patient {MoCap}: {Human} {Pose} {Estimation} {Under} {Blanket} {Occlusion} for {Hospital} {Monitoring} {Applications}},
	isbn = {978-3-319-46720-7},
	shorttitle = {Patient {MoCap}},
	doi = {10.1007/978-3-319-46720-7_57},
	abstract = {Motion analysis is typically used for a range of diagnostic procedures in the hospital. While automatic pose estimation from RGB-D input has entered the hospital in the domain of rehabilitation medicine and gait analysis, no such method is available for bed-ridden patients. However, patient pose estimation in the bed is required in several fields such as sleep laboratories, epilepsy monitoring and intensive care units. In this work, we propose a learning-based method that allows to automatically infer 3D patient pose from depth images. To this end we rely on a combination of convolutional neural network and recurrent neural network, which we train on a large database that covers a range of motions in the hospital bed. We compare to a state of the art pose estimation method which is trained on the same data and show the superior result of our method. Furthermore, we show that our method can estimate the joint positions under a simulated occluding blanket with an average joint error of 7.56 cm.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} {\textendash}  {MICCAI} 2016},
	publisher = {Springer International Publishing},
	author = {Achilles, Felix and Ichim, Alexandru-Eugen and Coskun, Huseyin and Tombari, Federico and Noachtar, Soheyl and Navab, Nassir},
	editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
	year = {2016},
	keywords = {CNN, Motion capture, Occlusion, Pose estimation, Random forest, RNN},
	pages = {491--499},
	file = {Springer Full Text PDF:/home/fernando/Zotero/storage/KEENGKSI/Achilles et al. - 2016 - Patient MoCap Human Pose Estimation Under Blanket.pdf:application/pdf},
}

@misc{noauthor_home_nodate,
	title = {Home - {Office} for {National} {Statistics}},
	url = {https://www.ons.gov.uk/},
	urldate = {2021-11-17},
	file = {Home - Office for National Statistics:/home/fernando/Zotero/storage/5BB92K77/www.ons.gov.uk.html:text/html},
}

@article{sutula_epileptic_2003,
	title = {Do epileptic seizures damage the brain?},
	volume = {16},
	issn = {1350-7540},
	url = {https://journals.lww.com/co-neurology/Fulltext/2003/04000/Do_epileptic_seizures_damage_the_brain_.00012.aspx},
	abstract = {Purpose of review~
        The possibility that recurrent seizures in patients with poorly controlled epilepsy may produce neuronal damage and contribute to progressive functional and cognitive declines observed in some patients with epilepsy has major clinical and therapeutic implications for urgency of treatment and effective intervention to achieve complete control.
        Recent findings~
        Advances in magnetic resonance imaging techniques, technical and conceptual advances in experimental analysis of neuronal death at the cellular and molecular level, and long-term neuropsychological observations have provided substantial new data and insights into phenomena of seizure-induced plasticity in neural circuitry that address the question {\textquoteleft}Do epileptic seizures damage the brain?{\textquoteright}
        Summary~
        The emerging perspective is that seizure-induced damage should be regarded not only as neuronal loss but as adverse long-term behavioral and cognitive consequences. This perspective provides a strong rationale for development of neuroprotective treatments to forestall adverse long-term consequences of repeated seizures, and for the importance of prompt, effective intervention that achieves complete seizure control.},
	language = {en-US},
	number = {2},
	urldate = {2021-11-25},
	journal = {Current Opinion in Neurology},
	author = {Sutula, Thomas P. and Hagen, Joshua and Pitk{\"a}nen, Asla},
	month = apr,
	year = {2003},
	pages = {189--195},
	file = {Snapshot:/home/fernando/Zotero/storage/6KJW9PIA/Do_epileptic_seizures_damage_the_brain_.00012.html:text/html},
}

@article{forsgren_mortality_2005,
	title = {Mortality of epilepsy in developed countries: a review},
	volume = {46 Suppl 11},
	issn = {0013-9580},
	shorttitle = {Mortality of epilepsy in developed countries},
	doi = {10.1111/j.1528-1167.2005.00403.x},
	abstract = {Mortality in people with epilepsy has been studied in many different populations. In population-based incidence cohorts of epilepsy with 7-29 years follow-up, there was up to a threefold increase in mortality, compared to the general population (standardized mortality ratios [SMR] ranged from 1.6 to 3.0). When studies include selected epilepsy populations where patients with frequent and severe seizures are more common, the mortality is even greater. Relative survivorship (RS) following the diagnosis of epilepsy was 91\%, 85\%, and 83\% after 5, 10, and 15 years, respectively. In a population with childhood-onset epilepsy, RS was 94\% and 88\% after 10 and 20 years. The level of increased mortality is affected by several factors. In idiopathic epilepsy where the causes of seizures are unknown, the results are conflicting. There was no significant increase in mortality in studies from Iceland, France, and Sweden, a barely increased risk in a study from the United Kingdom, and a significantly increased risk in a study from the United States. In contrast, all studies report a significant increased mortality in remote symptomatic epilepsy (standardized mortality ratios [SMRs] ranging from 2.2 to 6.5). The highest mortality is found in patients with epilepsy and neurodeficits present since birth, including mental retardation or cerebral palsy (SMRs ranging from 7 to 50). Mortality is also affected by age, with the highest SMRs in children, the combined effect of low mortality in the reference population, and high mortality in children with neurodeficits and epilepsy. The highest excess mortality is found in the elderly, {\textgreater} or =75 years. A pronounced increase in mortality is found during the first year following the onset of seizures due to underlying severe diseases. The increased mortality remains in different studies 2-14 years following diagnosis. Most of the factors responsible for the increased mortality are related to the underlying disorder causing epilepsy with pneumonia, cerebrovascular disease, and neoplastic disorders (risk remains elevated when primary brain tumors are excluded), as the most frequently recorded causes. The most common direct seizure-related cause of death in adolescents and young adults is sudden unexpected death, which is 24 times more common than in the general population.},
	language = {eng},
	journal = {Epilepsia},
	author = {Forsgren, Lars and Hauser, W. Allen and Olafsson, Elias and Sander, J. W. a. S. and Sillanp{\"a}{\"a}, Matti and Tomson, Torbj{\"o}rn},
	year = {2005},
	pmid = {16393174},
	keywords = {Adolescent, Adult, Age Distribution, Cause of Death, Child, Cross-Cultural Comparison, Death, Sudden, Developed Countries, Epilepsy, Humans, Incidence, Mortality, Prognosis, Risk Factors, Seizures, Sex Distribution, Survival Analysis, Terminology as Topic},
	pages = {18--27},
	file = {Full Text:/home/fernando/Zotero/storage/GW7SQDL3/Forsgren et al. - 2005 - Mortality of epilepsy in developed countries a re.pdf:application/pdf},
}
